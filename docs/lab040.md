# Lab 4.0 : Testing vLLM on MacOS 

Pull the vLLM Image created with CPU Only Inference with 

```
docker image pull schoolofdevops/vllm-cpu-nonuma:0.9.1
```

Export the merged model built earlier as a model directory on your Mac 

e.g. 
```
export MODEL_DIR=/Users/gshah/work/llmops/code/project/atharva-dental-assistant/artifacts/train/20251001-084805/merged-model
```
replace the path with actual path to `merged-model`

Launch the vLLM using CPU only mode with options which should work on MacOS Silicon 

```
docker run --rm -p 8123:8000 --name vllmtest \
  --security-opt seccomp=unconfined \
  -e VLLM_TARGET_DEVICE=cpu \
  -e OMP_NUM_THREADS=2 -e OPENBLAS_NUM_THREADS=1 -e MKL_NUM_THREADS=1 \
  -e VLLM_CPU_KVCACHE_SPACE=1 \
  -v "$MODEL_DIR:/models/model:ro" \
  schoolofdevops/vllm-cpu-nonuma:0.9.1 \
  --model /models/model \
  --host 0.0.0.0 \
  --port 8000 \
  --max-model-len 2048 \
  --served-model-name smollm2-135m-atharva \
  --dtype float16 \
  --disable-frontend-multiprocessing
```


## Quick local validation

### 1) Model list

```
curl -s http://127.0.0.1:8123/v1/models | jq .
```

### 2) Simple chat completion

```
curl -s -X POST http://127.0.0.1:8123/v1/chat/completions \
  -H 'Content-Type: application/json' \
  -d '{
        "model":"smollm2-135m-atharva",
        "messages":[{"role":"user","content":"Say hello in 5 words."}],
        "max_tokens": 32,
        "temperature": 0.2
      }' | jq .
```


```
curl -s -X POST http://127.0.0.1:30200/v1/chat/completions \
  -H 'Content-Type: application/json' \
  -d '{
        "model":"smollm2-135m-atharva",
        "messages":[{"role":"user","content":"Say hello in 5 words."}],
        "max_tokens": 32,
        "temperature": 0.2
      }' | jq .
```

### 3) Legacy completions (optional)

```
curl -s -X POST http://127.0.0.1:8123/v1/completions \
  -H 'Content-Type: application/json' \
  -d '{
        "model":"smollm2-135m-atharva",
        "prompt":"Write a haiku about dentists:",
        "max_tokens": 40
      }' | jq .
```

### 4) Embeddings (optional / Not Available)

```
curl -s -X POST http://127.0.0.1:8123/v1/embeddings \
  -H 'Content-Type: application/json' \
  -d '{
        "model":"smollm2-135m-atharva",
        "input":"dental care tips"
      }' | jq .
```

> tip: on CPU, `--dtype float32` or `bfloat16` is usually safer than `float16`. You can leave it as-is since it works; if you hit oddities, try `--dtype float32`.


Delete the container with 

```
docker rm -f vllmtest
```

## Load the Model to KIND Cluster 

Load it to a specific node `llmops-kind-worker2`

```
kind load  docker-image schoolofdevops/vllm-cpu-nonuma:0.9.1 --name llmops-kind --nodes llmops-kind-worker
```


#courses/llmops/labs/v1
