# Lab 1: Synthetic data + RAG system (CPU-only)

Below are copy-pasteable files + commands so you can run this end-to-end on the KIND cluster you set up in Lab 0.

---

# 📁 Repo layout added in this lab

```
atharva-dental-assistant/
├─ datasets/
│  └─ clinic/
│     ├─ treatments.json               # synthetic seed data (few rows to start)
│     ├─ policies/
│     │  ├─ appointments.md
│     │  ├─ billing.md
│     │  ├─ cancellations.md
│     │  ├─ emergency.md
│     │  └─ sterilization.md
│     ├─ faq.md
│     └─ recent_queries.jsonl
├─ artifacts/                          # generated by jobs (auto-created)
├─ tools/
│  ├─ synth_data.py                    # makes train/val/eval JSONL
│  └─ common.py                        # tiny helpers
├─ rag/
│  ├─ build_index.py                   # chunks + embeds + FAISS
│  └─ retriever.py                     # FastAPI service
├─ k8s/
│  ├─ 10-data/
│  │  ├─ job-generate-data.yaml
│  │  └─ job-build-index.yaml
│  └─ 40-serve/
│     ├─ deploy-retriever.yaml
│     └─ svc-retriever.yaml
└─ scripts/
   ├─ generate_data.sh
   ├─ build_index.sh
   └─ deploy_retriever.sh
```

> In Lab 0 your KIND nodes already mount `./project` → `/mnt/project`.
> Place this repo under `./project/atharva-dental-assistant` so Jobs can see it at `/mnt/project/atharva-dental-assistant`.

Start creating the repo 

```
cd project/atharva-dental-assistant
mkdir -p datasets/clinic/policies tools rag k8s/10-data k8s/40-serve scripts
```

Also set the namespace to `atharva-ml`

```
kubectl config get-contexts

kubectl config set-context --current --namespace=atharva-ml
```
---


## 1) Seed the clinic content (synthetic but realistic)

Create these minimal files; you can expand later.

### `datasets/clinic/treatments.json`

```
[
  {
    "code": "TX-SCALE-01",
    "name": "Scaling & Polishing",
    "category": "Preventive",
    "indications": ["Tartar buildup", "Gingival bleeding"],
    "contraindications": [],
    "steps": ["Ultrasonic scaling", "Polishing", "Fluoride advice"],
    "duration_minutes": 40,
    "visits": 1,
    "price_band_inr": [1200, 1800],
    "aftercare": ["Warm saline rinses 24h", "Soft bristle brushing"],
    "risks": ["Transient sensitivity"]
  },
  {
    "code": "TX-RCT-01",
    "name": "Root Canal Therapy",
    "category": "Endodontics",
    "indications": ["Deep caries", "Pulpitis", "Apical periodontitis"],
    "contraindications": ["Poor crown-root ratio (relative)"],
    "steps": ["Local anesthesia", "Canal cleaning", "Obturation", "Temporary filling"],
    "duration_minutes": 90,
    "visits": 2,
    "price_band_inr": [6000, 9500],
    "aftercare": ["Analgesics as advised", "Avoid hard chewing until crown"],
    "risks": ["Post-op soreness", "Instrument separation (rare)"]
  },
  {
    "code": "TX-FILL-01",
    "name": "Composite Filling",
    "category": "Restorative",
    "indications": ["Dental caries", "Chipped tooth (minor)"],
    "contraindications": [],
    "steps": ["Isolation", "Decay removal", "Bonding", "Composite placement and curing", "Polishing"],
    "duration_minutes": 20,
    "visits": 1,
    "price_band_inr": [1200, 2000],
    "aftercare": ["Avoid very hard foods for 24h", "Monitor sensitivity to cold/sweets"],
    "risks": ["Transient sensitivity", "Marginal staining over time"]
},
{
    "code": "TX-EXT-01",
    "name": "Tooth Extraction",
    "category": "Oral Surgery",
    "indications": ["Non-restorable tooth", "Severe mobility", "Impacted tooth (varies)"],
    "contraindications": ["Uncontrolled bleeding disorders (relative)", "Uncontrolled diabetes (relative)"],
    "steps": ["Local anesthesia", "Tooth luxation", "Removal", "Hemostasis"],
    "duration_minutes": 30,
    "visits": 1,
    "price_band_inr": [1500, 3500],
    "aftercare": ["Bite on gauze 30–45 min", "No spitting/rinsing 24h", "No straws/smoking 48–72h", "Soft diet first day"],
    "risks": ["Dry socket", "Bleeding", "Swelling"]
}

]
```

### `datasets/clinic/policies/appointments.md`

```
# Appointments
- Hours: Mon–Sat 9:30–18:30 (Pune, IST).
- Booking: phone/WhatsApp/website form.
- Emergency slots: limited, call ahead.
```

### `datasets/clinic/policies/billing.md`

```
# Billing
- Currency: INR.
- Payments: UPI, cards, netbanking.
- Insurance: reimbursement support; direct tie-ups listed at reception.
```

### `datasets/clinic/policies/cancellations.md`

```
# Cancellations
- 24h notice requested.
- Same-day cancellations may incur ₹300 chair-time fee.
```

### `datasets/clinic/policies/emergency.md`

```
# Emergency
- Red flags: uncontrolled bleeding, facial swelling, high fever, trauma.
- Call: +91-20-4000-0000 (day); after-hours +91-99-9999-9999.
- Fever after extraction with foul taste may indicate infection—seek evaluation.
- Rapidly worsening swelling with difficulty opening mouth (trismus) requires urgent care.
```

### `datasets/clinic/policies/sterilization.md`

```
# Sterilization
- Class-B autoclave cycles, pouched instruments, surface disinfection between patients.
```

### `datasets/clinic/faq.md`

```
# FAQs
Q: Is scaling painful?  
A: Mild discomfort; local anesthesia for sensitive cases.

Q: Do whitening results last?  
A: 6–12 months; depends on diet and habits.

Q: Do you work on Sundays?  
A: Only emergency slots on call.
```

### `datasets/clinic/recent_queries.jsonl`

```
{"ts":"2025-09-20T11:05:00Z","q":"Can I eat spicy food after scaling?","a":"Prefer soft foods for 24h; avoid very hot/spicy today."}
{"ts":"2025-09-22T15:22:00Z","q":"RCT pain next day normal?","a":"Mild soreness common 24–48h; call if severe swelling/fever."}
```

---

## 2) Data synth tool (train/val/eval JSONL)

### `tools/common.py`

```
import random, re
from pathlib import Path

def read_md(path: Path) -> str:
    return path.read_text(encoding="utf-8")

def normalize_ws(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()

def sys_prompt() -> str:
    return ("You are Atharva Dental Clinic assistant in Pune (INR). "
            "Be concise, safety-minded, include 'Source:' with file#section. "
            "If info is missing, ask follow-up questions.")
```

### `tools/synth_data.py`

```
import json, argparse, random, re
from pathlib import Path
from difflib import SequenceMatcher

# If you still want to keep common.read_md/normalize_ws, import them.
from common import read_md, normalize_ws  # noqa: F401

random.seed(42)

def make_system_prompt(clinic: str, currency: str) -> str:
    return (
        f"You are Atharva Dental Clinic assistant in {clinic}, India. "
        f"Respond in concise steps, use {currency} (₹) for any prices, be safety-minded, "
        f"ask for missing info when necessary, and ALWAYS include a final 'Source:' line "
        f"citing file#section for facts derived from context."
    )

def fmt_inr(x: int) -> str:
    return f"₹{x:,}"

def join_steps(items):
    """
    Return a plain, semicolon-separated list (no numbering).
    We'll format to numbered steps later in normalize_list_answer().
    """
    return "; ".join(s.strip() for s in items if s and str(s).strip())

_BULLET_PREFIX = re.compile(r'^\s*(?:[-*•]+|\d+[.)])\s*', re.IGNORECASE)

def _strip_bullet(s: str) -> str:
    return _BULLET_PREFIX.sub("", s).strip()

def _capitalize_first(s: str) -> str:
    return s[:1].upper() + s[1:] if s else s

def normalize_list_answer(text: str) -> str:
    """
    If the answer is a list (separated by ';' or newlines), convert to numbered 1) ... lines.
    If it's already numbered/bulleted, strip existing bullets/numbers and renumber cleanly.
    Single-line answers are returned as-is (with trimmed whitespace).
    """
    if text is None:
        return ""
    raw = text.strip()

    # If it already looks like multiple lines or contains semicolons, treat as a list
    is_listy = (";" in raw) or ("\n" in raw)

    # Split on semicolons OR newlines, keep non-empty
    parts = [p for p in re.split(r"[;\n]+", raw) if p.strip()]
    # If splitting produced only one item and it doesn't start with bullets/numbers, return trimmed
    if len(parts) <= 1 and not _BULLET_PREFIX.match(raw):
        return raw

    # If it's single line but has bullets/numbers, treat as one part
    if len(parts) <= 1:
        parts = [raw]

    # Clean bullets/numbers and whitespace; capitalize first letter of each step
    cleaned = [_capitalize_first(_strip_bullet(p)) for p in parts if _strip_bullet(p)]

    # If after cleaning we only have one part, just return it
    if len(cleaned) <= 1 and not is_listy:
        return cleaned[0] if cleaned else raw

    # Number them
    return "\n".join(f"{i+1}) {p}" for i, p in enumerate(cleaned))

def add_paraphrases(q: str) -> list[str]:
    out = [q]
    ql = q.lower()

    m = re.match(r"how long does (.+?) take and how many visits\??", q, flags=re.I)
    if m:
        proc = m.group(1)
        out.append(f"{proc} — duration and number of visits?")
        out.append(f"What's the time per visit and total visits for {proc}?")

    if "what is the cost for " in ql:
        proc = q[q.lower().find("what is the cost for ")+len("what is the cost for "):].rstrip("?")
        out.append(f"Approximate price range for {proc}?")
        out.append(f"How much does {proc} typically cost?")

    if re.search(r"\baftercare\b", ql):
        out.append(q.replace("What are aftercare steps", "Post-treatment care steps"))

    uniq = []
    seen = set()
    for cand in out:
        if cand not in seen:
            seen.add(cand)
            uniq.append(cand)
    return uniq

def near_duplicate(a: str, b: str, threshold: float = 0.90) -> bool:
    return SequenceMatcher(None, a.lower(), b.lower()).ratio() >= threshold

def emit_sample(system_prompt, q, a, source, ask_clarify: str | None = None):
    # Normalize to consistent numbered-steps style when list-like
    norm_a = normalize_list_answer(a)
    if ask_clarify:
        # Ensure the clarifying question starts on a new line and is properly capitalized
        clar = _capitalize_first(ask_clarify.strip())
        norm_a = f"{norm_a}\n{clar}"
    return {
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": q},
            {"role": "assistant", "content": f"{norm_a}\nSource: {source}"}
        ]
    }

def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--clinic", default="Pune")
    ap.add_argument("--currency", default="INR")
    ap.add_argument("--treatments", required=True)
    ap.add_argument("--policies", nargs="+", required=True)
    ap.add_argument("--faq", required=True)
    ap.add_argument("--recent", required=True)
    ap.add_argument("--out", required=True)
    ap.add_argument("--max_per_treatment", type=int, default=7,
                    help="Upper bound of Q/A variants per treatment")
    args = ap.parse_args()

    out = Path(args.out); out.mkdir(parents=True, exist_ok=True)
    train, val, evalq = [], [], []
    sys_p = make_system_prompt(args.clinic, args.currency)

    # ----------------------------
    # Treatments → richer Q/A set
    # ----------------------------
    treatments = json.loads(Path(args.treatments).read_text(encoding="utf-8"))
    for t in treatments:
        code = t.get("code", "TX-UNK")
        name = t["name"]
        dur = t.get("duration_minutes")
        visits = t.get("visits")
        low, high = t.get("price_band_inr", [None, None])
        aftercare = t.get("aftercare", [])
        risks = t.get("risks", [])
        indications = t.get("indications", [])
        contraind = t.get("contraindications", [])

        src = f"treatments.json#{code}"

        samples = []

        # 1) Duration + visits + price (+ 1–2 paraphrases)
        if dur and visits and low is not None and high is not None:
            q = f"How long does {name} take and how many visits?"
            a = join_steps([
                f"Typically {dur} minutes",
                f"About {visits} visit(s)",
                f"Price band: {fmt_inr(low)}–{fmt_inr(high)}"
            ])
            for pq in add_paraphrases(q)[:2]:
                samples.append(emit_sample(sys_p, pq, a, src))

        # 2) Aftercare (+ 1 paraphrase)
        if aftercare:
            q = f"What are aftercare steps for {name}?"
            a = join_steps(aftercare)
            for pq in add_paraphrases(q)[:2]:
                samples.append(emit_sample(sys_p, pq, a, src))

        # 3) Risks
        q = f"Any risks with {name}?"
        a = join_steps(risks) if risks else "Minimal risks when indicated by the clinician."
        samples.append(emit_sample(sys_p, q, a, src))

        # 4) Cost-only (+ 1 paraphrase)
        if low is not None and high is not None:
            q = f"What is the cost for {name}?"
            a = f"{fmt_inr(low)}–{fmt_inr(high)} depending on case complexity and materials."
            for pq in add_paraphrases(q)[:2]:
                samples.append(emit_sample(sys_p, pq, a, src))

        # 5) Pain/comfort expectation
        q = f"Is {name.lower()} painful?"
        if "Scaling" in name:
            a = "You may feel mild discomfort; anesthesia can be used for sensitive cases."
        elif "Root Canal" in name:
            a = "Local anesthesia is used; you may feel post-op soreness for 24–48h."
        else:
            a = "Local anesthesia minimizes pain; some soreness after the procedure is common."
        samples.append(emit_sample(sys_p, q, a, src))

        # 6) Eligibility/contraindications (if present)
        if contraind:
            q = f"Who should avoid or be cautious about {name}?"
            a = join_steps(contraind) + "; Consult your dentist to evaluate individual risks."
            samples.append(emit_sample(sys_p, q, a, src))

        # 7) When indicated
        if indications:
            q = f"When is {name} recommended?"
            a = join_steps([f"Indicated for: {', '.join(indications)}"])
            samples.append(emit_sample(sys_p, q, a, src))

        # 8) Clarifying-quote variant
        if low is not None and high is not None:
            q = f"Can I get a quick quote for {name}?"
            a = f"Typical range is {fmt_inr(low)}–{fmt_inr(high)}; exact estimate varies by tooth and complexity."
            ask = "Which tooth/area and any prior treatment? I can give a closer estimate."
            samples.append(emit_sample(sys_p, q, a, src, ask_clarify=ask))

        if args.max_per_treatment > 0:
            samples = samples[:args.max_per_treatment]

        train.extend(samples)

    # ----------------------------
    # Policies → Q/A
    # ----------------------------
    for p in args.policies:
        path = Path(p)
        if not path.exists():
            continue
        head = path.stem

        if "appointments" in head:
            train.append(emit_sample(
                sys_p,
                "What are clinic hours?",
                "Mon–Sat 9:30–18:30 IST; limited emergency slots on call.",
                f"{head}.md#hours"
            ))
            train.append(emit_sample(
                sys_p,
                "Do you accept walk-ins?",
                "Appointments preferred; limited same-day slots may be available. Call/WhatsApp to check wait time.",
                f"{head}.md#walkins"
            ))

        if "cancellations" in head:
            train.append(emit_sample(
                sys_p,
                "Cancellation policy?",
                "24h notice requested; same-day cancellations may incur a chair-time fee of ₹300.",
                f"{head}.md#policy"
            ))
            train.append(emit_sample(
                sys_p,
                "How to reschedule appointment?",
                "Call/WhatsApp at least 24h prior to reschedule; late changes may incur ₹300 fee.",
                f"{head}.md#reschedule"
            ))

        if "emergency" in head:
            train.append(emit_sample(
                sys_p,
                "What are dental red flags?",
                "Uncontrolled bleeding, facial swelling, high fever, trauma—seek urgent care/call immediately.",
                f"{head}.md#red-flags"
            ))
            train.append(emit_sample(
                sys_p,
                "Wisdom tooth pain with swelling—what to do?",
                join_steps([
                    "Avoid self-medicating antibiotics",
                    "Warm saline rinses",
                    "Seek urgent evaluation if fever, trismus, or spreading swelling"
                ]),
                f"{head}.md#wisdom-swelling"
            ))

        if "billing" in head:
            train.append(emit_sample(
                sys_p,
                "Payment methods accepted?",
                "UPI, cards, netbanking; Insurance reimbursement support available.",
                f"{head}.md#methods"
            ))

        if "sterilization" in head:
            train.append(emit_sample(
                sys_p,
                "How do you sterilize instruments?",
                "Class-B autoclave cycles, pouched instruments, surface disinfection between patients.",
                f"{head}.md#protocols"
            ))

    # ----------------------------
    # FAQs → a few seed items
    # ----------------------------
    if Path(args.faq).exists():
        _ = read_md(Path(args.faq))
        train.append(emit_sample(
            sys_p, "Is scaling painful?",
            "Mild discomfort; Local anesthesia for sensitive cases.",
            "faq.md#scaling-pain"
        ))
        train.append(emit_sample(
            sys_p, "Do whitening results last?",
            "Results typically last 6–12 months; Depends on diet and habits.",
            "faq.md#whitening-duration"
        ))
        train.append(emit_sample(
            sys_p, "Do you work on Sundays?",
            "Only emergency slots on call.",
            "faq.md#sunday-hours"
        ))

    # ----------------------------
    # Deduplicate near-identical questions
    # ----------------------------
    deduped = []
    seen_qs = []
    for ex in train:
        q = ex["messages"][1]["content"]
        if any(near_duplicate(q, s) for s in seen_qs):
            continue
        seen_qs.append(q)
        deduped.append(ex)
    train = deduped

    # ----------------------------
    # Shuffle + split (80/20)
    # ----------------------------
    random.shuffle(train)
    split = int(0.8 * len(train)) if len(train) else 0

    (Path(out) / "train.jsonl").write_text(
        "\n".join(json.dumps(x, ensure_ascii=False) for x in train[:split]),
        encoding="utf-8"
    )
    (Path(out) / "val.jsonl").write_text(
        "\n".join(json.dumps(x, ensure_ascii=False) for x in train[split:]),
        encoding="utf-8"
    )
    (Path(out) / "eval.jsonl").write_text(
        "\n".join(json.dumps(x, ensure_ascii=False) for x in evalq),
        encoding="utf-8"
    )

    print(f"Wrote {len(train[:split])} train, {len(train[split:])} val, {len(evalq)} eval to {out}")

if __name__ == "__main__":
    main()
```

---

## 3) Build the FAISS index + a tiny retriever API

### `rag/build_index.py`

```
import argparse
import json
import os
from pathlib import Path
from typing import Iterable, Tuple, Dict, Any, List

from sentence_transformers import SentenceTransformer
import faiss


# -------- Helpers to render concise, model-friendly chunk text --------

def _render_treatment_item(it: Dict[str, Any]) -> str:
    """
    Render a single treatment JSON object into a compact, informative snippet.
    """
    keys_order = (
        "code", "name", "category",
        "duration_minutes", "visits", "price_band_inr",
        "indications", "contraindications",
        "steps", "aftercare", "risks"
    )
    lines: List[str] = []
    for k in keys_order:
        if k not in it:
            continue
        v = it[k]
        if isinstance(v, (list, tuple)):
            v = ", ".join(map(str, v))
        lines.append(f"{k}: {v}")
    return "\n".join(lines)


def _render_markdown_snippet(text: str, max_lines: int = 8) -> str:
    """
    Take the heading and first few meaningful lines from markdown.
    """
    lines = [ln.rstrip() for ln in text.splitlines()]
    # keep non-empty lines; prefer headings/bullets first
    cleaned: List[str] = []
    for ln in lines:
        s = ln.strip()
        if not s:
            continue
        cleaned.append(s)
        if len(cleaned) >= max_lines:
            break
    return "\n".join(cleaned)


def _render_recent_qa(obj: Dict[str, Any]) -> str:
    q = str(obj.get("q", "")).strip()
    a = str(obj.get("a", "")).strip()
    return f"Q: {q}\nA: {a}"


# -------- Corpus iterator producing (text_for_embedding, meta_dict) --------

def iter_docs(root: Path) -> Iterable[Tuple[str, Dict[str, Any]]]:
    """
    Yields (text, meta) pairs. meta includes:
      - doc_id: file path relative to dataset root (e.g., 'policies/emergency.md', 'treatments.json')
      - section: semantic section id (e.g., 'TX-SCALE-01') or 'full' for whole docs, or timestamp for jsonl
      - path: doc_id#section (or doc_id when section == 'full')
      - type: md | json | jsonl
      - text: concise snippet for grounding (NOT the full document)
    The 'text' is also used as the embedding input.
    """
    # policies/*.md
    for md in (root / "policies").glob("*.md"):
        doc_id = f"policies/{md.name}"
        full = md.read_text(encoding="utf-8", errors="ignore")
        snippet = _render_markdown_snippet(full, max_lines=8)
        meta = {
            "doc_id": doc_id,
            "section": "full",
            "path": doc_id,
            "type": "md",
            "text": snippet,
        }
        yield snippet, meta

    # faq.md
    faq_p = (root / "faq.md")
    if faq_p.exists():
        faq_txt = faq_p.read_text(encoding="utf-8", errors="ignore")
        snippet = _render_markdown_snippet(faq_txt, max_lines=10)
        meta = {
            "doc_id": "faq.md",
            "section": "full",
            "path": "faq.md",
            "type": "md",
            "text": snippet,
        }
        yield snippet, meta

    # treatments.json: one chunk per treatment, section = code (semantic id)
    tr_p = (root / "treatments.json")
    if tr_p.exists():
        treatments = json.loads(tr_p.read_text(encoding="utf-8"))
        if isinstance(treatments, list):
            for it in treatments:
                # prefer semantic section id 'code' (e.g., TX-SCALE-01)
                code = it.get("code") or "item"
                snippet = _render_treatment_item(it)
                meta = {
                    "doc_id": "treatments.json",
                    "section": str(code),
                    "path": f"treatments.json#{code}",
                    "type": "json",
                    "text": snippet,
                }
                yield snippet, meta

    # recent_queries.jsonl: optional, include as weak signals (can downweight later)
    rq_p = (root / "recent_queries.jsonl")
    if rq_p.exists():
        for line in rq_p.read_text(encoding="utf-8", errors="ignore").splitlines():
            if not line.strip():
                continue
            try:
                obj = json.loads(line)
            except Exception:
                continue
            snippet = _render_recent_qa(obj)
            ts = str(obj.get("ts", "na"))
            meta = {
                "doc_id": "recent_queries.jsonl",
                "section": ts,
                "path": f"recent_queries.jsonl:{ts}",
                "type": "jsonl",
                "text": snippet,
            }
            yield snippet, meta


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--root", required=True, help="datasets/clinic")
    ap.add_argument("--outdir", required=True, help="artifacts/rag")
    args = ap.parse_args()

    root = Path(args.root)
    out = Path(args.outdir)
    out.mkdir(parents=True, exist_ok=True)

    # Build corpus
    texts: List[str] = []
    metas: List[Dict[str, Any]] = []
    for txt, meta in iter_docs(root):
        # keep text modest to avoid huge meta and keep embeddings focused
        txt_capped = txt.strip()[:1500]
        texts.append(txt_capped)
        # store the same snippet in meta (so retriever can return it directly)
        meta = dict(meta)
        meta["text"] = txt_capped
        metas.append(meta)

    # Embed and index
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    model = SentenceTransformer(model_name)
    embs = model.encode(
        texts,
        convert_to_numpy=True,
        show_progress_bar=True,
        normalize_embeddings=True,
    )
    index = faiss.IndexFlatIP(embs.shape[1])
    index.add(embs)

    # Persist
    faiss.write_index(index, str(out / "index.faiss"))
    (out / "meta.json").write_text(
        json.dumps(metas, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )
    print(f"Indexed {len(texts)} chunks → {out}")


if __name__ == "__main__":
    main()
```

### `rag/retriever.py`

```
import os
import json
from pathlib import Path
from typing import List, Optional, Tuple, Any

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

BACKEND   = os.getenv("BACKEND", "dense")  # "sparse" or "dense"
INDEX_PATH = Path(os.getenv("INDEX_PATH", "/mnt/project/atharva-dental-assistant/artifacts/rag/index.faiss"))
META_PATH  = Path(os.getenv("META_PATH",  "/mnt/project/atharva-dental-assistant/artifacts/rag/meta.json"))
MODEL_DIR  = os.getenv("MODEL_DIR")  # optional for dense
MODEL_NAME = os.getenv("MODEL_NAME", "sentence-transformers/all-MiniLM-L6-v2")

app = FastAPI(title=f"Atharva Retriever ({BACKEND})")

class SearchRequest(BaseModel):
    query: str
    k: int = 4

_ready_reason = "starting"
_model = None; _index = None; _meta: List[dict] = []
_vec = None; _X = None  # sparse objects


# ------------------ Utils ------------------

def _normalize_meta_loaded(data: Any) -> List[dict]:
    """
    Accepts various shapes of meta.json and returns a list of entries.
    Supported:
      - list[dict]
      - {"items": [...]}  (common pattern)
      - {"hits": [...]}   (fallback)
    """
    if isinstance(data, list):
        return data
    if isinstance(data, dict):
        if "items" in data and isinstance(data["items"], list):
            return data["items"]
        if "hits" in data and isinstance(data["hits"], list):
            return data["hits"]
    raise ValueError("META_PATH must contain a list or a dict with 'items'/'hits'.")


def _parse_doc_and_section(path: Optional[str]) -> Tuple[str, Optional[str]]:
    """
    Parse labels from meta.path:
      - 'treatments.json#0' -> ('treatments.json', '0')
      - 'faq.md'            -> ('faq.md', None)
      - 'policies/emergency.md' -> ('policies/emergency.md', None)
    """
    if not path:
        return "unknown", None
    if "#" in path:
        d, s = path.split("#", 1)
        return d, s
    return path, None


def _extract_text(m: dict) -> Optional[str]:
    """
    Try common keys for stored chunk text.
    """
    return m.get("text") or m.get("chunk") or m.get("content")


def _enrich_hit(idx: int, score: float) -> dict:
    """
    Build a single enriched hit from meta[idx].
    """
    if idx < 0 or idx >= len(_meta):
        # Guard against out-of-range
        doc_id, section, path, typ, txt = "unknown", None, None, None, None
    else:
        m   = _meta[idx] or {}
        path = m.get("path")
        typ  = m.get("type")
        doc_id, section = _parse_doc_and_section(path)
        txt = _extract_text(m)

    hit = {
        "score": float(score),
        "meta": {
            "doc_id": doc_id,
            "section": section,
            "path": path,
            "type": typ,
        },
    }
    if txt:
        hit["text"] = txt
    return hit


# ------------------ Loaders ------------------

def _load_dense():
    global _model, _index, _meta
    try:
        import faiss
        from sentence_transformers import SentenceTransformer
        _model = SentenceTransformer(MODEL_DIR) if (MODEL_DIR and Path(MODEL_DIR).exists()) else SentenceTransformer(MODEL_NAME)
        _index = faiss.read_index(str(INDEX_PATH))
        _meta = _normalize_meta_loaded(json.loads(META_PATH.read_text(encoding="utf-8")))
        return None
    except Exception as e:
        return f"dense load error: {e}"


def _load_sparse():
    global _vec, _X, _meta
    try:
        import joblib
        from scipy import sparse
        vec_p = Path(os.getenv("VEC_PATH", "/mnt/project/atharva-dental-assistant/artifacts/rag/tfidf_vectorizer.joblib"))
        X_p   = Path(os.getenv("MAT_PATH", "/mnt/project/atharva-dental-assistant/artifacts/rag/tfidf_matrix.npz"))
        _vec = joblib.load(vec_p)
        _X = sparse.load_npz(X_p)  # assume rows L2-normalized; dot == cosine
        _meta = _normalize_meta_loaded(json.loads(META_PATH.read_text(encoding="utf-8")))
        return None
    except Exception as e:
        return f"sparse load error: {e}"


@app.on_event("startup")
def startup():
    global _ready_reason
    _ready_reason = _load_sparse() if BACKEND == "sparse" else _load_dense()


# ------------------ Endpoints ------------------

@app.get("/health")
def health():
    return {"ok": True}


@app.get("/ready")
def ready():
    return {"ready": _ready_reason is None, "reason": _ready_reason}


@app.post("/reload")
def reload_index():
    global _ready_reason
    _ready_reason = _load_sparse() if BACKEND == "sparse" else _load_dense()
    if _ready_reason is not None:
        raise HTTPException(status_code=503, detail=_ready_reason)
    return {"reloaded": True}


@app.post("/search")
def search(req: SearchRequest):
    if _ready_reason is not None:
        raise HTTPException(status_code=503, detail=_ready_reason)

    k = max(1, min(int(req.k), 20))

    if BACKEND == "sparse":
        import numpy as np
        q = _vec.transform([req.query])
        scores = (_X @ q.T).toarray().ravel()  # cosine since rows normalized
        if scores.size == 0:
            return {"hits": []}
        # get top-k indices by score desc
        k_eff = min(k, scores.size)
        top = np.argpartition(-scores, range(k_eff))[:k_eff]
        top = top[np.argsort(-scores[top])]
        hits = [
            _enrich_hit(int(i), float(scores[int(i)]))
            for i in top
            if scores[int(i)] > 0
        ]
        return {"hits": hits}

    # dense
    import faiss
    import numpy as np
    v = _model.encode([req.query], normalize_embeddings=True)  # IP ~ cosine
    D, I = _index.search(v.astype("float32"), k)
    hits = []
    for score, idx in zip(D[0].tolist(), I[0].tolist()):
        if idx == -1:
            continue
        hits.append(_enrich_hit(int(idx), float(score)))
    return {"hits": hits}
```

---

## 4) Kubernetes manifests (Jobs + Deployment)

### `k8s/10-data/job-generate-data.yaml`

```
apiVersion: batch/v1
kind: Job
metadata:
  name: atharva-generate-data
  namespace: atharva-ml
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: synth
        image: python:3.11-slim
        command: ["bash","-lc"]
        args:
          - |
            pip install --no-cache-dir sentence-transformers==2.7.0
            python /mnt/project/atharva-dental-assistant/tools/synth_data.py \
              --clinic Pune --currency INR \
              --treatments /mnt/project/atharva-dental-assistant/datasets/clinic/treatments.json \
              --policies /mnt/project/atharva-dental-assistant/datasets/clinic/policies/*.md \
              --faq /mnt/project/atharva-dental-assistant/datasets/clinic/faq.md \
              --recent /mnt/project/atharva-dental-assistant/datasets/clinic/recent_queries.jsonl \
              --out /mnt/project/atharva-dental-assistant/datasets/training
        volumeMounts:
        - name: host
          mountPath: /mnt/project
      volumes:
      - name: host
        hostPath: { path: /mnt/project, type: Directory }
```

### `k8s/10-data/job-build-index.yaml`

```
apiVersion: batch/v1
kind: Job
metadata:
  name: atharva-build-index
  namespace: atharva-ml
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: index
        image: public.ecr.aws/docker/library/python:3.11-slim
        command: ["bash","-lc"]
        args:
          - |
            set -euo pipefail
            export HOME=/mnt/project
            VENV=/mnt/project/.venv-build
            ROOT=/mnt/project/atharva-dental-assistant/datasets/clinic
            OUT=/mnt/project/atharva-dental-assistant/artifacts/rag
            mkdir -p "$OUT"
            python -m venv "$VENV"
            . "$VENV/bin/activate"
            python -m pip install -U pip
            python -m pip install --no-cache-dir "numpy==1.26.4" "scipy==1.10.1" "scikit-learn==1.3.2" "joblib==1.3.2"

            python - << 'PY'
            from pathlib import Path
            import json, re
            from typing import Any, Dict, List, Tuple
            import numpy as np
            from sklearn.feature_extraction.text import TfidfVectorizer
            from scipy import sparse
            import joblib

            ROOT   = Path("/mnt/project/atharva-dental-assistant/datasets/clinic")
            OUTDIR = Path("/mnt/project/atharva-dental-assistant/artifacts/rag")
            OUTDIR.mkdir(parents=True, exist_ok=True)

            # ---- helpers to render concise snippets ----
            def render_markdown_snippet(txt: str, max_lines: int = 8) -> str:
                lines = [ln.strip() for ln in txt.splitlines()]
                lines = [ln for ln in lines if ln]
                return "\n".join(lines[:max_lines])

            def render_treatment_item(it: Dict[str, Any]) -> str:
                keys = ("code","name","category","duration_minutes","visits","price_band_inr",
                        "indications","steps","aftercare","risks")
                parts = []
                for k in keys:
                    if k in it:
                        v = it[k]
                        if isinstance(v, (list, tuple)):
                            v = ", ".join(map(str, v))
                        parts.append(f"{k}: {v}")
                return "\n".join(parts)

            def render_recent_qa(obj: Dict[str, Any]) -> str:
                q = str(obj.get("q","")).strip()
                a = str(obj.get("a","")).strip()
                return f"Q: {q}\nA: {a}"

            texts: List[str] = []
            meta:  List[Dict[str, Any]] = []

            # policies/*.md
            for p in sorted((ROOT/"policies").glob("*.md")):
                t = p.read_text(encoding="utf-8", errors="ignore")
                snip = render_markdown_snippet(t, max_lines=8)
                snip = snip.strip()[:1500]
                doc_id = f"policies/{p.name}"
                texts.append(snip)
                meta.append({
                    "doc_id": doc_id,
                    "section": "full",
                    "path": doc_id,
                    "type": "md",
                    "text": snip,
                })

            # faq.md
            faq = ROOT/"faq.md"
            if faq.exists():
                t = faq.read_text(encoding="utf-8", errors="ignore")
                snip = render_markdown_snippet(t, max_lines=10).strip()[:1500]
                texts.append(snip)
                meta.append({
                    "doc_id": "faq.md",
                    "section": "full",
                    "path": "faq.md",
                    "type": "md",
                    "text": snip,
                })

            # treatments.json (semantic section id = code)
            tr = ROOT/"treatments.json"
            if tr.exists():
                items = json.loads(tr.read_text(encoding="utf-8"))
                if isinstance(items, list):
                    for it in items:
                        code = it.get("code") or "item"
                        snip = render_treatment_item(it).strip()[:1500]
                        texts.append(snip)
                        meta.append({
                            "doc_id": "treatments.json",
                            "section": str(code),
                            "path": f"treatments.json#{code}",
                            "type": "json",
                            "text": snip,
                        })

            # recent_queries.jsonl (optional, weak source)
            rq = ROOT/"recent_queries.jsonl"
            if rq.exists():
                for line in rq.read_text(encoding="utf-8", errors="ignore").splitlines():
                    line=line.strip()
                    if not line: continue
                    try:
                        obj = json.loads(line)
                    except Exception:
                        continue
                    ts = str(obj.get("ts","na"))
                    snip = render_recent_qa(obj).strip()[:1500]
                    texts.append(snip)
                    meta.append({
                        "doc_id": "recent_queries.jsonl",
                        "section": ts,
                        "path": f"recent_queries.jsonl:{ts}",
                        "type": "jsonl",
                        "text": snip,
                    })

            if not texts:
                raise SystemExit(f"No ingestible files in {ROOT}")

            # Build sparse TF-IDF
            vec = TfidfVectorizer(
                lowercase=True,
                ngram_range=(1,2),
                max_df=0.9,
                min_df=1,
                norm="l2",
            )
            X = vec.fit_transform(texts).astype(np.float32)

            # Save artifacts
            joblib.dump(vec, OUTDIR/"tfidf_vectorizer.joblib")
            sparse.save_npz(OUTDIR/"tfidf_matrix.npz", X)
            (OUTDIR/"meta.json").write_text(
                json.dumps(meta, ensure_ascii=False, indent=2),
                encoding="utf-8"
            )

            print("TF-IDF built:", X.shape, "saved to", OUTDIR)
            PY

            ls -lah "$OUT" && wc -c "$OUT"/tfidf_* "$OUT"/meta.json
        volumeMounts:
        - name: host
          mountPath: /mnt/project
      volumes:
      - name: host
        hostPath: { path: /mnt/project, type: Directory }
```

### `k8s/40-serve/deploy-retriever.yaml`

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: atharva-retriever
  namespace: atharva-ml
  labels: { app: retriever }
spec:
  replicas: 1
  selector:
    matchLabels: { app: retriever }
  template:
    metadata:
      labels: { app: retriever }
    spec:
      terminationGracePeriodSeconds: 20
      containers:
      - name: api
        image: public.ecr.aws/docker/library/python:3.11-slim
        imagePullPolicy: IfNotPresent
        workingDir: /mnt/project/atharva-dental-assistant
        env:
        - { name: HOME, value: /mnt/project }
        - { name: VIRTUAL_ENV, value: /opt/venv }   # venv on emptyDir
        - { name: PATH, value: /opt/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin }
        - { name: PYTHONPATH, value: /mnt/project/atharva-dental-assistant }
        - { name: HF_HOME, value: /mnt/project/.hf_cache }
        - { name: TOKENIZERS_PARALLELISM, value: "false" }
        - { name: INDEX_PATH, value: /mnt/project/atharva-dental-assistant/artifacts/rag/index.faiss }
        - { name: META_PATH,  value: /mnt/project/atharva-dental-assistant/artifacts/rag/meta.json }
        - { name: MODEL_NAME, value: sentence-transformers/all-MiniLM-L6-v2 }
        - { name: BACKEND, value: sparse }          # using sparse now
        - { name: PIP_DISABLE_PIP_VERSION_CHECK, value: "1" }
        - { name: PIP_NO_CACHE_DIR, value: "1" }
        - { name: PIP_CACHE_DIR, value: /opt/tmp/pip }
        - { name: TMPDIR, value: /opt/tmp }
        - { name: PIP_ONLY_BINARY, value: ":all:" } # ← do NOT build from source
        ports:
        - { name: http, containerPort: 8001 }
        command: ["bash","-lc"]
        args:
          - |
            set -euo pipefail
            python -m venv "$VIRTUAL_ENV"
            . "$VIRTUAL_ENV/bin/activate"
            python -m pip install --upgrade pip
            if [ "${BACKEND:-sparse}" = "sparse" ]; then
              # Pure sparse stack (forces wheels only; will fail if a wheel is unavailable for your arch)
              python -m pip install --only-binary=:all: \
                "numpy==1.26.4" "scipy==1.10.1" "scikit-learn==1.3.2" joblib==1.4.2 \
                fastapi==0.112.2 uvicorn==0.30.6
            else
              python -m pip install --only-binary=:all: \
                "numpy==1.26.4" sentence-transformers==2.7.0 "faiss-cpu==1.7.4" \
                fastapi==0.112.2 uvicorn==0.30.6
            fi
            uvicorn rag.retriever:app --host 0.0.0.0 --port 8001
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
            ephemeral-storage: "2Gi"   # ↑ reserve more scratch
          limits:
            cpu: "2"
            memory: "4Gi"
            ephemeral-storage: "6Gi"   # ↑ allow larger wheels
        volumeMounts:
        - { name: host,  mountPath: /mnt/project }
        - { name: venv,  mountPath: /opt/venv }
        - { name: tmp,   mountPath: /opt/tmp }
      volumes:
      - name: host
        hostPath: { path: /mnt/project, type: Directory }
      - name: venv
        emptyDir:
          sizeLimit: 3Gi              # ↑ allow the venv to exist
      - name: tmp
        emptyDir:
          sizeLimit: 3Gi              # ↑ pip/tmp workspace
```

### `k8s/40-serve/svc-retriever.yaml`

```
apiVersion: v1
kind: Service
metadata:
  name: atharva-retriever
  namespace: atharva-ml
spec:
  type: NodePort
  selector: { app: retriever }
  ports:
  - name: http
    port: 8001
    targetPort: 8001
    nodePort: 30100
```

---

## 5) Helper scripts (local convenience)

### `scripts/generate_data.sh`

```
#!/usr/bin/env bash
set -euo pipefail
kubectl apply -f k8s/10-data/job-generate-data.yaml
kubectl -n atharva-ml wait --for=condition=complete job/atharva-generate-data --timeout=300s
kubectl -n atharva-ml logs job/atharva-generate-data
```

### `scripts/build_index.sh`

```
#!/usr/bin/env bash
set -euo pipefail
kubectl apply -f k8s/10-data/job-build-index.yaml
kubectl -n atharva-ml wait --for=condition=complete job/atharva-build-index --timeout=300s
kubectl -n atharva-ml logs job/atharva-build-index
```

### `scripts/deploy_retriever.sh`

```
#!/usr/bin/env bash
set -euo pipefail
kubectl apply -f k8s/40-serve/deploy-retriever.yaml
kubectl apply -f k8s/40-serve/svc-retriever.yaml
kubectl -n atharva-ml rollout status deploy/atharva-retriever --timeout=180s
kubectl -n atharva-ml get svc/atharva-retriever
```

> **Windows (PowerShell)**: you can run the same `kubectl apply`/`wait` commands directly; or convert the .sh scripts to `.ps1` with the same lines minus the `set -euo pipefail`.

---

## 6) Run the lab

From the repo root (which lives under `./project/atharva-dental-assistant` on your host):

```
# 1) Generate training/eval datasets
bash scripts/generate_data.sh

# validate 
kubectl get pods
ls datasets/training

# 2) Build FAISS index
bash scripts/build_index.sh

# validate
kubectl get pods 
ls artifacts/rag

# 3) Deploy retriever API
bash scripts/deploy_retriever.sh

# validate
kubectl get all
kubectl logs -f -l "app=retriever" 
```

Send some requests to the retriever to validate if RAG is working

```

# Is retriever healthy ?
curl -s http://127.0.0.1:30100/health

# Reload once after rebuilding the index to clear any cached startup error
curl -s -X POST http://127.0.0.1:30100/reload | jq .


# 0) Ready before search?
curl -s http://127.0.0.1:30100/ready | jq .


# 1) Run a query and save it
curl -s -X POST http://127.0.0.1:30100/search \
  -H 'content-type: application/json' \
  -d '{"query":"How long does scaling take?","k":4}' | tee /tmp/r.json | jq .


# 2) hits length <= k
jq '(.hits|length) <= 4' /tmp/r.json

# 3) every hit has score+meta
jq -e 'all(.hits[]; has("score") and has("meta"))' /tmp/r.json

# 4) scores are sorted non-increasing
jq -e '[.hits[].score] as $s | reduce range(1; $s|length) as $i (true; . and ($s[$i] <= $s[$i-1]))' /tmp/r.json

# 5) score range sanity
# use ONE of these depending on backend:
#   sparse TF-IDF: scores in [0,1]
jq -e 'all(.hits[].score; .>=0 and .<=1)' /tmp/r.json
#   dense cosine/IP (normalized): scores in [-1,1]
# jq -e 'all(.hits[].score; .>=-1 and .<=1)' /tmp/r.json


# 6) idempotence (same request twice → same top doc paths)
curl -s -X POST http://127.0.0.1:30100/search \
  -H 'content-type: application/json' \
  -d '{"query":"How long does scaling take?","k":4}' \
  | jq '[.hits[].meta.path]' > /tmp/r2.json
diff -u /tmp/r2.json <(jq '[.hits[].meta.path]' /tmp/r.json) || true


# 8) latency snapshot (should be sub-second after warmup for small corpora)
curl -o /dev/null -s -w 'time_total=%{time_total}\n' \
  -X POST http://127.0.0.1:30100/search \
  -H 'content-type: application/json' \
  -d '{"query":"root canal","k":4}'

# Test a nonsense query. Should return empty results
curl -s -X POST http://127.0.0.1:30100/search \
  -H 'content-type: application/json' \
  -d '{"query":"zzzzzz qwerty asdf","k":4}' | jq '.hits'

# Search for a specific policies from the dataset 
curl -s -X POST http://127.0.0.1:30100/search \
  -H 'content-type: application/json' \
  -d '{"query":"Whats the currency you accept?","k":4}'

curl -s -X POST http://127.0.0.1:30100/search \
  -H 'content-type: application/json' \
  -d '{"query":"Are you open on Sundays?","k":4}'

```

You should see top hits with `doc_id` like `treatments.json#TX-SCALE-01` and policies.

---

## Lab Summary 

This is what we accomplished in this lab

* Created **synthetic, Pune/INR-aware** clinic data (treatments, policies, FAQs, recent queries).

* Built a **FAISS** index (CPU) with **MiniLM-L6-v2** embeddings.

* Deployed a **Retriever API** on Kubernetes that returns top-k hits w/ metadata.

* Proved the **RAG backbone** works before any model fine-tuning.



#courses/llmops/labs/v1
