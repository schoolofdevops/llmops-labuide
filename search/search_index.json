{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LLMOps with Kubernetes Hands-on labs to fine-tune, deploy, observe, and scale GenAI workloads on Kubernetes \u2014 from zero to production. This course walks you through building a complete LLMOps workflow for the Atharva Dental Clinic Assistant \u2014 a domain-adapted chatbot powered by Retrieval-Augmented Generation (RAG) and a LoRA-fine-tuned LLM. You\u2019ll start from a blank KIND cluster and progress all the way to a fully automated, observable, and scalable GenAI system powered by KServe, vLLM, Prometheus, and ArgoCD. What You\u2019ll Learn Deploy and operate LLM-based applications on Kubernetes Generate synthetic domain data and build a FAISS retriever Fine-tune open LLMs with LoRA on CPU-only environments Package models as OCI artifacts and mount them via ImageVolumes Serve models using KServe (RawDeployment) + vLLM Instrument and visualize LLM observability metrics in Prometheus & Grafana Implement autoscaling (HPA, VPA, KEDA) for GenAI services Automate continuous delivery using ArgoCD GitOps Lab Index Each lab builds on the previous one, progressively assembling the full GenAI stack. # Lab Title Description Lab 00 \u2013 Setup Create a 3-node KIND cluster with ImageVolume feature gates and namespaces for ML, app, and monitoring workloads. Lab 01 \u2013 Synthetic Data + RAG System Generate synthetic clinic data, build a FAISS (or TF-IDF) index, and deploy the Retriever API on Kubernetes. Lab 02 \u2013 CPU LoRA Fine-tuning (SmolLM2-135M) Fine-tune a small open LLM on CPU using LoRA and merge adapters into a single model folder for serving. Lab 03 \u2013 Packaging Model as OCI Image Package the merged model into an OCI image and mount it using Kubernetes ImageVolumes. Lab 04 \u2013 Serving with KServe and vLLM Serve the model with vLLM (OpenAI-compatible) through KServe RawDeployment and connect it to the RAG retriever and Chat API. Lab 04.0 \u2013 Testing vLLM on macOS Optional: run the vLLM container locally on macOS to validate CPU-only inference before deploying to Kubernetes. Lab 05 \u2013 Observability with Prometheus + Grafana Instrument vLLM, Retriever, and Chat API with Prometheus metrics; build a Grafana dashboard for model and system insights. Lab 06 \u2013 Autoscaling vLLM + RAG + Chat API Implement CPU-based HPA, Prometheus-driven scaling via KEDA, and VPA recommendations for resource optimization. Lab 07 \u2013 GitOps for GenAI with ArgoCD Manage continuous delivery for all GenAI components using ArgoCD App-of-Apps and GitOps workflows. Lab 08 - Automating LLM pipelines with Argo Workflows Automate LLM Workflows with DAG Based Kubeflow alternative i.e. Argo Workflow. --- Tech Stack Kubernetes 1.34+ (KIND) \u2013 local multi-node cluster KServe RawDeployment + vLLM \u2013 model serving ImageVolumes \u2013 mount OCI model artifacts PEFT LoRA / SmolLM2-135M \u2013 CPU-friendly fine-tuning FAISS / TF-IDF Retriever \u2013 semantic search backbone Prometheus + Grafana \u2013 observability and metrics KEDA / HPA / VPA \u2013 intelligent autoscaling ArgoCD \u2013 GitOps-based continuous delivery How to Use This Guide Follow the labs in order \u2014 each lab builds on artifacts from previous ones. All commands are copy-paste-ready for macOS / Linux (KIND + kubectl + helm). Expect small CPU-only workloads suitable for laptops with \u2265 16 GB RAM. Each lab ends with a \u201cLab Summary\u201d to recap what you achieved. Author: Gourav Shah Project: School of DevOps \u2013 LLMOps Bootcamp: Kubernetes for GenAI Workloads License: CC BY-NC-SA \u00a9 2025 Gourav Shah \u00b7 School of DevOps","title":"Home"},{"location":"#llmops-with-kubernetes","text":"Hands-on labs to fine-tune, deploy, observe, and scale GenAI workloads on Kubernetes \u2014 from zero to production. This course walks you through building a complete LLMOps workflow for the Atharva Dental Clinic Assistant \u2014 a domain-adapted chatbot powered by Retrieval-Augmented Generation (RAG) and a LoRA-fine-tuned LLM. You\u2019ll start from a blank KIND cluster and progress all the way to a fully automated, observable, and scalable GenAI system powered by KServe, vLLM, Prometheus, and ArgoCD.","title":"LLMOps with Kubernetes"},{"location":"#what-youll-learn","text":"Deploy and operate LLM-based applications on Kubernetes Generate synthetic domain data and build a FAISS retriever Fine-tune open LLMs with LoRA on CPU-only environments Package models as OCI artifacts and mount them via ImageVolumes Serve models using KServe (RawDeployment) + vLLM Instrument and visualize LLM observability metrics in Prometheus & Grafana Implement autoscaling (HPA, VPA, KEDA) for GenAI services Automate continuous delivery using ArgoCD GitOps","title":"What You\u2019ll Learn"},{"location":"#lab-index","text":"Each lab builds on the previous one, progressively assembling the full GenAI stack. # Lab Title Description Lab 00 \u2013 Setup Create a 3-node KIND cluster with ImageVolume feature gates and namespaces for ML, app, and monitoring workloads. Lab 01 \u2013 Synthetic Data + RAG System Generate synthetic clinic data, build a FAISS (or TF-IDF) index, and deploy the Retriever API on Kubernetes. Lab 02 \u2013 CPU LoRA Fine-tuning (SmolLM2-135M) Fine-tune a small open LLM on CPU using LoRA and merge adapters into a single model folder for serving. Lab 03 \u2013 Packaging Model as OCI Image Package the merged model into an OCI image and mount it using Kubernetes ImageVolumes. Lab 04 \u2013 Serving with KServe and vLLM Serve the model with vLLM (OpenAI-compatible) through KServe RawDeployment and connect it to the RAG retriever and Chat API. Lab 04.0 \u2013 Testing vLLM on macOS Optional: run the vLLM container locally on macOS to validate CPU-only inference before deploying to Kubernetes. Lab 05 \u2013 Observability with Prometheus + Grafana Instrument vLLM, Retriever, and Chat API with Prometheus metrics; build a Grafana dashboard for model and system insights. Lab 06 \u2013 Autoscaling vLLM + RAG + Chat API Implement CPU-based HPA, Prometheus-driven scaling via KEDA, and VPA recommendations for resource optimization. Lab 07 \u2013 GitOps for GenAI with ArgoCD Manage continuous delivery for all GenAI components using ArgoCD App-of-Apps and GitOps workflows. Lab 08 - Automating LLM pipelines with Argo Workflows Automate LLM Workflows with DAG Based Kubeflow alternative i.e. Argo Workflow. ---","title":"Lab Index"},{"location":"#tech-stack","text":"Kubernetes 1.34+ (KIND) \u2013 local multi-node cluster KServe RawDeployment + vLLM \u2013 model serving ImageVolumes \u2013 mount OCI model artifacts PEFT LoRA / SmolLM2-135M \u2013 CPU-friendly fine-tuning FAISS / TF-IDF Retriever \u2013 semantic search backbone Prometheus + Grafana \u2013 observability and metrics KEDA / HPA / VPA \u2013 intelligent autoscaling ArgoCD \u2013 GitOps-based continuous delivery","title":"Tech Stack"},{"location":"#how-to-use-this-guide","text":"Follow the labs in order \u2014 each lab builds on artifacts from previous ones. All commands are copy-paste-ready for macOS / Linux (KIND + kubectl + helm). Expect small CPU-only workloads suitable for laptops with \u2265 16 GB RAM. Each lab ends with a \u201cLab Summary\u201d to recap what you achieved. Author: Gourav Shah Project: School of DevOps \u2013 LLMOps Bootcamp: Kubernetes for GenAI Workloads License: CC BY-NC-SA \u00a9 2025 Gourav Shah \u00b7 School of DevOps","title":"How to Use This Guide"},{"location":"lab00/","text":"Lab 0 - Setup a 3 Node Kubernetes Cluster This lab gets you a KIND cluster that\u2019s ready for ImageVolumes and KServe (RawDeployment) that you would setup and use later, plus namespaces and monitoring. \ud83d\udd0e Notes \u2022 ImageVolumes (mounting an OCI image as a read-only volume) require K8s v1.31+ , are beta in v1.33 and disabled by default ; you must enable the ImageVolume feature gate and use a runtime that supports it. ( Kubernetes ) Create a project directory mkdir project cd project mkdir atharva-dental-assistant cd atharva-dental-assistant mkdir scripts setup Install Supporting Tools Install kind : https://kind.sigs.k8s.io/docs/user/quick-start/#installation . Install kubectl: https://kubernetes.io/docs/tasks/tools/ . Install HELM: https://helm.sh/docs/intro/install/ . setup/kind-config.yaml Create this file to: (1) pin a recent node image, (2) enable the ImageVolume feature gate across control-plane components and kubelet, and (3) mount your local ./project directory into each node. # Enable ImageVolume feature gate (beta, disabled-by-default as of v1.33+) # Ref: \"Use an Image Volume With a Pod\" + Feature Gates docs. kubeadmConfigPatches: - | apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration apiServer: extraArgs: feature-gates: \"ImageVolume=true\" controllerManager: extraArgs: feature-gates: \"ImageVolume=true\" scheduler: extraArgs: feature-gates: \"ImageVolume=true\" - | apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration featureGates: ImageVolume: true # three node (two workers) cluster config kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: llmops-kind nodes: - role: control-plane image: kindest/node:v1.34.0 extraMounts: - hostPath: /Users/gshah/work/llmops/code/project containerPath: /mnt/project extraPortMappings: - containerPort: 32000 hostPort: 32000 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 32100 hostPort: 32100 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30000 hostPort: 30000 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30055 hostPort: 30055 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30056 hostPort: 30056 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30100 hostPort: 30100 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30200 hostPort: 30200 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30300 hostPort: 30300 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30400 hostPort: 30400 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30500 hostPort: 30500 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30600 hostPort: 30600 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30700 hostPort: 30700 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30800 hostPort: 30800 listenAddress: \"0.0.0.0\" protocol: tcp - role: worker image: kindest/node:v1.34.0 extraMounts: - hostPath: /Users/gshah/work/llmops/code/project containerPath: /mnt/project extraPortMappings: - containerPort: 80 hostPort: 80 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 8000 hostPort: 8000 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 8001 hostPort: 8001 listenAddress: \"0.0.0.0\" protocol: tcp - role: worker image: kindest/node:v1.34.0 extraMounts: - hostPath: /Users/gshah/work/llmops/code/project containerPath: /mnt/project You must replace /Users/gshah/work/llmops/code/project with the actual path. * In case of windows just use ./project * On MacOS replace it with absolute path e.g. /Users/xxxx/work/llmops/code/project Why the gates here? The docs specify enabling the ImageVolume feature gate and using a runtime that supports it; we toggle it for apiserver/controller/scheduler and kubelet to be thorough. ( Kubernetes ) scripts/bootstrap_kind.sh This script: creates the cluster, namespaces, installs KServe (RawDeployment) , and the kube-prometheus-stack (Prometheus + Grafana) into monitoring . #!/usr/bin/env bash set -euo pipefail CLUSTER_NAME=\"llmops-kind\" KIND_CONFIG=\"setup/kind-config.yaml\" MON_NS=\"monitoring\" echo \"==> Preflight checks\" command -v kind >/dev/null 2>&1 || { echo \"Please install kind\"; exit 1; } command -v kubectl >/dev/null 2>&1 || { echo \"Please install kubectl\"; exit 1; } command -v helm >/dev/null 2>&1 || { echo \"Please install helm\"; exit 1; } mkdir -p project echo \"==> Creating KIND cluster (${CLUSTER_NAME}) with ImageVolume feature-gate enabled\" kind create cluster --name \"${CLUSTER_NAME}\" --config \"${KIND_CONFIG}\" echo \"==> Verifying Kubernetes server version\" SERVER_MINOR=$(kubectl version -o json | jq -r '.serverVersion.minor' | sed 's/[^0-9].*//') SERVER_MAJOR=$(kubectl version -o json | jq -r '.serverVersion.major') echo \"Server version: ${SERVER_MAJOR}.${SERVER_MINOR}\" # ImageVolumes need >= 1.31; KServe Quickstart needs >= 1.29 if [ \"${SERVER_MAJOR}\" -lt 1 ] || [ \"${SERVER_MINOR}\" -lt 31 ]; then echo \"ERROR: Kubernetes >=1.31 required for ImageVolumes. Current: ${SERVER_MAJOR}.${SERVER_MINOR}\" exit 1 fi echo \"==> Creating namespaces\" kubectl apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: { name: atharva-ml } --- apiVersion: v1 kind: Namespace metadata: { name: atharva-app } --- apiVersion: v1 kind: Namespace metadata: { name: ${MON_NS} } EOF echo \"==> (Optional) Check container runtime inside nodes for ImageVolume support\" CONTROL_NODE=$(kubectl get nodes -o name | head -n1 | sed 's|node/||') docker exec \"${CLUSTER_NAME}-control-plane\" containerd --version || true echo \"==> All set! Namespaces: - atharva-ml (ML training & model artifacts) - atharva-app (chat API / frontend) - monitoring (Prometheus + Grafana) Next: \u2022 Lab 1 will generate synthetic data and build the FAISS index. \" Execute the script with bash scripts/bootstrap_kind.sh validate the cluster using # Validate the nodes are listed Ready kubectl get nodes # Validate the pods are running kubectl get pods -A # Validate namespaces atharva-ml, atharva-app and monitoring are created kubectl get ns Lab Summary This is what we accomplished in this lab A KIND cluster using K8s v1.34 node images to comfortably meet ImageVolumes\u2019 version requirement. ( Docker Hub ) ImageVolume feature gate enabled at control-plane and kubelet level. Docs note it\u2019s beta and off by default ; we turned it on. ( Kubernetes )","title":"Lab 00 - Setting up Kubernetes Environment"},{"location":"lab00/#lab-0-setup-a-3-node-kubernetes-cluster","text":"This lab gets you a KIND cluster that\u2019s ready for ImageVolumes and KServe (RawDeployment) that you would setup and use later, plus namespaces and monitoring. \ud83d\udd0e Notes \u2022 ImageVolumes (mounting an OCI image as a read-only volume) require K8s v1.31+ , are beta in v1.33 and disabled by default ; you must enable the ImageVolume feature gate and use a runtime that supports it. ( Kubernetes ) Create a project directory mkdir project cd project mkdir atharva-dental-assistant cd atharva-dental-assistant mkdir scripts setup","title":"Lab 0 - Setup a 3 Node Kubernetes Cluster"},{"location":"lab00/#install-supporting-tools","text":"Install kind : https://kind.sigs.k8s.io/docs/user/quick-start/#installation . Install kubectl: https://kubernetes.io/docs/tasks/tools/ . Install HELM: https://helm.sh/docs/intro/install/ .","title":"Install Supporting Tools"},{"location":"lab00/#setupkind-configyaml","text":"Create this file to: (1) pin a recent node image, (2) enable the ImageVolume feature gate across control-plane components and kubelet, and (3) mount your local ./project directory into each node. # Enable ImageVolume feature gate (beta, disabled-by-default as of v1.33+) # Ref: \"Use an Image Volume With a Pod\" + Feature Gates docs. kubeadmConfigPatches: - | apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration apiServer: extraArgs: feature-gates: \"ImageVolume=true\" controllerManager: extraArgs: feature-gates: \"ImageVolume=true\" scheduler: extraArgs: feature-gates: \"ImageVolume=true\" - | apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration featureGates: ImageVolume: true # three node (two workers) cluster config kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: llmops-kind nodes: - role: control-plane image: kindest/node:v1.34.0 extraMounts: - hostPath: /Users/gshah/work/llmops/code/project containerPath: /mnt/project extraPortMappings: - containerPort: 32000 hostPort: 32000 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 32100 hostPort: 32100 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30000 hostPort: 30000 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30055 hostPort: 30055 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30056 hostPort: 30056 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30100 hostPort: 30100 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30200 hostPort: 30200 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30300 hostPort: 30300 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30400 hostPort: 30400 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30500 hostPort: 30500 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30600 hostPort: 30600 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30700 hostPort: 30700 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 30800 hostPort: 30800 listenAddress: \"0.0.0.0\" protocol: tcp - role: worker image: kindest/node:v1.34.0 extraMounts: - hostPath: /Users/gshah/work/llmops/code/project containerPath: /mnt/project extraPortMappings: - containerPort: 80 hostPort: 80 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 8000 hostPort: 8000 listenAddress: \"0.0.0.0\" protocol: tcp - containerPort: 8001 hostPort: 8001 listenAddress: \"0.0.0.0\" protocol: tcp - role: worker image: kindest/node:v1.34.0 extraMounts: - hostPath: /Users/gshah/work/llmops/code/project containerPath: /mnt/project You must replace /Users/gshah/work/llmops/code/project with the actual path. * In case of windows just use ./project * On MacOS replace it with absolute path e.g. /Users/xxxx/work/llmops/code/project Why the gates here? The docs specify enabling the ImageVolume feature gate and using a runtime that supports it; we toggle it for apiserver/controller/scheduler and kubelet to be thorough. ( Kubernetes )","title":"setup/kind-config.yaml"},{"location":"lab00/#scriptsbootstrap_kindsh","text":"This script: creates the cluster, namespaces, installs KServe (RawDeployment) , and the kube-prometheus-stack (Prometheus + Grafana) into monitoring . #!/usr/bin/env bash set -euo pipefail CLUSTER_NAME=\"llmops-kind\" KIND_CONFIG=\"setup/kind-config.yaml\" MON_NS=\"monitoring\" echo \"==> Preflight checks\" command -v kind >/dev/null 2>&1 || { echo \"Please install kind\"; exit 1; } command -v kubectl >/dev/null 2>&1 || { echo \"Please install kubectl\"; exit 1; } command -v helm >/dev/null 2>&1 || { echo \"Please install helm\"; exit 1; } mkdir -p project echo \"==> Creating KIND cluster (${CLUSTER_NAME}) with ImageVolume feature-gate enabled\" kind create cluster --name \"${CLUSTER_NAME}\" --config \"${KIND_CONFIG}\" echo \"==> Verifying Kubernetes server version\" SERVER_MINOR=$(kubectl version -o json | jq -r '.serverVersion.minor' | sed 's/[^0-9].*//') SERVER_MAJOR=$(kubectl version -o json | jq -r '.serverVersion.major') echo \"Server version: ${SERVER_MAJOR}.${SERVER_MINOR}\" # ImageVolumes need >= 1.31; KServe Quickstart needs >= 1.29 if [ \"${SERVER_MAJOR}\" -lt 1 ] || [ \"${SERVER_MINOR}\" -lt 31 ]; then echo \"ERROR: Kubernetes >=1.31 required for ImageVolumes. Current: ${SERVER_MAJOR}.${SERVER_MINOR}\" exit 1 fi echo \"==> Creating namespaces\" kubectl apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: { name: atharva-ml } --- apiVersion: v1 kind: Namespace metadata: { name: atharva-app } --- apiVersion: v1 kind: Namespace metadata: { name: ${MON_NS} } EOF echo \"==> (Optional) Check container runtime inside nodes for ImageVolume support\" CONTROL_NODE=$(kubectl get nodes -o name | head -n1 | sed 's|node/||') docker exec \"${CLUSTER_NAME}-control-plane\" containerd --version || true echo \"==> All set! Namespaces: - atharva-ml (ML training & model artifacts) - atharva-app (chat API / frontend) - monitoring (Prometheus + Grafana) Next: \u2022 Lab 1 will generate synthetic data and build the FAISS index. \" Execute the script with bash scripts/bootstrap_kind.sh validate the cluster using # Validate the nodes are listed Ready kubectl get nodes # Validate the pods are running kubectl get pods -A # Validate namespaces atharva-ml, atharva-app and monitoring are created kubectl get ns","title":"scripts/bootstrap_kind.sh"},{"location":"lab00/#lab-summary","text":"This is what we accomplished in this lab A KIND cluster using K8s v1.34 node images to comfortably meet ImageVolumes\u2019 version requirement. ( Docker Hub ) ImageVolume feature gate enabled at control-plane and kubelet level. Docs note it\u2019s beta and off by default ; we turned it on. ( Kubernetes )","title":"Lab Summary"},{"location":"lab01/","text":"Lab 1: Synthetic data + RAG system (CPU-only) Below are copy-pasteable files + commands so you can run this end-to-end on the KIND cluster you set up in Lab 0. \ud83d\udcc1 Repo layout added in this lab atharva-dental-assistant/ \u251c\u2500 datasets/ \u2502 \u2514\u2500 clinic/ \u2502 \u251c\u2500 treatments.json # synthetic seed data (few rows to start) \u2502 \u251c\u2500 policies/ \u2502 \u2502 \u251c\u2500 appointments.md \u2502 \u2502 \u251c\u2500 billing.md \u2502 \u2502 \u251c\u2500 cancellations.md \u2502 \u2502 \u251c\u2500 emergency.md \u2502 \u2502 \u2514\u2500 sterilization.md \u2502 \u251c\u2500 faq.md \u2502 \u2514\u2500 recent_queries.jsonl \u251c\u2500 artifacts/ # generated by jobs (auto-created) \u251c\u2500 tools/ \u2502 \u251c\u2500 synth_data.py # makes train/val/eval JSONL \u2502 \u2514\u2500 common.py # tiny helpers \u251c\u2500 rag/ \u2502 \u251c\u2500 build_index.py # chunks + embeds + FAISS \u2502 \u2514\u2500 retriever.py # FastAPI service \u251c\u2500 k8s/ \u2502 \u251c\u2500 10-data/ \u2502 \u2502 \u251c\u2500 job-generate-data.yaml \u2502 \u2502 \u2514\u2500 job-build-index.yaml \u2502 \u2514\u2500 40-serve/ \u2502 \u251c\u2500 deploy-retriever.yaml \u2502 \u2514\u2500 svc-retriever.yaml \u2514\u2500 scripts/ \u251c\u2500 generate_data.sh \u251c\u2500 build_index.sh \u2514\u2500 deploy_retriever.sh In Lab 0 your KIND nodes already mount ./project \u2192 /mnt/project . Place this repo under ./project/atharva-dental-assistant so Jobs can see it at /mnt/project/atharva-dental-assistant . Start creating the repo cd project/atharva-dental-assistant mkdir -p datasets/clinic/policies tools rag k8s/10-data k8s/40-serve scripts Also set the namespace to atharva-ml kubectl config get-contexts kubectl config set-context --current --namespace=atharva-ml 1) Seed the clinic content (synthetic but realistic) Create these minimal files; you can expand later. datasets/clinic/treatments.json [ { \"code\": \"TX-SCALE-01\", \"name\": \"Scaling & Polishing\", \"category\": \"Preventive\", \"indications\": [\"Tartar buildup\", \"Gingival bleeding\"], \"contraindications\": [], \"steps\": [\"Ultrasonic scaling\", \"Polishing\", \"Fluoride advice\"], \"duration_minutes\": 40, \"visits\": 1, \"price_band_inr\": [1200, 1800], \"aftercare\": [\"Warm saline rinses 24h\", \"Soft bristle brushing\"], \"risks\": [\"Transient sensitivity\"] }, { \"code\": \"TX-RCT-01\", \"name\": \"Root Canal Therapy\", \"category\": \"Endodontics\", \"indications\": [\"Deep caries\", \"Pulpitis\", \"Apical periodontitis\"], \"contraindications\": [\"Poor crown-root ratio (relative)\"], \"steps\": [\"Local anesthesia\", \"Canal cleaning\", \"Obturation\", \"Temporary filling\"], \"duration_minutes\": 90, \"visits\": 2, \"price_band_inr\": [6000, 9500], \"aftercare\": [\"Analgesics as advised\", \"Avoid hard chewing until crown\"], \"risks\": [\"Post-op soreness\", \"Instrument separation (rare)\"] }, { \"code\": \"TX-FILL-01\", \"name\": \"Composite Filling\", \"category\": \"Restorative\", \"indications\": [\"Dental caries\", \"Chipped tooth (minor)\"], \"contraindications\": [], \"steps\": [\"Isolation\", \"Decay removal\", \"Bonding\", \"Composite placement and curing\", \"Polishing\"], \"duration_minutes\": 20, \"visits\": 1, \"price_band_inr\": [1200, 2000], \"aftercare\": [\"Avoid very hard foods for 24h\", \"Monitor sensitivity to cold/sweets\"], \"risks\": [\"Transient sensitivity\", \"Marginal staining over time\"] }, { \"code\": \"TX-EXT-01\", \"name\": \"Tooth Extraction\", \"category\": \"Oral Surgery\", \"indications\": [\"Non-restorable tooth\", \"Severe mobility\", \"Impacted tooth (varies)\"], \"contraindications\": [\"Uncontrolled bleeding disorders (relative)\", \"Uncontrolled diabetes (relative)\"], \"steps\": [\"Local anesthesia\", \"Tooth luxation\", \"Removal\", \"Hemostasis\"], \"duration_minutes\": 30, \"visits\": 1, \"price_band_inr\": [1500, 3500], \"aftercare\": [\"Bite on gauze 30\u201345 min\", \"No spitting/rinsing 24h\", \"No straws/smoking 48\u201372h\", \"Soft diet first day\"], \"risks\": [\"Dry socket\", \"Bleeding\", \"Swelling\"] } ] datasets/clinic/policies/appointments.md # Appointments - Hours: Mon\u2013Sat 9:30\u201318:30 (Pune, IST). - Booking: phone/WhatsApp/website form. - Emergency slots: limited, call ahead. datasets/clinic/policies/billing.md # Billing - Currency: INR. - Payments: UPI, cards, netbanking. - Insurance: reimbursement support; direct tie-ups listed at reception. datasets/clinic/policies/cancellations.md # Cancellations - 24h notice requested. - Same-day cancellations may incur \u20b9300 chair-time fee. datasets/clinic/policies/emergency.md # Emergency - Red flags: uncontrolled bleeding, facial swelling, high fever, trauma. - Call: +91-20-4000-0000 (day); after-hours +91-99-9999-9999. - Fever after extraction with foul taste may indicate infection\u2014seek evaluation. - Rapidly worsening swelling with difficulty opening mouth (trismus) requires urgent care. datasets/clinic/policies/sterilization.md # Sterilization - Class-B autoclave cycles, pouched instruments, surface disinfection between patients. datasets/clinic/faq.md # FAQs Q: Is scaling painful? A: Mild discomfort; local anesthesia for sensitive cases. Q: Do whitening results last? A: 6\u201312 months; depends on diet and habits. Q: Do you work on Sundays? A: Only emergency slots on call. datasets/clinic/recent_queries.jsonl {\"ts\":\"2025-09-20T11:05:00Z\",\"q\":\"Can I eat spicy food after scaling?\",\"a\":\"Prefer soft foods for 24h; avoid very hot/spicy today.\"} {\"ts\":\"2025-09-22T15:22:00Z\",\"q\":\"RCT pain next day normal?\",\"a\":\"Mild soreness common 24\u201348h; call if severe swelling/fever.\"} 2) Data synth tool (train/val/eval JSONL) tools/common.py import random, re from pathlib import Path def read_md(path: Path) -> str: return path.read_text(encoding=\"utf-8\") def normalize_ws(s: str) -> str: return re.sub(r\"\\s+\", \" \", s).strip() def sys_prompt() -> str: return (\"You are Atharva Dental Clinic assistant in Pune (INR). \" \"Be concise, safety-minded, include 'Source:' with file#section. \" \"If info is missing, ask follow-up questions.\") tools/synth_data.py import json, argparse, random, re from pathlib import Path from difflib import SequenceMatcher # If you still want to keep common.read_md/normalize_ws, import them. from common import read_md, normalize_ws # noqa: F401 random.seed(42) def make_system_prompt(clinic: str, currency: str) -> str: return ( f\"You are Atharva Dental Clinic assistant in {clinic}, India. \" f\"Respond in concise steps, use {currency} (\u20b9) for any prices, be safety-minded, \" f\"ask for missing info when necessary, and ALWAYS include a final 'Source:' line \" f\"citing file#section for facts derived from context.\" ) def fmt_inr(x: int) -> str: return f\"\u20b9{x:,}\" def join_steps(items): \"\"\" Return a plain, semicolon-separated list (no numbering). We'll format to numbered steps later in normalize_list_answer(). \"\"\" return \"; \".join(s.strip() for s in items if s and str(s).strip()) _BULLET_PREFIX = re.compile(r'^\\s*(?:[-*\u2022]+|\\d+[.)])\\s*', re.IGNORECASE) def _strip_bullet(s: str) -> str: return _BULLET_PREFIX.sub(\"\", s).strip() def _capitalize_first(s: str) -> str: return s[:1].upper() + s[1:] if s else s def normalize_list_answer(text: str) -> str: \"\"\" If the answer is a list (separated by ';' or newlines), convert to numbered 1) ... lines. If it's already numbered/bulleted, strip existing bullets/numbers and renumber cleanly. Single-line answers are returned as-is (with trimmed whitespace). \"\"\" if text is None: return \"\" raw = text.strip() # If it already looks like multiple lines or contains semicolons, treat as a list is_listy = (\";\" in raw) or (\"\\n\" in raw) # Split on semicolons OR newlines, keep non-empty parts = [p for p in re.split(r\"[;\\n]+\", raw) if p.strip()] # If splitting produced only one item and it doesn't start with bullets/numbers, return trimmed if len(parts) <= 1 and not _BULLET_PREFIX.match(raw): return raw # If it's single line but has bullets/numbers, treat as one part if len(parts) <= 1: parts = [raw] # Clean bullets/numbers and whitespace; capitalize first letter of each step cleaned = [_capitalize_first(_strip_bullet(p)) for p in parts if _strip_bullet(p)] # If after cleaning we only have one part, just return it if len(cleaned) <= 1 and not is_listy: return cleaned[0] if cleaned else raw # Number them return \"\\n\".join(f\"{i+1}) {p}\" for i, p in enumerate(cleaned)) def add_paraphrases(q: str) -> list[str]: out = [q] ql = q.lower() m = re.match(r\"how long does (.+?) take and how many visits\\??\", q, flags=re.I) if m: proc = m.group(1) out.append(f\"{proc} \u2014 duration and number of visits?\") out.append(f\"What's the time per visit and total visits for {proc}?\") if \"what is the cost for \" in ql: proc = q[q.lower().find(\"what is the cost for \")+len(\"what is the cost for \"):].rstrip(\"?\") out.append(f\"Approximate price range for {proc}?\") out.append(f\"How much does {proc} typically cost?\") if re.search(r\"\\baftercare\\b\", ql): out.append(q.replace(\"What are aftercare steps\", \"Post-treatment care steps\")) uniq = [] seen = set() for cand in out: if cand not in seen: seen.add(cand) uniq.append(cand) return uniq def near_duplicate(a: str, b: str, threshold: float = 0.90) -> bool: return SequenceMatcher(None, a.lower(), b.lower()).ratio() >= threshold def emit_sample(system_prompt, q, a, source, ask_clarify: str | None = None): # Normalize to consistent numbered-steps style when list-like norm_a = normalize_list_answer(a) if ask_clarify: # Ensure the clarifying question starts on a new line and is properly capitalized clar = _capitalize_first(ask_clarify.strip()) norm_a = f\"{norm_a}\\n{clar}\" return { \"messages\": [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": q}, {\"role\": \"assistant\", \"content\": f\"{norm_a}\\nSource: {source}\"} ] } def clean_text(s: str) -> str: return re.sub(r\"\\s+\", \" \", s).strip() def main(): ap = argparse.ArgumentParser() ap.add_argument(\"--clinic\", default=\"Pune\") ap.add_argument(\"--currency\", default=\"INR\") ap.add_argument(\"--treatments\", required=True) ap.add_argument(\"--policies\", nargs=\"+\", required=True) ap.add_argument(\"--faq\", required=True) ap.add_argument(\"--recent\", required=True) ap.add_argument(\"--out\", required=True) ap.add_argument(\"--max_per_treatment\", type=int, default=7, help=\"Upper bound of Q/A variants per treatment\") args = ap.parse_args() out = Path(args.out); out.mkdir(parents=True, exist_ok=True) train, val, evalq = [], [], [] sys_p = make_system_prompt(args.clinic, args.currency) # ---------------------------- # Treatments \u2192 richer Q/A set # ---------------------------- treatments = json.loads(Path(args.treatments).read_text(encoding=\"utf-8\")) for t in treatments: code = t.get(\"code\", \"TX-UNK\") name = t[\"name\"] dur = t.get(\"duration_minutes\") visits = t.get(\"visits\") low, high = t.get(\"price_band_inr\", [None, None]) aftercare = t.get(\"aftercare\", []) risks = t.get(\"risks\", []) indications = t.get(\"indications\", []) contraind = t.get(\"contraindications\", []) src = f\"treatments.json#{code}\" samples = [] # 1) Duration + visits + price (+ 1\u20132 paraphrases) if dur and visits and low is not None and high is not None: q = f\"How long does {name} take and how many visits?\" a = join_steps([ f\"Typically {dur} minutes\", f\"About {visits} visit(s)\", f\"Price band: {fmt_inr(low)}\u2013{fmt_inr(high)}\" ]) for pq in add_paraphrases(q)[:2]: samples.append(emit_sample(sys_p, pq, a, src)) # 2) Aftercare (+ 1 paraphrase) if aftercare: q = f\"What are aftercare steps for {name}?\" a = join_steps(aftercare) for pq in add_paraphrases(q)[:2]: samples.append(emit_sample(sys_p, pq, a, src)) # 3) Risks q = f\"Any risks with {name}?\" a = join_steps(risks) if risks else \"Minimal risks when indicated by the clinician.\" samples.append(emit_sample(sys_p, q, a, src)) # 4) Cost-only (+ 1 paraphrase) if low is not None and high is not None: q = f\"What is the cost for {name}?\" a = f\"{fmt_inr(low)}\u2013{fmt_inr(high)} depending on case complexity and materials.\" for pq in add_paraphrases(q)[:2]: samples.append(emit_sample(sys_p, pq, a, src)) # 5) Pain/comfort expectation q = f\"Is {name.lower()} painful?\" if \"Scaling\" in name: a = \"You may feel mild discomfort; anesthesia can be used for sensitive cases.\" elif \"Root Canal\" in name: a = \"Local anesthesia is used; you may feel post-op soreness for 24\u201348h.\" else: a = \"Local anesthesia minimizes pain; some soreness after the procedure is common.\" samples.append(emit_sample(sys_p, q, a, src)) # 6) Eligibility/contraindications (if present) if contraind: q = f\"Who should avoid or be cautious about {name}?\" a = join_steps(contraind) + \"; Consult your dentist to evaluate individual risks.\" samples.append(emit_sample(sys_p, q, a, src)) # 7) When indicated if indications: q = f\"When is {name} recommended?\" a = join_steps([f\"Indicated for: {', '.join(indications)}\"]) samples.append(emit_sample(sys_p, q, a, src)) # 8) Clarifying-quote variant if low is not None and high is not None: q = f\"Can I get a quick quote for {name}?\" a = f\"Typical range is {fmt_inr(low)}\u2013{fmt_inr(high)}; exact estimate varies by tooth and complexity.\" ask = \"Which tooth/area and any prior treatment? I can give a closer estimate.\" samples.append(emit_sample(sys_p, q, a, src, ask_clarify=ask)) if args.max_per_treatment > 0: samples = samples[:args.max_per_treatment] train.extend(samples) # ---------------------------- # Policies \u2192 Q/A # ---------------------------- for p in args.policies: path = Path(p) if not path.exists(): continue head = path.stem if \"appointments\" in head: train.append(emit_sample( sys_p, \"What are clinic hours?\", \"Mon\u2013Sat 9:30\u201318:30 IST; limited emergency slots on call.\", f\"{head}.md#hours\" )) train.append(emit_sample( sys_p, \"Do you accept walk-ins?\", \"Appointments preferred; limited same-day slots may be available. Call/WhatsApp to check wait time.\", f\"{head}.md#walkins\" )) if \"cancellations\" in head: train.append(emit_sample( sys_p, \"Cancellation policy?\", \"24h notice requested; same-day cancellations may incur a chair-time fee of \u20b9300.\", f\"{head}.md#policy\" )) train.append(emit_sample( sys_p, \"How to reschedule appointment?\", \"Call/WhatsApp at least 24h prior to reschedule; late changes may incur \u20b9300 fee.\", f\"{head}.md#reschedule\" )) if \"emergency\" in head: train.append(emit_sample( sys_p, \"What are dental red flags?\", \"Uncontrolled bleeding, facial swelling, high fever, trauma\u2014seek urgent care/call immediately.\", f\"{head}.md#red-flags\" )) train.append(emit_sample( sys_p, \"Wisdom tooth pain with swelling\u2014what to do?\", join_steps([ \"Avoid self-medicating antibiotics\", \"Warm saline rinses\", \"Seek urgent evaluation if fever, trismus, or spreading swelling\" ]), f\"{head}.md#wisdom-swelling\" )) if \"billing\" in head: train.append(emit_sample( sys_p, \"Payment methods accepted?\", \"UPI, cards, netbanking; Insurance reimbursement support available.\", f\"{head}.md#methods\" )) if \"sterilization\" in head: train.append(emit_sample( sys_p, \"How do you sterilize instruments?\", \"Class-B autoclave cycles, pouched instruments, surface disinfection between patients.\", f\"{head}.md#protocols\" )) # ---------------------------- # FAQs \u2192 a few seed items # ---------------------------- if Path(args.faq).exists(): _ = read_md(Path(args.faq)) train.append(emit_sample( sys_p, \"Is scaling painful?\", \"Mild discomfort; Local anesthesia for sensitive cases.\", \"faq.md#scaling-pain\" )) train.append(emit_sample( sys_p, \"Do whitening results last?\", \"Results typically last 6\u201312 months; Depends on diet and habits.\", \"faq.md#whitening-duration\" )) train.append(emit_sample( sys_p, \"Do you work on Sundays?\", \"Only emergency slots on call.\", \"faq.md#sunday-hours\" )) # ---------------------------- # Deduplicate near-identical questions # ---------------------------- deduped = [] seen_qs = [] for ex in train: q = ex[\"messages\"][1][\"content\"] if any(near_duplicate(q, s) for s in seen_qs): continue seen_qs.append(q) deduped.append(ex) train = deduped # ---------------------------- # Shuffle + split (80/20) # ---------------------------- random.shuffle(train) split = int(0.8 * len(train)) if len(train) else 0 (Path(out) / \"train.jsonl\").write_text( \"\\n\".join(json.dumps(x, ensure_ascii=False) for x in train[:split]), encoding=\"utf-8\" ) (Path(out) / \"val.jsonl\").write_text( \"\\n\".join(json.dumps(x, ensure_ascii=False) for x in train[split:]), encoding=\"utf-8\" ) (Path(out) / \"eval.jsonl\").write_text( \"\\n\".join(json.dumps(x, ensure_ascii=False) for x in evalq), encoding=\"utf-8\" ) print(f\"Wrote {len(train[:split])} train, {len(train[split:])} val, {len(evalq)} eval to {out}\") if __name__ == \"__main__\": main() 3) Build the FAISS index + a tiny retriever API rag/build_index.py import argparse import json import os from pathlib import Path from typing import Iterable, Tuple, Dict, Any, List from sentence_transformers import SentenceTransformer import faiss # -------- Helpers to render concise, model-friendly chunk text -------- def _render_treatment_item(it: Dict[str, Any]) -> str: \"\"\" Render a single treatment JSON object into a compact, informative snippet. \"\"\" keys_order = ( \"code\", \"name\", \"category\", \"duration_minutes\", \"visits\", \"price_band_inr\", \"indications\", \"contraindications\", \"steps\", \"aftercare\", \"risks\" ) lines: List[str] = [] for k in keys_order: if k not in it: continue v = it[k] if isinstance(v, (list, tuple)): v = \", \".join(map(str, v)) lines.append(f\"{k}: {v}\") return \"\\n\".join(lines) def _render_markdown_snippet(text: str, max_lines: int = 8) -> str: \"\"\" Take the heading and first few meaningful lines from markdown. \"\"\" lines = [ln.rstrip() for ln in text.splitlines()] # keep non-empty lines; prefer headings/bullets first cleaned: List[str] = [] for ln in lines: s = ln.strip() if not s: continue cleaned.append(s) if len(cleaned) >= max_lines: break return \"\\n\".join(cleaned) def _render_recent_qa(obj: Dict[str, Any]) -> str: q = str(obj.get(\"q\", \"\")).strip() a = str(obj.get(\"a\", \"\")).strip() return f\"Q: {q}\\nA: {a}\" # -------- Corpus iterator producing (text_for_embedding, meta_dict) -------- def iter_docs(root: Path) -> Iterable[Tuple[str, Dict[str, Any]]]: \"\"\" Yields (text, meta) pairs. meta includes: - doc_id: file path relative to dataset root (e.g., 'policies/emergency.md', 'treatments.json') - section: semantic section id (e.g., 'TX-SCALE-01') or 'full' for whole docs, or timestamp for jsonl - path: doc_id#section (or doc_id when section == 'full') - type: md | json | jsonl - text: concise snippet for grounding (NOT the full document) The 'text' is also used as the embedding input. \"\"\" # policies/*.md for md in (root / \"policies\").glob(\"*.md\"): doc_id = f\"policies/{md.name}\" full = md.read_text(encoding=\"utf-8\", errors=\"ignore\") snippet = _render_markdown_snippet(full, max_lines=8) meta = { \"doc_id\": doc_id, \"section\": \"full\", \"path\": doc_id, \"type\": \"md\", \"text\": snippet, } yield snippet, meta # faq.md faq_p = (root / \"faq.md\") if faq_p.exists(): faq_txt = faq_p.read_text(encoding=\"utf-8\", errors=\"ignore\") snippet = _render_markdown_snippet(faq_txt, max_lines=10) meta = { \"doc_id\": \"faq.md\", \"section\": \"full\", \"path\": \"faq.md\", \"type\": \"md\", \"text\": snippet, } yield snippet, meta # treatments.json: one chunk per treatment, section = code (semantic id) tr_p = (root / \"treatments.json\") if tr_p.exists(): treatments = json.loads(tr_p.read_text(encoding=\"utf-8\")) if isinstance(treatments, list): for it in treatments: # prefer semantic section id 'code' (e.g., TX-SCALE-01) code = it.get(\"code\") or \"item\" snippet = _render_treatment_item(it) meta = { \"doc_id\": \"treatments.json\", \"section\": str(code), \"path\": f\"treatments.json#{code}\", \"type\": \"json\", \"text\": snippet, } yield snippet, meta # recent_queries.jsonl: optional, include as weak signals (can downweight later) rq_p = (root / \"recent_queries.jsonl\") if rq_p.exists(): for line in rq_p.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines(): if not line.strip(): continue try: obj = json.loads(line) except Exception: continue snippet = _render_recent_qa(obj) ts = str(obj.get(\"ts\", \"na\")) meta = { \"doc_id\": \"recent_queries.jsonl\", \"section\": ts, \"path\": f\"recent_queries.jsonl:{ts}\", \"type\": \"jsonl\", \"text\": snippet, } yield snippet, meta def main(): ap = argparse.ArgumentParser() ap.add_argument(\"--root\", required=True, help=\"datasets/clinic\") ap.add_argument(\"--outdir\", required=True, help=\"artifacts/rag\") args = ap.parse_args() root = Path(args.root) out = Path(args.outdir) out.mkdir(parents=True, exist_ok=True) # Build corpus texts: List[str] = [] metas: List[Dict[str, Any]] = [] for txt, meta in iter_docs(root): # keep text modest to avoid huge meta and keep embeddings focused txt_capped = txt.strip()[:1500] texts.append(txt_capped) # store the same snippet in meta (so retriever can return it directly) meta = dict(meta) meta[\"text\"] = txt_capped metas.append(meta) # Embed and index model_name = \"sentence-transformers/all-MiniLM-L6-v2\" model = SentenceTransformer(model_name) embs = model.encode( texts, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True, ) index = faiss.IndexFlatIP(embs.shape[1]) index.add(embs) # Persist faiss.write_index(index, str(out / \"index.faiss\")) (out / \"meta.json\").write_text( json.dumps(metas, ensure_ascii=False, indent=2), encoding=\"utf-8\", ) print(f\"Indexed {len(texts)} chunks \u2192 {out}\") if __name__ == \"__main__\": main() rag/retriever.py import os import json from pathlib import Path from typing import List, Optional, Tuple, Any from fastapi import FastAPI, HTTPException from pydantic import BaseModel BACKEND = os.getenv(\"BACKEND\", \"dense\") # \"sparse\" or \"dense\" INDEX_PATH = Path(os.getenv(\"INDEX_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/index.faiss\")) META_PATH = Path(os.getenv(\"META_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/meta.json\")) MODEL_DIR = os.getenv(\"MODEL_DIR\") # optional for dense MODEL_NAME = os.getenv(\"MODEL_NAME\", \"sentence-transformers/all-MiniLM-L6-v2\") app = FastAPI(title=f\"Atharva Retriever ({BACKEND})\") class SearchRequest(BaseModel): query: str k: int = 4 _ready_reason = \"starting\" _model = None; _index = None; _meta: List[dict] = [] _vec = None; _X = None # sparse objects # ------------------ Utils ------------------ def _normalize_meta_loaded(data: Any) -> List[dict]: \"\"\" Accepts various shapes of meta.json and returns a list of entries. Supported: - list[dict] - {\"items\": [...]} (common pattern) - {\"hits\": [...]} (fallback) \"\"\" if isinstance(data, list): return data if isinstance(data, dict): if \"items\" in data and isinstance(data[\"items\"], list): return data[\"items\"] if \"hits\" in data and isinstance(data[\"hits\"], list): return data[\"hits\"] raise ValueError(\"META_PATH must contain a list or a dict with 'items'/'hits'.\") def _parse_doc_and_section(path: Optional[str]) -> Tuple[str, Optional[str]]: \"\"\" Parse labels from meta.path: - 'treatments.json#0' -> ('treatments.json', '0') - 'faq.md' -> ('faq.md', None) - 'policies/emergency.md' -> ('policies/emergency.md', None) \"\"\" if not path: return \"unknown\", None if \"#\" in path: d, s = path.split(\"#\", 1) return d, s return path, None def _extract_text(m: dict) -> Optional[str]: \"\"\" Try common keys for stored chunk text. \"\"\" return m.get(\"text\") or m.get(\"chunk\") or m.get(\"content\") def _enrich_hit(idx: int, score: float) -> dict: \"\"\" Build a single enriched hit from meta[idx]. \"\"\" if idx < 0 or idx >= len(_meta): # Guard against out-of-range doc_id, section, path, typ, txt = \"unknown\", None, None, None, None else: m = _meta[idx] or {} path = m.get(\"path\") typ = m.get(\"type\") doc_id, section = _parse_doc_and_section(path) txt = _extract_text(m) hit = { \"score\": float(score), \"meta\": { \"doc_id\": doc_id, \"section\": section, \"path\": path, \"type\": typ, }, } if txt: hit[\"text\"] = txt return hit # ------------------ Loaders ------------------ def _load_dense(): global _model, _index, _meta try: import faiss from sentence_transformers import SentenceTransformer _model = SentenceTransformer(MODEL_DIR) if (MODEL_DIR and Path(MODEL_DIR).exists()) else SentenceTransformer(MODEL_NAME) _index = faiss.read_index(str(INDEX_PATH)) _meta = _normalize_meta_loaded(json.loads(META_PATH.read_text(encoding=\"utf-8\"))) return None except Exception as e: return f\"dense load error: {e}\" def _load_sparse(): global _vec, _X, _meta try: import joblib from scipy import sparse vec_p = Path(os.getenv(\"VEC_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/tfidf_vectorizer.joblib\")) X_p = Path(os.getenv(\"MAT_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/tfidf_matrix.npz\")) _vec = joblib.load(vec_p) _X = sparse.load_npz(X_p) # assume rows L2-normalized; dot == cosine _meta = _normalize_meta_loaded(json.loads(META_PATH.read_text(encoding=\"utf-8\"))) return None except Exception as e: return f\"sparse load error: {e}\" @app.on_event(\"startup\") def startup(): global _ready_reason _ready_reason = _load_sparse() if BACKEND == \"sparse\" else _load_dense() # ------------------ Endpoints ------------------ @app.get(\"/health\") def health(): return {\"ok\": True} @app.get(\"/ready\") def ready(): return {\"ready\": _ready_reason is None, \"reason\": _ready_reason} @app.post(\"/reload\") def reload_index(): global _ready_reason _ready_reason = _load_sparse() if BACKEND == \"sparse\" else _load_dense() if _ready_reason is not None: raise HTTPException(status_code=503, detail=_ready_reason) return {\"reloaded\": True} @app.post(\"/search\") def search(req: SearchRequest): if _ready_reason is not None: raise HTTPException(status_code=503, detail=_ready_reason) k = max(1, min(int(req.k), 20)) if BACKEND == \"sparse\": import numpy as np q = _vec.transform([req.query]) scores = (_X @ q.T).toarray().ravel() # cosine since rows normalized if scores.size == 0: return {\"hits\": []} # get top-k indices by score desc k_eff = min(k, scores.size) top = np.argpartition(-scores, range(k_eff))[:k_eff] top = top[np.argsort(-scores[top])] hits = [ _enrich_hit(int(i), float(scores[int(i)])) for i in top if scores[int(i)] > 0 ] return {\"hits\": hits} # dense import faiss import numpy as np v = _model.encode([req.query], normalize_embeddings=True) # IP ~ cosine D, I = _index.search(v.astype(\"float32\"), k) hits = [] for score, idx in zip(D[0].tolist(), I[0].tolist()): if idx == -1: continue hits.append(_enrich_hit(int(idx), float(score))) return {\"hits\": hits} 4) Kubernetes manifests (Jobs + Deployment) k8s/10-data/job-generate-data.yaml apiVersion: batch/v1 kind: Job metadata: name: atharva-generate-data namespace: atharva-ml spec: template: spec: restartPolicy: Never containers: - name: synth image: python:3.11-slim command: [\"bash\",\"-lc\"] args: - | pip install --no-cache-dir sentence-transformers==2.7.0 python /mnt/project/atharva-dental-assistant/tools/synth_data.py \\ --clinic Pune --currency INR \\ --treatments /mnt/project/atharva-dental-assistant/datasets/clinic/treatments.json \\ --policies /mnt/project/atharva-dental-assistant/datasets/clinic/policies/*.md \\ --faq /mnt/project/atharva-dental-assistant/datasets/clinic/faq.md \\ --recent /mnt/project/atharva-dental-assistant/datasets/clinic/recent_queries.jsonl \\ --out /mnt/project/atharva-dental-assistant/datasets/training volumeMounts: - name: host mountPath: /mnt/project volumes: - name: host hostPath: { path: /mnt/project, type: Directory } k8s/10-data/job-build-index.yaml apiVersion: batch/v1 kind: Job metadata: name: atharva-build-index namespace: atharva-ml spec: template: spec: restartPolicy: Never containers: - name: index image: public.ecr.aws/docker/library/python:3.11-slim command: [\"bash\",\"-lc\"] args: - | set -euo pipefail export HOME=/mnt/project VENV=/mnt/project/.venv-build ROOT=/mnt/project/atharva-dental-assistant/datasets/clinic OUT=/mnt/project/atharva-dental-assistant/artifacts/rag mkdir -p \"$OUT\" python -m venv \"$VENV\" . \"$VENV/bin/activate\" python -m pip install -U pip python -m pip install --no-cache-dir \"numpy==1.26.4\" \"scipy==1.10.1\" \"scikit-learn==1.3.2\" \"joblib==1.3.2\" python - << 'PY' from pathlib import Path import json, re from typing import Any, Dict, List, Tuple import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer from scipy import sparse import joblib ROOT = Path(\"/mnt/project/atharva-dental-assistant/datasets/clinic\") OUTDIR = Path(\"/mnt/project/atharva-dental-assistant/artifacts/rag\") OUTDIR.mkdir(parents=True, exist_ok=True) # ---- helpers to render concise snippets ---- def render_markdown_snippet(txt: str, max_lines: int = 8) -> str: lines = [ln.strip() for ln in txt.splitlines()] lines = [ln for ln in lines if ln] return \"\\n\".join(lines[:max_lines]) def render_treatment_item(it: Dict[str, Any]) -> str: keys = (\"code\",\"name\",\"category\",\"duration_minutes\",\"visits\",\"price_band_inr\", \"indications\",\"steps\",\"aftercare\",\"risks\") parts = [] for k in keys: if k in it: v = it[k] if isinstance(v, (list, tuple)): v = \", \".join(map(str, v)) parts.append(f\"{k}: {v}\") return \"\\n\".join(parts) def render_recent_qa(obj: Dict[str, Any]) -> str: q = str(obj.get(\"q\",\"\")).strip() a = str(obj.get(\"a\",\"\")).strip() return f\"Q: {q}\\nA: {a}\" texts: List[str] = [] meta: List[Dict[str, Any]] = [] # policies/*.md for p in sorted((ROOT/\"policies\").glob(\"*.md\")): t = p.read_text(encoding=\"utf-8\", errors=\"ignore\") snip = render_markdown_snippet(t, max_lines=8) snip = snip.strip()[:1500] doc_id = f\"policies/{p.name}\" texts.append(snip) meta.append({ \"doc_id\": doc_id, \"section\": \"full\", \"path\": doc_id, \"type\": \"md\", \"text\": snip, }) # faq.md faq = ROOT/\"faq.md\" if faq.exists(): t = faq.read_text(encoding=\"utf-8\", errors=\"ignore\") snip = render_markdown_snippet(t, max_lines=10).strip()[:1500] texts.append(snip) meta.append({ \"doc_id\": \"faq.md\", \"section\": \"full\", \"path\": \"faq.md\", \"type\": \"md\", \"text\": snip, }) # treatments.json (semantic section id = code) tr = ROOT/\"treatments.json\" if tr.exists(): items = json.loads(tr.read_text(encoding=\"utf-8\")) if isinstance(items, list): for it in items: code = it.get(\"code\") or \"item\" snip = render_treatment_item(it).strip()[:1500] texts.append(snip) meta.append({ \"doc_id\": \"treatments.json\", \"section\": str(code), \"path\": f\"treatments.json#{code}\", \"type\": \"json\", \"text\": snip, }) # recent_queries.jsonl (optional, weak source) rq = ROOT/\"recent_queries.jsonl\" if rq.exists(): for line in rq.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines(): line=line.strip() if not line: continue try: obj = json.loads(line) except Exception: continue ts = str(obj.get(\"ts\",\"na\")) snip = render_recent_qa(obj).strip()[:1500] texts.append(snip) meta.append({ \"doc_id\": \"recent_queries.jsonl\", \"section\": ts, \"path\": f\"recent_queries.jsonl:{ts}\", \"type\": \"jsonl\", \"text\": snip, }) if not texts: raise SystemExit(f\"No ingestible files in {ROOT}\") # Build sparse TF-IDF vec = TfidfVectorizer( lowercase=True, ngram_range=(1,2), max_df=0.9, min_df=1, norm=\"l2\", ) X = vec.fit_transform(texts).astype(np.float32) # Save artifacts joblib.dump(vec, OUTDIR/\"tfidf_vectorizer.joblib\") sparse.save_npz(OUTDIR/\"tfidf_matrix.npz\", X) (OUTDIR/\"meta.json\").write_text( json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\" ) print(\"TF-IDF built:\", X.shape, \"saved to\", OUTDIR) PY ls -lah \"$OUT\" && wc -c \"$OUT\"/tfidf_* \"$OUT\"/meta.json volumeMounts: - name: host mountPath: /mnt/project volumes: - name: host hostPath: { path: /mnt/project, type: Directory } k8s/40-serve/deploy-retriever.yaml apiVersion: apps/v1 kind: Deployment metadata: name: atharva-retriever namespace: atharva-ml labels: { app: retriever } spec: replicas: 1 selector: matchLabels: { app: retriever } template: metadata: labels: { app: retriever } spec: terminationGracePeriodSeconds: 20 containers: - name: api image: public.ecr.aws/docker/library/python:3.11-slim imagePullPolicy: IfNotPresent workingDir: /mnt/project/atharva-dental-assistant env: - { name: HOME, value: /mnt/project } - { name: VIRTUAL_ENV, value: /opt/venv } # venv on emptyDir - { name: PATH, value: /opt/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin } - { name: PYTHONPATH, value: /mnt/project/atharva-dental-assistant } - { name: HF_HOME, value: /mnt/project/.hf_cache } - { name: TOKENIZERS_PARALLELISM, value: \"false\" } - { name: INDEX_PATH, value: /mnt/project/atharva-dental-assistant/artifacts/rag/index.faiss } - { name: META_PATH, value: /mnt/project/atharva-dental-assistant/artifacts/rag/meta.json } - { name: MODEL_NAME, value: sentence-transformers/all-MiniLM-L6-v2 } - { name: BACKEND, value: sparse } # using sparse now - { name: PIP_DISABLE_PIP_VERSION_CHECK, value: \"1\" } - { name: PIP_NO_CACHE_DIR, value: \"1\" } - { name: PIP_CACHE_DIR, value: /opt/tmp/pip } - { name: TMPDIR, value: /opt/tmp } - { name: PIP_ONLY_BINARY, value: \":all:\" } # \u2190 do NOT build from source ports: - { name: http, containerPort: 8001 } command: [\"bash\",\"-lc\"] args: - | set -euo pipefail python -m venv \"$VIRTUAL_ENV\" . \"$VIRTUAL_ENV/bin/activate\" python -m pip install --upgrade pip if [ \"${BACKEND:-sparse}\" = \"sparse\" ]; then # Pure sparse stack (forces wheels only; will fail if a wheel is unavailable for your arch) python -m pip install --only-binary=:all: \\ \"numpy==1.26.4\" \"scipy==1.10.1\" \"scikit-learn==1.3.2\" joblib==1.4.2 \\ fastapi==0.112.2 uvicorn==0.30.6 else python -m pip install --only-binary=:all: \\ \"numpy==1.26.4\" sentence-transformers==2.7.0 \"faiss-cpu==1.7.4\" \\ fastapi==0.112.2 uvicorn==0.30.6 fi uvicorn rag.retriever:app --host 0.0.0.0 --port 8001 resources: requests: cpu: \"500m\" memory: \"1Gi\" ephemeral-storage: \"2Gi\" # \u2191 reserve more scratch limits: cpu: \"2\" memory: \"4Gi\" ephemeral-storage: \"6Gi\" # \u2191 allow larger wheels volumeMounts: - { name: host, mountPath: /mnt/project } - { name: venv, mountPath: /opt/venv } - { name: tmp, mountPath: /opt/tmp } volumes: - name: host hostPath: { path: /mnt/project, type: Directory } - name: venv emptyDir: sizeLimit: 3Gi # \u2191 allow the venv to exist - name: tmp emptyDir: sizeLimit: 3Gi # \u2191 pip/tmp workspace k8s/40-serve/svc-retriever.yaml apiVersion: v1 kind: Service metadata: name: atharva-retriever namespace: atharva-ml spec: type: NodePort selector: { app: retriever } ports: - name: http port: 8001 targetPort: 8001 nodePort: 30100 5) Helper scripts (local convenience) scripts/generate_data.sh #!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/10-data/job-generate-data.yaml kubectl -n atharva-ml wait --for=condition=complete job/atharva-generate-data --timeout=300s kubectl -n atharva-ml logs job/atharva-generate-data scripts/build_index.sh #!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/10-data/job-build-index.yaml kubectl -n atharva-ml wait --for=condition=complete job/atharva-build-index --timeout=300s kubectl -n atharva-ml logs job/atharva-build-index scripts/deploy_retriever.sh #!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/40-serve/deploy-retriever.yaml kubectl apply -f k8s/40-serve/svc-retriever.yaml kubectl -n atharva-ml rollout status deploy/atharva-retriever --timeout=180s kubectl -n atharva-ml get svc/atharva-retriever Windows (PowerShell) : you can run the same kubectl apply / wait commands directly; or convert the .sh scripts to .ps1 with the same lines minus the set -euo pipefail . 6) Run the lab From the repo root (which lives under ./project/atharva-dental-assistant on your host): # 1) Generate training/eval datasets bash scripts/generate_data.sh # validate kubectl get pods ls datasets/training # 2) Build FAISS index bash scripts/build_index.sh # validate kubectl get pods ls artifacts/rag # 3) Deploy retriever API bash scripts/deploy_retriever.sh # validate kubectl get all kubectl logs -f -l \"app=retriever\" Send some requests to the retriever to validate if RAG is working # Is retriever healthy ? curl -s http://127.0.0.1:30100/health # Reload once after rebuilding the index to clear any cached startup error curl -s -X POST http://127.0.0.1:30100/reload | jq . # 0) Ready before search? curl -s http://127.0.0.1:30100/ready | jq . # 1) Run a query and save it curl -s -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"How long does scaling take?\",\"k\":4}' | tee /tmp/r.json | jq . # 2) hits length <= k jq '(.hits|length) <= 4' /tmp/r.json # 3) every hit has score+meta jq -e 'all(.hits[]; has(\"score\") and has(\"meta\"))' /tmp/r.json # 4) scores are sorted non-increasing jq -e '[.hits[].score] as $s | reduce range(1; $s|length) as $i (true; . and ($s[$i] <= $s[$i-1]))' /tmp/r.json # 5) score range sanity # use ONE of these depending on backend: # sparse TF-IDF: scores in [0,1] jq -e 'all(.hits[].score; .>=0 and .<=1)' /tmp/r.json # dense cosine/IP (normalized): scores in [-1,1] # jq -e 'all(.hits[].score; .>=-1 and .<=1)' /tmp/r.json # 6) idempotence (same request twice \u2192 same top doc paths) curl -s -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"How long does scaling take?\",\"k\":4}' \\ | jq '[.hits[].meta.path]' > /tmp/r2.json diff -u /tmp/r2.json <(jq '[.hits[].meta.path]' /tmp/r.json) || true # 8) latency snapshot (should be sub-second after warmup for small corpora) curl -o /dev/null -s -w 'time_total=%{time_total}\\n' \\ -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"root canal\",\"k\":4}' # Test a nonsense query. Should return empty results curl -s -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"zzzzzz qwerty asdf\",\"k\":4}' | jq '.hits' # Search for a specific policies from the dataset curl -s -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"Whats the currency you accept?\",\"k\":4}' curl -s -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"Are you open on Sundays?\",\"k\":4}' You should see top hits with doc_id like treatments.json#TX-SCALE-01 and policies. Lab Summary This is what we accomplished in this lab Created synthetic, Pune/INR-aware clinic data (treatments, policies, FAQs, recent queries). Built a FAISS index (CPU) with MiniLM-L6-v2 embeddings. Deployed a Retriever API on Kubernetes that returns top-k hits w/ metadata. Proved the RAG backbone works before any model fine-tuning. courses/llmops/labs/v1","title":"Lab 01 - Building RAG system"},{"location":"lab01/#lab-1-synthetic-data-rag-system-cpu-only","text":"Below are copy-pasteable files + commands so you can run this end-to-end on the KIND cluster you set up in Lab 0.","title":"Lab 1: Synthetic data + RAG system (CPU-only)"},{"location":"lab01/#repo-layout-added-in-this-lab","text":"atharva-dental-assistant/ \u251c\u2500 datasets/ \u2502 \u2514\u2500 clinic/ \u2502 \u251c\u2500 treatments.json # synthetic seed data (few rows to start) \u2502 \u251c\u2500 policies/ \u2502 \u2502 \u251c\u2500 appointments.md \u2502 \u2502 \u251c\u2500 billing.md \u2502 \u2502 \u251c\u2500 cancellations.md \u2502 \u2502 \u251c\u2500 emergency.md \u2502 \u2502 \u2514\u2500 sterilization.md \u2502 \u251c\u2500 faq.md \u2502 \u2514\u2500 recent_queries.jsonl \u251c\u2500 artifacts/ # generated by jobs (auto-created) \u251c\u2500 tools/ \u2502 \u251c\u2500 synth_data.py # makes train/val/eval JSONL \u2502 \u2514\u2500 common.py # tiny helpers \u251c\u2500 rag/ \u2502 \u251c\u2500 build_index.py # chunks + embeds + FAISS \u2502 \u2514\u2500 retriever.py # FastAPI service \u251c\u2500 k8s/ \u2502 \u251c\u2500 10-data/ \u2502 \u2502 \u251c\u2500 job-generate-data.yaml \u2502 \u2502 \u2514\u2500 job-build-index.yaml \u2502 \u2514\u2500 40-serve/ \u2502 \u251c\u2500 deploy-retriever.yaml \u2502 \u2514\u2500 svc-retriever.yaml \u2514\u2500 scripts/ \u251c\u2500 generate_data.sh \u251c\u2500 build_index.sh \u2514\u2500 deploy_retriever.sh In Lab 0 your KIND nodes already mount ./project \u2192 /mnt/project . Place this repo under ./project/atharva-dental-assistant so Jobs can see it at /mnt/project/atharva-dental-assistant . Start creating the repo cd project/atharva-dental-assistant mkdir -p datasets/clinic/policies tools rag k8s/10-data k8s/40-serve scripts Also set the namespace to atharva-ml kubectl config get-contexts kubectl config set-context --current --namespace=atharva-ml","title":"\ud83d\udcc1 Repo layout added in this lab"},{"location":"lab01/#1-seed-the-clinic-content-synthetic-but-realistic","text":"Create these minimal files; you can expand later.","title":"1) Seed the clinic content (synthetic but realistic)"},{"location":"lab01/#datasetsclinictreatmentsjson","text":"[ { \"code\": \"TX-SCALE-01\", \"name\": \"Scaling & Polishing\", \"category\": \"Preventive\", \"indications\": [\"Tartar buildup\", \"Gingival bleeding\"], \"contraindications\": [], \"steps\": [\"Ultrasonic scaling\", \"Polishing\", \"Fluoride advice\"], \"duration_minutes\": 40, \"visits\": 1, \"price_band_inr\": [1200, 1800], \"aftercare\": [\"Warm saline rinses 24h\", \"Soft bristle brushing\"], \"risks\": [\"Transient sensitivity\"] }, { \"code\": \"TX-RCT-01\", \"name\": \"Root Canal Therapy\", \"category\": \"Endodontics\", \"indications\": [\"Deep caries\", \"Pulpitis\", \"Apical periodontitis\"], \"contraindications\": [\"Poor crown-root ratio (relative)\"], \"steps\": [\"Local anesthesia\", \"Canal cleaning\", \"Obturation\", \"Temporary filling\"], \"duration_minutes\": 90, \"visits\": 2, \"price_band_inr\": [6000, 9500], \"aftercare\": [\"Analgesics as advised\", \"Avoid hard chewing until crown\"], \"risks\": [\"Post-op soreness\", \"Instrument separation (rare)\"] }, { \"code\": \"TX-FILL-01\", \"name\": \"Composite Filling\", \"category\": \"Restorative\", \"indications\": [\"Dental caries\", \"Chipped tooth (minor)\"], \"contraindications\": [], \"steps\": [\"Isolation\", \"Decay removal\", \"Bonding\", \"Composite placement and curing\", \"Polishing\"], \"duration_minutes\": 20, \"visits\": 1, \"price_band_inr\": [1200, 2000], \"aftercare\": [\"Avoid very hard foods for 24h\", \"Monitor sensitivity to cold/sweets\"], \"risks\": [\"Transient sensitivity\", \"Marginal staining over time\"] }, { \"code\": \"TX-EXT-01\", \"name\": \"Tooth Extraction\", \"category\": \"Oral Surgery\", \"indications\": [\"Non-restorable tooth\", \"Severe mobility\", \"Impacted tooth (varies)\"], \"contraindications\": [\"Uncontrolled bleeding disorders (relative)\", \"Uncontrolled diabetes (relative)\"], \"steps\": [\"Local anesthesia\", \"Tooth luxation\", \"Removal\", \"Hemostasis\"], \"duration_minutes\": 30, \"visits\": 1, \"price_band_inr\": [1500, 3500], \"aftercare\": [\"Bite on gauze 30\u201345 min\", \"No spitting/rinsing 24h\", \"No straws/smoking 48\u201372h\", \"Soft diet first day\"], \"risks\": [\"Dry socket\", \"Bleeding\", \"Swelling\"] } ]","title":"datasets/clinic/treatments.json"},{"location":"lab01/#datasetsclinicpoliciesappointmentsmd","text":"# Appointments - Hours: Mon\u2013Sat 9:30\u201318:30 (Pune, IST). - Booking: phone/WhatsApp/website form. - Emergency slots: limited, call ahead.","title":"datasets/clinic/policies/appointments.md"},{"location":"lab01/#datasetsclinicpoliciesbillingmd","text":"# Billing - Currency: INR. - Payments: UPI, cards, netbanking. - Insurance: reimbursement support; direct tie-ups listed at reception.","title":"datasets/clinic/policies/billing.md"},{"location":"lab01/#datasetsclinicpoliciescancellationsmd","text":"# Cancellations - 24h notice requested. - Same-day cancellations may incur \u20b9300 chair-time fee.","title":"datasets/clinic/policies/cancellations.md"},{"location":"lab01/#datasetsclinicpoliciesemergencymd","text":"# Emergency - Red flags: uncontrolled bleeding, facial swelling, high fever, trauma. - Call: +91-20-4000-0000 (day); after-hours +91-99-9999-9999. - Fever after extraction with foul taste may indicate infection\u2014seek evaluation. - Rapidly worsening swelling with difficulty opening mouth (trismus) requires urgent care.","title":"datasets/clinic/policies/emergency.md"},{"location":"lab01/#datasetsclinicpoliciessterilizationmd","text":"# Sterilization - Class-B autoclave cycles, pouched instruments, surface disinfection between patients.","title":"datasets/clinic/policies/sterilization.md"},{"location":"lab01/#datasetsclinicfaqmd","text":"# FAQs Q: Is scaling painful? A: Mild discomfort; local anesthesia for sensitive cases. Q: Do whitening results last? A: 6\u201312 months; depends on diet and habits. Q: Do you work on Sundays? A: Only emergency slots on call.","title":"datasets/clinic/faq.md"},{"location":"lab01/#datasetsclinicrecent_queriesjsonl","text":"{\"ts\":\"2025-09-20T11:05:00Z\",\"q\":\"Can I eat spicy food after scaling?\",\"a\":\"Prefer soft foods for 24h; avoid very hot/spicy today.\"} {\"ts\":\"2025-09-22T15:22:00Z\",\"q\":\"RCT pain next day normal?\",\"a\":\"Mild soreness common 24\u201348h; call if severe swelling/fever.\"}","title":"datasets/clinic/recent_queries.jsonl"},{"location":"lab01/#2-data-synth-tool-trainvaleval-jsonl","text":"","title":"2) Data synth tool (train/val/eval JSONL)"},{"location":"lab01/#toolscommonpy","text":"import random, re from pathlib import Path def read_md(path: Path) -> str: return path.read_text(encoding=\"utf-8\") def normalize_ws(s: str) -> str: return re.sub(r\"\\s+\", \" \", s).strip() def sys_prompt() -> str: return (\"You are Atharva Dental Clinic assistant in Pune (INR). \" \"Be concise, safety-minded, include 'Source:' with file#section. \" \"If info is missing, ask follow-up questions.\")","title":"tools/common.py"},{"location":"lab01/#toolssynth_datapy","text":"import json, argparse, random, re from pathlib import Path from difflib import SequenceMatcher # If you still want to keep common.read_md/normalize_ws, import them. from common import read_md, normalize_ws # noqa: F401 random.seed(42) def make_system_prompt(clinic: str, currency: str) -> str: return ( f\"You are Atharva Dental Clinic assistant in {clinic}, India. \" f\"Respond in concise steps, use {currency} (\u20b9) for any prices, be safety-minded, \" f\"ask for missing info when necessary, and ALWAYS include a final 'Source:' line \" f\"citing file#section for facts derived from context.\" ) def fmt_inr(x: int) -> str: return f\"\u20b9{x:,}\" def join_steps(items): \"\"\" Return a plain, semicolon-separated list (no numbering). We'll format to numbered steps later in normalize_list_answer(). \"\"\" return \"; \".join(s.strip() for s in items if s and str(s).strip()) _BULLET_PREFIX = re.compile(r'^\\s*(?:[-*\u2022]+|\\d+[.)])\\s*', re.IGNORECASE) def _strip_bullet(s: str) -> str: return _BULLET_PREFIX.sub(\"\", s).strip() def _capitalize_first(s: str) -> str: return s[:1].upper() + s[1:] if s else s def normalize_list_answer(text: str) -> str: \"\"\" If the answer is a list (separated by ';' or newlines), convert to numbered 1) ... lines. If it's already numbered/bulleted, strip existing bullets/numbers and renumber cleanly. Single-line answers are returned as-is (with trimmed whitespace). \"\"\" if text is None: return \"\" raw = text.strip() # If it already looks like multiple lines or contains semicolons, treat as a list is_listy = (\";\" in raw) or (\"\\n\" in raw) # Split on semicolons OR newlines, keep non-empty parts = [p for p in re.split(r\"[;\\n]+\", raw) if p.strip()] # If splitting produced only one item and it doesn't start with bullets/numbers, return trimmed if len(parts) <= 1 and not _BULLET_PREFIX.match(raw): return raw # If it's single line but has bullets/numbers, treat as one part if len(parts) <= 1: parts = [raw] # Clean bullets/numbers and whitespace; capitalize first letter of each step cleaned = [_capitalize_first(_strip_bullet(p)) for p in parts if _strip_bullet(p)] # If after cleaning we only have one part, just return it if len(cleaned) <= 1 and not is_listy: return cleaned[0] if cleaned else raw # Number them return \"\\n\".join(f\"{i+1}) {p}\" for i, p in enumerate(cleaned)) def add_paraphrases(q: str) -> list[str]: out = [q] ql = q.lower() m = re.match(r\"how long does (.+?) take and how many visits\\??\", q, flags=re.I) if m: proc = m.group(1) out.append(f\"{proc} \u2014 duration and number of visits?\") out.append(f\"What's the time per visit and total visits for {proc}?\") if \"what is the cost for \" in ql: proc = q[q.lower().find(\"what is the cost for \")+len(\"what is the cost for \"):].rstrip(\"?\") out.append(f\"Approximate price range for {proc}?\") out.append(f\"How much does {proc} typically cost?\") if re.search(r\"\\baftercare\\b\", ql): out.append(q.replace(\"What are aftercare steps\", \"Post-treatment care steps\")) uniq = [] seen = set() for cand in out: if cand not in seen: seen.add(cand) uniq.append(cand) return uniq def near_duplicate(a: str, b: str, threshold: float = 0.90) -> bool: return SequenceMatcher(None, a.lower(), b.lower()).ratio() >= threshold def emit_sample(system_prompt, q, a, source, ask_clarify: str | None = None): # Normalize to consistent numbered-steps style when list-like norm_a = normalize_list_answer(a) if ask_clarify: # Ensure the clarifying question starts on a new line and is properly capitalized clar = _capitalize_first(ask_clarify.strip()) norm_a = f\"{norm_a}\\n{clar}\" return { \"messages\": [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": q}, {\"role\": \"assistant\", \"content\": f\"{norm_a}\\nSource: {source}\"} ] } def clean_text(s: str) -> str: return re.sub(r\"\\s+\", \" \", s).strip() def main(): ap = argparse.ArgumentParser() ap.add_argument(\"--clinic\", default=\"Pune\") ap.add_argument(\"--currency\", default=\"INR\") ap.add_argument(\"--treatments\", required=True) ap.add_argument(\"--policies\", nargs=\"+\", required=True) ap.add_argument(\"--faq\", required=True) ap.add_argument(\"--recent\", required=True) ap.add_argument(\"--out\", required=True) ap.add_argument(\"--max_per_treatment\", type=int, default=7, help=\"Upper bound of Q/A variants per treatment\") args = ap.parse_args() out = Path(args.out); out.mkdir(parents=True, exist_ok=True) train, val, evalq = [], [], [] sys_p = make_system_prompt(args.clinic, args.currency) # ---------------------------- # Treatments \u2192 richer Q/A set # ---------------------------- treatments = json.loads(Path(args.treatments).read_text(encoding=\"utf-8\")) for t in treatments: code = t.get(\"code\", \"TX-UNK\") name = t[\"name\"] dur = t.get(\"duration_minutes\") visits = t.get(\"visits\") low, high = t.get(\"price_band_inr\", [None, None]) aftercare = t.get(\"aftercare\", []) risks = t.get(\"risks\", []) indications = t.get(\"indications\", []) contraind = t.get(\"contraindications\", []) src = f\"treatments.json#{code}\" samples = [] # 1) Duration + visits + price (+ 1\u20132 paraphrases) if dur and visits and low is not None and high is not None: q = f\"How long does {name} take and how many visits?\" a = join_steps([ f\"Typically {dur} minutes\", f\"About {visits} visit(s)\", f\"Price band: {fmt_inr(low)}\u2013{fmt_inr(high)}\" ]) for pq in add_paraphrases(q)[:2]: samples.append(emit_sample(sys_p, pq, a, src)) # 2) Aftercare (+ 1 paraphrase) if aftercare: q = f\"What are aftercare steps for {name}?\" a = join_steps(aftercare) for pq in add_paraphrases(q)[:2]: samples.append(emit_sample(sys_p, pq, a, src)) # 3) Risks q = f\"Any risks with {name}?\" a = join_steps(risks) if risks else \"Minimal risks when indicated by the clinician.\" samples.append(emit_sample(sys_p, q, a, src)) # 4) Cost-only (+ 1 paraphrase) if low is not None and high is not None: q = f\"What is the cost for {name}?\" a = f\"{fmt_inr(low)}\u2013{fmt_inr(high)} depending on case complexity and materials.\" for pq in add_paraphrases(q)[:2]: samples.append(emit_sample(sys_p, pq, a, src)) # 5) Pain/comfort expectation q = f\"Is {name.lower()} painful?\" if \"Scaling\" in name: a = \"You may feel mild discomfort; anesthesia can be used for sensitive cases.\" elif \"Root Canal\" in name: a = \"Local anesthesia is used; you may feel post-op soreness for 24\u201348h.\" else: a = \"Local anesthesia minimizes pain; some soreness after the procedure is common.\" samples.append(emit_sample(sys_p, q, a, src)) # 6) Eligibility/contraindications (if present) if contraind: q = f\"Who should avoid or be cautious about {name}?\" a = join_steps(contraind) + \"; Consult your dentist to evaluate individual risks.\" samples.append(emit_sample(sys_p, q, a, src)) # 7) When indicated if indications: q = f\"When is {name} recommended?\" a = join_steps([f\"Indicated for: {', '.join(indications)}\"]) samples.append(emit_sample(sys_p, q, a, src)) # 8) Clarifying-quote variant if low is not None and high is not None: q = f\"Can I get a quick quote for {name}?\" a = f\"Typical range is {fmt_inr(low)}\u2013{fmt_inr(high)}; exact estimate varies by tooth and complexity.\" ask = \"Which tooth/area and any prior treatment? I can give a closer estimate.\" samples.append(emit_sample(sys_p, q, a, src, ask_clarify=ask)) if args.max_per_treatment > 0: samples = samples[:args.max_per_treatment] train.extend(samples) # ---------------------------- # Policies \u2192 Q/A # ---------------------------- for p in args.policies: path = Path(p) if not path.exists(): continue head = path.stem if \"appointments\" in head: train.append(emit_sample( sys_p, \"What are clinic hours?\", \"Mon\u2013Sat 9:30\u201318:30 IST; limited emergency slots on call.\", f\"{head}.md#hours\" )) train.append(emit_sample( sys_p, \"Do you accept walk-ins?\", \"Appointments preferred; limited same-day slots may be available. Call/WhatsApp to check wait time.\", f\"{head}.md#walkins\" )) if \"cancellations\" in head: train.append(emit_sample( sys_p, \"Cancellation policy?\", \"24h notice requested; same-day cancellations may incur a chair-time fee of \u20b9300.\", f\"{head}.md#policy\" )) train.append(emit_sample( sys_p, \"How to reschedule appointment?\", \"Call/WhatsApp at least 24h prior to reschedule; late changes may incur \u20b9300 fee.\", f\"{head}.md#reschedule\" )) if \"emergency\" in head: train.append(emit_sample( sys_p, \"What are dental red flags?\", \"Uncontrolled bleeding, facial swelling, high fever, trauma\u2014seek urgent care/call immediately.\", f\"{head}.md#red-flags\" )) train.append(emit_sample( sys_p, \"Wisdom tooth pain with swelling\u2014what to do?\", join_steps([ \"Avoid self-medicating antibiotics\", \"Warm saline rinses\", \"Seek urgent evaluation if fever, trismus, or spreading swelling\" ]), f\"{head}.md#wisdom-swelling\" )) if \"billing\" in head: train.append(emit_sample( sys_p, \"Payment methods accepted?\", \"UPI, cards, netbanking; Insurance reimbursement support available.\", f\"{head}.md#methods\" )) if \"sterilization\" in head: train.append(emit_sample( sys_p, \"How do you sterilize instruments?\", \"Class-B autoclave cycles, pouched instruments, surface disinfection between patients.\", f\"{head}.md#protocols\" )) # ---------------------------- # FAQs \u2192 a few seed items # ---------------------------- if Path(args.faq).exists(): _ = read_md(Path(args.faq)) train.append(emit_sample( sys_p, \"Is scaling painful?\", \"Mild discomfort; Local anesthesia for sensitive cases.\", \"faq.md#scaling-pain\" )) train.append(emit_sample( sys_p, \"Do whitening results last?\", \"Results typically last 6\u201312 months; Depends on diet and habits.\", \"faq.md#whitening-duration\" )) train.append(emit_sample( sys_p, \"Do you work on Sundays?\", \"Only emergency slots on call.\", \"faq.md#sunday-hours\" )) # ---------------------------- # Deduplicate near-identical questions # ---------------------------- deduped = [] seen_qs = [] for ex in train: q = ex[\"messages\"][1][\"content\"] if any(near_duplicate(q, s) for s in seen_qs): continue seen_qs.append(q) deduped.append(ex) train = deduped # ---------------------------- # Shuffle + split (80/20) # ---------------------------- random.shuffle(train) split = int(0.8 * len(train)) if len(train) else 0 (Path(out) / \"train.jsonl\").write_text( \"\\n\".join(json.dumps(x, ensure_ascii=False) for x in train[:split]), encoding=\"utf-8\" ) (Path(out) / \"val.jsonl\").write_text( \"\\n\".join(json.dumps(x, ensure_ascii=False) for x in train[split:]), encoding=\"utf-8\" ) (Path(out) / \"eval.jsonl\").write_text( \"\\n\".join(json.dumps(x, ensure_ascii=False) for x in evalq), encoding=\"utf-8\" ) print(f\"Wrote {len(train[:split])} train, {len(train[split:])} val, {len(evalq)} eval to {out}\") if __name__ == \"__main__\": main()","title":"tools/synth_data.py"},{"location":"lab01/#3-build-the-faiss-index-a-tiny-retriever-api","text":"","title":"3) Build the FAISS index + a tiny retriever API"},{"location":"lab01/#ragbuild_indexpy","text":"import argparse import json import os from pathlib import Path from typing import Iterable, Tuple, Dict, Any, List from sentence_transformers import SentenceTransformer import faiss # -------- Helpers to render concise, model-friendly chunk text -------- def _render_treatment_item(it: Dict[str, Any]) -> str: \"\"\" Render a single treatment JSON object into a compact, informative snippet. \"\"\" keys_order = ( \"code\", \"name\", \"category\", \"duration_minutes\", \"visits\", \"price_band_inr\", \"indications\", \"contraindications\", \"steps\", \"aftercare\", \"risks\" ) lines: List[str] = [] for k in keys_order: if k not in it: continue v = it[k] if isinstance(v, (list, tuple)): v = \", \".join(map(str, v)) lines.append(f\"{k}: {v}\") return \"\\n\".join(lines) def _render_markdown_snippet(text: str, max_lines: int = 8) -> str: \"\"\" Take the heading and first few meaningful lines from markdown. \"\"\" lines = [ln.rstrip() for ln in text.splitlines()] # keep non-empty lines; prefer headings/bullets first cleaned: List[str] = [] for ln in lines: s = ln.strip() if not s: continue cleaned.append(s) if len(cleaned) >= max_lines: break return \"\\n\".join(cleaned) def _render_recent_qa(obj: Dict[str, Any]) -> str: q = str(obj.get(\"q\", \"\")).strip() a = str(obj.get(\"a\", \"\")).strip() return f\"Q: {q}\\nA: {a}\" # -------- Corpus iterator producing (text_for_embedding, meta_dict) -------- def iter_docs(root: Path) -> Iterable[Tuple[str, Dict[str, Any]]]: \"\"\" Yields (text, meta) pairs. meta includes: - doc_id: file path relative to dataset root (e.g., 'policies/emergency.md', 'treatments.json') - section: semantic section id (e.g., 'TX-SCALE-01') or 'full' for whole docs, or timestamp for jsonl - path: doc_id#section (or doc_id when section == 'full') - type: md | json | jsonl - text: concise snippet for grounding (NOT the full document) The 'text' is also used as the embedding input. \"\"\" # policies/*.md for md in (root / \"policies\").glob(\"*.md\"): doc_id = f\"policies/{md.name}\" full = md.read_text(encoding=\"utf-8\", errors=\"ignore\") snippet = _render_markdown_snippet(full, max_lines=8) meta = { \"doc_id\": doc_id, \"section\": \"full\", \"path\": doc_id, \"type\": \"md\", \"text\": snippet, } yield snippet, meta # faq.md faq_p = (root / \"faq.md\") if faq_p.exists(): faq_txt = faq_p.read_text(encoding=\"utf-8\", errors=\"ignore\") snippet = _render_markdown_snippet(faq_txt, max_lines=10) meta = { \"doc_id\": \"faq.md\", \"section\": \"full\", \"path\": \"faq.md\", \"type\": \"md\", \"text\": snippet, } yield snippet, meta # treatments.json: one chunk per treatment, section = code (semantic id) tr_p = (root / \"treatments.json\") if tr_p.exists(): treatments = json.loads(tr_p.read_text(encoding=\"utf-8\")) if isinstance(treatments, list): for it in treatments: # prefer semantic section id 'code' (e.g., TX-SCALE-01) code = it.get(\"code\") or \"item\" snippet = _render_treatment_item(it) meta = { \"doc_id\": \"treatments.json\", \"section\": str(code), \"path\": f\"treatments.json#{code}\", \"type\": \"json\", \"text\": snippet, } yield snippet, meta # recent_queries.jsonl: optional, include as weak signals (can downweight later) rq_p = (root / \"recent_queries.jsonl\") if rq_p.exists(): for line in rq_p.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines(): if not line.strip(): continue try: obj = json.loads(line) except Exception: continue snippet = _render_recent_qa(obj) ts = str(obj.get(\"ts\", \"na\")) meta = { \"doc_id\": \"recent_queries.jsonl\", \"section\": ts, \"path\": f\"recent_queries.jsonl:{ts}\", \"type\": \"jsonl\", \"text\": snippet, } yield snippet, meta def main(): ap = argparse.ArgumentParser() ap.add_argument(\"--root\", required=True, help=\"datasets/clinic\") ap.add_argument(\"--outdir\", required=True, help=\"artifacts/rag\") args = ap.parse_args() root = Path(args.root) out = Path(args.outdir) out.mkdir(parents=True, exist_ok=True) # Build corpus texts: List[str] = [] metas: List[Dict[str, Any]] = [] for txt, meta in iter_docs(root): # keep text modest to avoid huge meta and keep embeddings focused txt_capped = txt.strip()[:1500] texts.append(txt_capped) # store the same snippet in meta (so retriever can return it directly) meta = dict(meta) meta[\"text\"] = txt_capped metas.append(meta) # Embed and index model_name = \"sentence-transformers/all-MiniLM-L6-v2\" model = SentenceTransformer(model_name) embs = model.encode( texts, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True, ) index = faiss.IndexFlatIP(embs.shape[1]) index.add(embs) # Persist faiss.write_index(index, str(out / \"index.faiss\")) (out / \"meta.json\").write_text( json.dumps(metas, ensure_ascii=False, indent=2), encoding=\"utf-8\", ) print(f\"Indexed {len(texts)} chunks \u2192 {out}\") if __name__ == \"__main__\": main()","title":"rag/build_index.py"},{"location":"lab01/#ragretrieverpy","text":"import os import json from pathlib import Path from typing import List, Optional, Tuple, Any from fastapi import FastAPI, HTTPException from pydantic import BaseModel BACKEND = os.getenv(\"BACKEND\", \"dense\") # \"sparse\" or \"dense\" INDEX_PATH = Path(os.getenv(\"INDEX_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/index.faiss\")) META_PATH = Path(os.getenv(\"META_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/meta.json\")) MODEL_DIR = os.getenv(\"MODEL_DIR\") # optional for dense MODEL_NAME = os.getenv(\"MODEL_NAME\", \"sentence-transformers/all-MiniLM-L6-v2\") app = FastAPI(title=f\"Atharva Retriever ({BACKEND})\") class SearchRequest(BaseModel): query: str k: int = 4 _ready_reason = \"starting\" _model = None; _index = None; _meta: List[dict] = [] _vec = None; _X = None # sparse objects # ------------------ Utils ------------------ def _normalize_meta_loaded(data: Any) -> List[dict]: \"\"\" Accepts various shapes of meta.json and returns a list of entries. Supported: - list[dict] - {\"items\": [...]} (common pattern) - {\"hits\": [...]} (fallback) \"\"\" if isinstance(data, list): return data if isinstance(data, dict): if \"items\" in data and isinstance(data[\"items\"], list): return data[\"items\"] if \"hits\" in data and isinstance(data[\"hits\"], list): return data[\"hits\"] raise ValueError(\"META_PATH must contain a list or a dict with 'items'/'hits'.\") def _parse_doc_and_section(path: Optional[str]) -> Tuple[str, Optional[str]]: \"\"\" Parse labels from meta.path: - 'treatments.json#0' -> ('treatments.json', '0') - 'faq.md' -> ('faq.md', None) - 'policies/emergency.md' -> ('policies/emergency.md', None) \"\"\" if not path: return \"unknown\", None if \"#\" in path: d, s = path.split(\"#\", 1) return d, s return path, None def _extract_text(m: dict) -> Optional[str]: \"\"\" Try common keys for stored chunk text. \"\"\" return m.get(\"text\") or m.get(\"chunk\") or m.get(\"content\") def _enrich_hit(idx: int, score: float) -> dict: \"\"\" Build a single enriched hit from meta[idx]. \"\"\" if idx < 0 or idx >= len(_meta): # Guard against out-of-range doc_id, section, path, typ, txt = \"unknown\", None, None, None, None else: m = _meta[idx] or {} path = m.get(\"path\") typ = m.get(\"type\") doc_id, section = _parse_doc_and_section(path) txt = _extract_text(m) hit = { \"score\": float(score), \"meta\": { \"doc_id\": doc_id, \"section\": section, \"path\": path, \"type\": typ, }, } if txt: hit[\"text\"] = txt return hit # ------------------ Loaders ------------------ def _load_dense(): global _model, _index, _meta try: import faiss from sentence_transformers import SentenceTransformer _model = SentenceTransformer(MODEL_DIR) if (MODEL_DIR and Path(MODEL_DIR).exists()) else SentenceTransformer(MODEL_NAME) _index = faiss.read_index(str(INDEX_PATH)) _meta = _normalize_meta_loaded(json.loads(META_PATH.read_text(encoding=\"utf-8\"))) return None except Exception as e: return f\"dense load error: {e}\" def _load_sparse(): global _vec, _X, _meta try: import joblib from scipy import sparse vec_p = Path(os.getenv(\"VEC_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/tfidf_vectorizer.joblib\")) X_p = Path(os.getenv(\"MAT_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/tfidf_matrix.npz\")) _vec = joblib.load(vec_p) _X = sparse.load_npz(X_p) # assume rows L2-normalized; dot == cosine _meta = _normalize_meta_loaded(json.loads(META_PATH.read_text(encoding=\"utf-8\"))) return None except Exception as e: return f\"sparse load error: {e}\" @app.on_event(\"startup\") def startup(): global _ready_reason _ready_reason = _load_sparse() if BACKEND == \"sparse\" else _load_dense() # ------------------ Endpoints ------------------ @app.get(\"/health\") def health(): return {\"ok\": True} @app.get(\"/ready\") def ready(): return {\"ready\": _ready_reason is None, \"reason\": _ready_reason} @app.post(\"/reload\") def reload_index(): global _ready_reason _ready_reason = _load_sparse() if BACKEND == \"sparse\" else _load_dense() if _ready_reason is not None: raise HTTPException(status_code=503, detail=_ready_reason) return {\"reloaded\": True} @app.post(\"/search\") def search(req: SearchRequest): if _ready_reason is not None: raise HTTPException(status_code=503, detail=_ready_reason) k = max(1, min(int(req.k), 20)) if BACKEND == \"sparse\": import numpy as np q = _vec.transform([req.query]) scores = (_X @ q.T).toarray().ravel() # cosine since rows normalized if scores.size == 0: return {\"hits\": []} # get top-k indices by score desc k_eff = min(k, scores.size) top = np.argpartition(-scores, range(k_eff))[:k_eff] top = top[np.argsort(-scores[top])] hits = [ _enrich_hit(int(i), float(scores[int(i)])) for i in top if scores[int(i)] > 0 ] return {\"hits\": hits} # dense import faiss import numpy as np v = _model.encode([req.query], normalize_embeddings=True) # IP ~ cosine D, I = _index.search(v.astype(\"float32\"), k) hits = [] for score, idx in zip(D[0].tolist(), I[0].tolist()): if idx == -1: continue hits.append(_enrich_hit(int(idx), float(score))) return {\"hits\": hits}","title":"rag/retriever.py"},{"location":"lab01/#4-kubernetes-manifests-jobs-deployment","text":"","title":"4) Kubernetes manifests (Jobs + Deployment)"},{"location":"lab01/#k8s10-datajob-generate-datayaml","text":"apiVersion: batch/v1 kind: Job metadata: name: atharva-generate-data namespace: atharva-ml spec: template: spec: restartPolicy: Never containers: - name: synth image: python:3.11-slim command: [\"bash\",\"-lc\"] args: - | pip install --no-cache-dir sentence-transformers==2.7.0 python /mnt/project/atharva-dental-assistant/tools/synth_data.py \\ --clinic Pune --currency INR \\ --treatments /mnt/project/atharva-dental-assistant/datasets/clinic/treatments.json \\ --policies /mnt/project/atharva-dental-assistant/datasets/clinic/policies/*.md \\ --faq /mnt/project/atharva-dental-assistant/datasets/clinic/faq.md \\ --recent /mnt/project/atharva-dental-assistant/datasets/clinic/recent_queries.jsonl \\ --out /mnt/project/atharva-dental-assistant/datasets/training volumeMounts: - name: host mountPath: /mnt/project volumes: - name: host hostPath: { path: /mnt/project, type: Directory }","title":"k8s/10-data/job-generate-data.yaml"},{"location":"lab01/#k8s10-datajob-build-indexyaml","text":"apiVersion: batch/v1 kind: Job metadata: name: atharva-build-index namespace: atharva-ml spec: template: spec: restartPolicy: Never containers: - name: index image: public.ecr.aws/docker/library/python:3.11-slim command: [\"bash\",\"-lc\"] args: - | set -euo pipefail export HOME=/mnt/project VENV=/mnt/project/.venv-build ROOT=/mnt/project/atharva-dental-assistant/datasets/clinic OUT=/mnt/project/atharva-dental-assistant/artifacts/rag mkdir -p \"$OUT\" python -m venv \"$VENV\" . \"$VENV/bin/activate\" python -m pip install -U pip python -m pip install --no-cache-dir \"numpy==1.26.4\" \"scipy==1.10.1\" \"scikit-learn==1.3.2\" \"joblib==1.3.2\" python - << 'PY' from pathlib import Path import json, re from typing import Any, Dict, List, Tuple import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer from scipy import sparse import joblib ROOT = Path(\"/mnt/project/atharva-dental-assistant/datasets/clinic\") OUTDIR = Path(\"/mnt/project/atharva-dental-assistant/artifacts/rag\") OUTDIR.mkdir(parents=True, exist_ok=True) # ---- helpers to render concise snippets ---- def render_markdown_snippet(txt: str, max_lines: int = 8) -> str: lines = [ln.strip() for ln in txt.splitlines()] lines = [ln for ln in lines if ln] return \"\\n\".join(lines[:max_lines]) def render_treatment_item(it: Dict[str, Any]) -> str: keys = (\"code\",\"name\",\"category\",\"duration_minutes\",\"visits\",\"price_band_inr\", \"indications\",\"steps\",\"aftercare\",\"risks\") parts = [] for k in keys: if k in it: v = it[k] if isinstance(v, (list, tuple)): v = \", \".join(map(str, v)) parts.append(f\"{k}: {v}\") return \"\\n\".join(parts) def render_recent_qa(obj: Dict[str, Any]) -> str: q = str(obj.get(\"q\",\"\")).strip() a = str(obj.get(\"a\",\"\")).strip() return f\"Q: {q}\\nA: {a}\" texts: List[str] = [] meta: List[Dict[str, Any]] = [] # policies/*.md for p in sorted((ROOT/\"policies\").glob(\"*.md\")): t = p.read_text(encoding=\"utf-8\", errors=\"ignore\") snip = render_markdown_snippet(t, max_lines=8) snip = snip.strip()[:1500] doc_id = f\"policies/{p.name}\" texts.append(snip) meta.append({ \"doc_id\": doc_id, \"section\": \"full\", \"path\": doc_id, \"type\": \"md\", \"text\": snip, }) # faq.md faq = ROOT/\"faq.md\" if faq.exists(): t = faq.read_text(encoding=\"utf-8\", errors=\"ignore\") snip = render_markdown_snippet(t, max_lines=10).strip()[:1500] texts.append(snip) meta.append({ \"doc_id\": \"faq.md\", \"section\": \"full\", \"path\": \"faq.md\", \"type\": \"md\", \"text\": snip, }) # treatments.json (semantic section id = code) tr = ROOT/\"treatments.json\" if tr.exists(): items = json.loads(tr.read_text(encoding=\"utf-8\")) if isinstance(items, list): for it in items: code = it.get(\"code\") or \"item\" snip = render_treatment_item(it).strip()[:1500] texts.append(snip) meta.append({ \"doc_id\": \"treatments.json\", \"section\": str(code), \"path\": f\"treatments.json#{code}\", \"type\": \"json\", \"text\": snip, }) # recent_queries.jsonl (optional, weak source) rq = ROOT/\"recent_queries.jsonl\" if rq.exists(): for line in rq.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines(): line=line.strip() if not line: continue try: obj = json.loads(line) except Exception: continue ts = str(obj.get(\"ts\",\"na\")) snip = render_recent_qa(obj).strip()[:1500] texts.append(snip) meta.append({ \"doc_id\": \"recent_queries.jsonl\", \"section\": ts, \"path\": f\"recent_queries.jsonl:{ts}\", \"type\": \"jsonl\", \"text\": snip, }) if not texts: raise SystemExit(f\"No ingestible files in {ROOT}\") # Build sparse TF-IDF vec = TfidfVectorizer( lowercase=True, ngram_range=(1,2), max_df=0.9, min_df=1, norm=\"l2\", ) X = vec.fit_transform(texts).astype(np.float32) # Save artifacts joblib.dump(vec, OUTDIR/\"tfidf_vectorizer.joblib\") sparse.save_npz(OUTDIR/\"tfidf_matrix.npz\", X) (OUTDIR/\"meta.json\").write_text( json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\" ) print(\"TF-IDF built:\", X.shape, \"saved to\", OUTDIR) PY ls -lah \"$OUT\" && wc -c \"$OUT\"/tfidf_* \"$OUT\"/meta.json volumeMounts: - name: host mountPath: /mnt/project volumes: - name: host hostPath: { path: /mnt/project, type: Directory }","title":"k8s/10-data/job-build-index.yaml"},{"location":"lab01/#k8s40-servedeploy-retrieveryaml","text":"apiVersion: apps/v1 kind: Deployment metadata: name: atharva-retriever namespace: atharva-ml labels: { app: retriever } spec: replicas: 1 selector: matchLabels: { app: retriever } template: metadata: labels: { app: retriever } spec: terminationGracePeriodSeconds: 20 containers: - name: api image: public.ecr.aws/docker/library/python:3.11-slim imagePullPolicy: IfNotPresent workingDir: /mnt/project/atharva-dental-assistant env: - { name: HOME, value: /mnt/project } - { name: VIRTUAL_ENV, value: /opt/venv } # venv on emptyDir - { name: PATH, value: /opt/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin } - { name: PYTHONPATH, value: /mnt/project/atharva-dental-assistant } - { name: HF_HOME, value: /mnt/project/.hf_cache } - { name: TOKENIZERS_PARALLELISM, value: \"false\" } - { name: INDEX_PATH, value: /mnt/project/atharva-dental-assistant/artifacts/rag/index.faiss } - { name: META_PATH, value: /mnt/project/atharva-dental-assistant/artifacts/rag/meta.json } - { name: MODEL_NAME, value: sentence-transformers/all-MiniLM-L6-v2 } - { name: BACKEND, value: sparse } # using sparse now - { name: PIP_DISABLE_PIP_VERSION_CHECK, value: \"1\" } - { name: PIP_NO_CACHE_DIR, value: \"1\" } - { name: PIP_CACHE_DIR, value: /opt/tmp/pip } - { name: TMPDIR, value: /opt/tmp } - { name: PIP_ONLY_BINARY, value: \":all:\" } # \u2190 do NOT build from source ports: - { name: http, containerPort: 8001 } command: [\"bash\",\"-lc\"] args: - | set -euo pipefail python -m venv \"$VIRTUAL_ENV\" . \"$VIRTUAL_ENV/bin/activate\" python -m pip install --upgrade pip if [ \"${BACKEND:-sparse}\" = \"sparse\" ]; then # Pure sparse stack (forces wheels only; will fail if a wheel is unavailable for your arch) python -m pip install --only-binary=:all: \\ \"numpy==1.26.4\" \"scipy==1.10.1\" \"scikit-learn==1.3.2\" joblib==1.4.2 \\ fastapi==0.112.2 uvicorn==0.30.6 else python -m pip install --only-binary=:all: \\ \"numpy==1.26.4\" sentence-transformers==2.7.0 \"faiss-cpu==1.7.4\" \\ fastapi==0.112.2 uvicorn==0.30.6 fi uvicorn rag.retriever:app --host 0.0.0.0 --port 8001 resources: requests: cpu: \"500m\" memory: \"1Gi\" ephemeral-storage: \"2Gi\" # \u2191 reserve more scratch limits: cpu: \"2\" memory: \"4Gi\" ephemeral-storage: \"6Gi\" # \u2191 allow larger wheels volumeMounts: - { name: host, mountPath: /mnt/project } - { name: venv, mountPath: /opt/venv } - { name: tmp, mountPath: /opt/tmp } volumes: - name: host hostPath: { path: /mnt/project, type: Directory } - name: venv emptyDir: sizeLimit: 3Gi # \u2191 allow the venv to exist - name: tmp emptyDir: sizeLimit: 3Gi # \u2191 pip/tmp workspace","title":"k8s/40-serve/deploy-retriever.yaml"},{"location":"lab01/#k8s40-servesvc-retrieveryaml","text":"apiVersion: v1 kind: Service metadata: name: atharva-retriever namespace: atharva-ml spec: type: NodePort selector: { app: retriever } ports: - name: http port: 8001 targetPort: 8001 nodePort: 30100","title":"k8s/40-serve/svc-retriever.yaml"},{"location":"lab01/#5-helper-scripts-local-convenience","text":"","title":"5) Helper scripts (local convenience)"},{"location":"lab01/#scriptsgenerate_datash","text":"#!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/10-data/job-generate-data.yaml kubectl -n atharva-ml wait --for=condition=complete job/atharva-generate-data --timeout=300s kubectl -n atharva-ml logs job/atharva-generate-data","title":"scripts/generate_data.sh"},{"location":"lab01/#scriptsbuild_indexsh","text":"#!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/10-data/job-build-index.yaml kubectl -n atharva-ml wait --for=condition=complete job/atharva-build-index --timeout=300s kubectl -n atharva-ml logs job/atharva-build-index","title":"scripts/build_index.sh"},{"location":"lab01/#scriptsdeploy_retrieversh","text":"#!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/40-serve/deploy-retriever.yaml kubectl apply -f k8s/40-serve/svc-retriever.yaml kubectl -n atharva-ml rollout status deploy/atharva-retriever --timeout=180s kubectl -n atharva-ml get svc/atharva-retriever Windows (PowerShell) : you can run the same kubectl apply / wait commands directly; or convert the .sh scripts to .ps1 with the same lines minus the set -euo pipefail .","title":"scripts/deploy_retriever.sh"},{"location":"lab01/#6-run-the-lab","text":"From the repo root (which lives under ./project/atharva-dental-assistant on your host): # 1) Generate training/eval datasets bash scripts/generate_data.sh # validate kubectl get pods ls datasets/training # 2) Build FAISS index bash scripts/build_index.sh # validate kubectl get pods ls artifacts/rag # 3) Deploy retriever API bash scripts/deploy_retriever.sh # validate kubectl get all kubectl logs -f -l \"app=retriever\" Send some requests to the retriever to validate if RAG is working # Is retriever healthy ? curl -s http://127.0.0.1:30100/health # Reload once after rebuilding the index to clear any cached startup error curl -s -X POST http://127.0.0.1:30100/reload | jq . # 0) Ready before search? curl -s http://127.0.0.1:30100/ready | jq . # 1) Run a query and save it curl -s -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"How long does scaling take?\",\"k\":4}' | tee /tmp/r.json | jq . # 2) hits length <= k jq '(.hits|length) <= 4' /tmp/r.json # 3) every hit has score+meta jq -e 'all(.hits[]; has(\"score\") and has(\"meta\"))' /tmp/r.json # 4) scores are sorted non-increasing jq -e '[.hits[].score] as $s | reduce range(1; $s|length) as $i (true; . and ($s[$i] <= $s[$i-1]))' /tmp/r.json # 5) score range sanity # use ONE of these depending on backend: # sparse TF-IDF: scores in [0,1] jq -e 'all(.hits[].score; .>=0 and .<=1)' /tmp/r.json # dense cosine/IP (normalized): scores in [-1,1] # jq -e 'all(.hits[].score; .>=-1 and .<=1)' /tmp/r.json # 6) idempotence (same request twice \u2192 same top doc paths) curl -s -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"How long does scaling take?\",\"k\":4}' \\ | jq '[.hits[].meta.path]' > /tmp/r2.json diff -u /tmp/r2.json <(jq '[.hits[].meta.path]' /tmp/r.json) || true # 8) latency snapshot (should be sub-second after warmup for small corpora) curl -o /dev/null -s -w 'time_total=%{time_total}\\n' \\ -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"root canal\",\"k\":4}' # Test a nonsense query. Should return empty results curl -s -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"zzzzzz qwerty asdf\",\"k\":4}' | jq '.hits' # Search for a specific policies from the dataset curl -s -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"Whats the currency you accept?\",\"k\":4}' curl -s -X POST http://127.0.0.1:30100/search \\ -H 'content-type: application/json' \\ -d '{\"query\":\"Are you open on Sundays?\",\"k\":4}' You should see top hits with doc_id like treatments.json#TX-SCALE-01 and policies.","title":"6) Run the lab"},{"location":"lab01/#lab-summary","text":"This is what we accomplished in this lab Created synthetic, Pune/INR-aware clinic data (treatments, policies, FAQs, recent queries). Built a FAISS index (CPU) with MiniLM-L6-v2 embeddings. Deployed a Retriever API on Kubernetes that returns top-k hits w/ metadata. Proved the RAG backbone works before any model fine-tuning.","title":"Lab Summary"},{"location":"lab01/#coursesllmopslabsv1","text":"","title":"courses/llmops/labs/v1"},{"location":"lab02/","text":"Lab 2: CPU LoRA fine-tuning (SmolLM2-135M) Amazing\u2014here\u2019s Lab 2: CPU LoRA fine-tuning (SmolLM2-135M) , ready to drop into your repo and run on the KIND cluster from Lab 0. This lab: fine-tunes HuggingFaceTB/SmolLM2-135M-Instruct on CPU with PEFT LoRA , using the synthetic chat JSONL you generated in Lab 1, saves LoRA adapters + checkpoints under artifacts/train/<run-id>/ , merges the LoRA into base weights for serving, produces a Transformers-style model folder , and a tarball you\u2019ll later wrap into an OCI image for ImageVolumes. \ud83d\udcc1 Files added in this lab atharva-dental-assistant/ \u251c\u2500 training/ \u2502 \u251c\u2500 Dockerfile \u2502 \u251c\u2500 train_lora.py \u2502 \u251c\u2500 merge_lora.py \u2502 \u2514\u2500 prompt_utils.py \u251c\u2500 k8s/ \u2502 \u2514\u2500 20-train/ \u2502 \u251c\u2500 job-train-lora.yaml \u2502 \u2514\u2500 job-merge-model.yaml \u2514\u2500 scripts/ \u251c\u2500 train_lora.sh \u2514\u2500 merge_model.sh Assumes your repo still lives under ./project/atharva-dental-assistant on the host (mounted into nodes at /mnt/project/atharva-dental-assistant per Lab 0). # make sure you are in the right project path project/atharva-dental-assistant # create directories you would need as part of this lab mkdir training k8s/20-train training/Dockerfile CPU-only image with pinned libs. (No CUDA; uses PyTorch CPU wheel.) File : training/Dockerfile #training/Dockerfile FROM python:3.11-slim ENV DEBIAN_FRONTEND=noninteractive PIP_DISABLE_PIP_VERSION_CHECK=1 \\ PYTHONDONTWRITEBYTECODE=1 PYTHONUNBUFFERED=1 RUN apt-get update && apt-get install -y --no-install-recommends \\ git curl ca-certificates build-essential && \\ rm -rf /var/lib/apt/lists/* # Install CPU PyTorch + Transformers/PEFT/Datasets/Accelerate RUN pip install --no-cache-dir \"torch==2.3.1\" \\ \"transformers==4.43.3\" \"peft==0.12.0\" \"datasets==2.20.0\" \\ \"accelerate==0.33.0\" \"tqdm==4.66.5\" \"bitsandbytes==0.43.1\" \\ \"sentencepiece==0.2.0\" \"numpy==1.26.4\" \"pydantic==2.8.2\" WORKDIR /workspace COPY training/train_lora.py training/merge_lora.py training/prompt_utils.py /workspace/ bitsandbytes is optional; it will no-op on CPU (kept for parity if you demonstrate QLoRA later). training/prompt_utils.py Utilities to turn our messages (from JSONL) into tokenizable text. We try tokenizer.apply_chat_template (preferred) and fallback to a simple template. # training/prompt_utils.py from typing import List, Dict DEFAULT_SYSTEM = ( \"You are Atharva Dental Clinic assistant in Pune, India (INR). \" \"Always use INR (\u20b9) as a currency for prices and cost ranges, \" \"Be concise, safety-minded, ask follow-ups if info is missing, \" \"Consider the context provided and derive answered based on that, \" \"and always include a final 'Source:' line citing file#section.\" ) def to_chat(messages: List[Dict], default_system: str = DEFAULT_SYSTEM): \"\"\" Ensure we have system\u2192user\u2192assistant message order for a single-turn sample. Our dataset already stores messages with roles. We enforce one assistant turn. \"\"\" sys_seen = any(m[\"role\"] == \"system\" for m in messages) msgs = [] if not sys_seen: msgs.append({\"role\": \"system\", \"content\": default_system}) msgs.extend(messages) # Basic guard: keep only first assistant answer for label masking out, assistant_added = [], False for m in msgs: if m[\"role\"] == \"assistant\": if assistant_added: # drop extra assistant turns for simplicity continue assistant_added = True out.append(m) return out def simple_template(messages: List[Dict]) -> str: \"\"\" Fallback formatting if tokenizer has no chat template. \"\"\" lines = [] for m in messages: role = m[\"role\"] prefix = {\"system\":\"[SYS]\", \"user\":\"[USER]\", \"assistant\":\"[ASSISTANT]\"}.get(role, f\"[{role.upper()}]\") lines.append(f\"{prefix}\\n{m['content'].strip()}\\n\") # Ensure the string ends with assistant text (trainer expects labels on last turn) return \"\\n\".join(lines).strip() training/train_lora.py Loads datasets/training/train.jsonl and val.jsonl Uses chat template if model has one; otherwise falls back Masks labels so loss is computed only on the assistant span Trains with Trainer on CPU with your hyperparams Saves adapter + tokenizer + minimal run.json # training/train_lora.py (fast demo edition) import os, json, time, math, random from pathlib import Path from dataclasses import dataclass from typing import Dict, List, Any import torch from datasets import load_dataset from transformers import (AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments) from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training from prompt_utils import to_chat, simple_template, DEFAULT_SYSTEM BASE_DIR = Path(\"/mnt/project/atharva-dental-assistant\") DATA_DIR = BASE_DIR / \"datasets\" / \"training\" # ------------------------------ # Demo-friendly defaults (override via env) # ------------------------------ BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"HuggingFaceTB/SmolLM2-135M-Instruct\") MAX_SEQ_LEN = int(os.environ.get(\"MAX_SEQ_LEN\", \"256\")) # \u2193 from 512 LORA_R = int(os.environ.get(\"LORA_R\", \"4\")) # \u2193 from 8 LORA_ALPHA = int(os.environ.get(\"LORA_ALPHA\", \"8\")) # \u2193 from 16 LORA_DROPOUT = float(os.environ.get(\"LORA_DROPOUT\", \"0.05\")) LR = float(os.environ.get(\"LR\", \"2e-4\")) WARMUP_RATIO = float(os.environ.get(\"WARMUP_RATIO\", \"0.02\")) BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", \"1\")) GRAD_ACCUM = int(os.environ.get(\"GRAD_ACCUM\", \"4\")) # \u2193 from 8 MAX_STEPS = int(os.environ.get(\"MAX_STEPS\", \"80\")) # \u2193 from 400 (\u22485\u201310 min) # Optional dataset subsample for speed (0 = use all) DEMO_MAX_TRAIN_SAMPLES = int(os.environ.get(\"DEMO_MAX_TRAIN_SAMPLES\", \"0\")) DEMO_MAX_VAL_SAMPLES = int(os.environ.get(\"DEMO_MAX_VAL_SAMPLES\", \"0\")) OUTPUT_ROOT = BASE_DIR / \"artifacts\" / \"train\" / time.strftime(\"%Y%m%d-%H%M%S\") # Use all CPU cores for a faster demo torch.set_num_threads(max(1, os.cpu_count())) print(f\"Base model: {BASE_MODEL}\") print(f\"Output dir: {OUTPUT_ROOT}\") tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True) if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token model = AutoModelForCausalLM.from_pretrained( BASE_MODEL, torch_dtype=torch.float32, device_map=None ) model = prepare_model_for_kbit_training(model) # safe on CPU # Trim LoRA to attention projections only (fewer trainable params) peft_cfg = LoraConfig( r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"], lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\" ) model = get_peft_model(model, peft_cfg) def build_example(messages: List[Dict[str, str]]) -> Dict[str, Any]: msgs = to_chat(messages, DEFAULT_SYSTEM) use_chat_template = hasattr(tokenizer, \"apply_chat_template\") text_prompt = ( tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False) if use_chat_template else simple_template(msgs) ) # Find assistant content for label masking assistant_text = [m[\"content\"] for m in msgs if m[\"role\"]==\"assistant\"][-1] _ = assistant_text.strip() tok = tokenizer(text_prompt, truncation=True, max_length=MAX_SEQ_LEN, padding=False, return_tensors=None) prefix_msgs = [m for m in msgs if m[\"role\"]!=\"assistant\"] prefix_text = ( tokenizer.apply_chat_template(prefix_msgs, tokenize=False, add_generation_prompt=True) if use_chat_template else simple_template(prefix_msgs) + \"\\n[ASSISTANT]\\n\" ) prefix_tok = tokenizer(prefix_text, truncation=True, max_length=MAX_SEQ_LEN, padding=False, return_tensors=None) input_ids = tok[\"input_ids\"] labels = input_ids.copy() mask_len = min(len(prefix_tok[\"input_ids\"]), len(labels)) labels[:mask_len] = [-100] * mask_len return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": [1]*len(input_ids)} def load_jsonl(path: Path): for line in path.read_text(encoding=\"utf-8\").splitlines(): if not line.strip(): continue yield json.loads(line) # ------------------------------ # Load + optional subsample # ------------------------------ train_records = list(load_jsonl(DATA_DIR/\"train.jsonl\")) val_records = list(load_jsonl(DATA_DIR/\"val.jsonl\")) if DEMO_MAX_TRAIN_SAMPLES > 0 and len(train_records) > DEMO_MAX_TRAIN_SAMPLES: random.seed(42) train_records = random.sample(train_records, DEMO_MAX_TRAIN_SAMPLES) if DEMO_MAX_VAL_SAMPLES > 0 and len(val_records) > DEMO_MAX_VAL_SAMPLES: random.seed(123) val_records = random.sample(val_records, DEMO_MAX_VAL_SAMPLES) train_ds = [build_example(rec[\"messages\"]) for rec in train_records] val_ds = [build_example(rec[\"messages\"]) for rec in val_records] @dataclass class Collator: pad_token_id: int = tokenizer.pad_token_id def __call__(self, batch): maxlen = max(len(x[\"input_ids\"]) for x in batch) input_ids, labels, attn = [], [], [] for x in batch: pad = [self.pad_token_id] * (maxlen - len(x[\"input_ids\"])) maskpd = [0] * (maxlen - len(x[\"attention_mask\"])) lblpd = [-100] * (maxlen - len(x[\"labels\"])) # \u2190 fixed missing ')' input_ids.append(x[\"input_ids\"] + pad) labels.append(x[\"labels\"] + lblpd) attn.append(x[\"attention_mask\"] + maskpd) return { \"input_ids\": torch.tensor(input_ids, dtype=torch.long), \"labels\": torch.tensor(labels, dtype=torch.long), \"attention_mask\": torch.tensor(attn, dtype=torch.long), } OUTPUT_ROOT.mkdir(parents=True, exist_ok=True) # ------------------------------ # Training args: no eval during train, single final save, fewer steps # ------------------------------ args = TrainingArguments( output_dir=str(OUTPUT_ROOT), per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=1, gradient_accumulation_steps=GRAD_ACCUM, learning_rate=LR, warmup_ratio=WARMUP_RATIO, max_steps=MAX_STEPS, lr_scheduler_type=\"cosine\", logging_steps=50, evaluation_strategy=\"no\", # \u2190 no eval to save time save_steps=10_000_000, # \u2190 avoid mid-run checkpoints save_total_limit=1, # \u2190 keep only final bf16=False, fp16=False, dataloader_num_workers=0, report_to=\"none\" ) # Helpful run summary N = len(train_ds) steps_per_epoch = max(1, math.ceil(N / (BATCH_SIZE * GRAD_ACCUM))) est_epochs = args.max_steps / steps_per_epoch print(f\"Train examples: {N}, steps/epoch: {steps_per_epoch}, \" f\"optimizer steps: {args.max_steps}, ~epochs: {est_epochs:.2f}\") trainer = Trainer( model=model, args=args, train_dataset=train_ds, eval_dataset=None, # no eval during training data_collator=Collator(), ) trainer.train() # Save adapter + tokenizer model.save_pretrained(str(OUTPUT_ROOT / \"lora_adapter\")) tokenizer.save_pretrained(str(OUTPUT_ROOT / \"tokenizer\")) # Run manifest (OUTPUT_ROOT / \"run.json\").write_text(json.dumps({ \"base_model\": BASE_MODEL, \"max_seq_len\": MAX_SEQ_LEN, \"lora\": {\"r\": LORA_R, \"alpha\": LORA_ALPHA, \"dropout\": LORA_DROPOUT}, \"lr\": LR, \"warmup_ratio\": WARMUP_RATIO, \"batch\": BATCH_SIZE, \"grad_accum\": GRAD_ACCUM, \"max_steps\": MAX_STEPS, \"demo_max_train_samples\": DEMO_MAX_TRAIN_SAMPLES, \"demo_max_val_samples\": DEMO_MAX_VAL_SAMPLES }, indent=2), encoding=\"utf-8\") print(f\"Training complete. Artifacts at {OUTPUT_ROOT}\") training/merge_lora.py Merges the adapter into the base model and writes a Transformers folder you can point vLLM at later. Also creates a model.tgz tarball for OCI packaging in the next lab. # training/merge_lora.py import os, json, shutil, tarfile from pathlib import Path import torch from transformers import AutoModelForCausalLM, AutoTokenizer from peft import PeftModel BASE_DIR = Path(\"/mnt/project/atharva-dental-assistant\") ART_ROOT = BASE_DIR / \"artifacts\" / \"train\" BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"HuggingFaceTB/SmolLM2-135M-Instruct\") RUN_ID = os.environ[\"RUN_ID\"] # e.g., 20250928-121314 (folder under artifacts/train) run_dir = ART_ROOT / RUN_ID adapter_dir = run_dir / \"lora_adapter\" tok_dir = run_dir / \"tokenizer\" out_dir = run_dir / \"merged-model\" assert adapter_dir.exists(), f\"Missing adapter at {adapter_dir}\" print(f\"Loading base {BASE_MODEL} and merging adapter from {adapter_dir}\") base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.float32, device_map=None) merged = PeftModel.from_pretrained(base, str(adapter_dir)) merged = merged.merge_and_unload() # apply LoRA into base weights tok = AutoTokenizer.from_pretrained(tok_dir if tok_dir.exists() else BASE_MODEL, use_fast=True) out_dir.mkdir(parents=True, exist_ok=True) merged.save_pretrained(out_dir) tok.save_pretrained(out_dir) # Create a tarball for OCI packaging in Lab 3 #tgz_path = run_dir / \"model.tgz\" #with tarfile.open(tgz_path, \"w:gz\") as tar: # tar.add(out_dir, arcname=\"model\") #print(f\"Merged model saved at {out_dir}, tarball at {tgz_path}\") k8s/20-train/job-train-lora.yaml Runs the training container mounting your repo path. Adjust MAX_STEPS to 300\u2013600. apiVersion: batch/v1 kind: Job metadata: name: atharva-train-lora namespace: atharva-ml spec: ttlSecondsAfterFinished: 7200 # auto-clean up pod 2h after completion template: spec: nodeName: llmops-kind-worker restartPolicy: Never containers: - name: train image: schoolofdevops/lora-build-python:3.11-slim imagePullPolicy: IfNotPresent command: [\"bash\",\"-lc\"] args: - | python /mnt/project/atharva-dental-assistant/training/train_lora.py env: - name: BASE_MODEL value: \"HuggingFaceTB/SmolLM2-135M-Instruct\" - name: MAX_SEQ_LEN value: \"256\" - name: LORA_R value: \"4\" - name: LORA_ALPHA value: \"8\" - name: LORA_DROPOUT value: \"0.05\" - name: LR value: \"2e-4\" - name: WARMUP_RATIO value: \"0.02\" - name: BATCH_SIZE value: \"1\" - name: GRAD_ACCUM value: \"1\" - name: MAX_STEPS value: \"80\" - name: DEMO_MAX_TRAIN_SAMPLES value: \"0\" - name: DEMO_MAX_VAL_SAMPLES value: \"0\" - name: HF_HOME value: \"/cache/hf\" # model/dataset cache across runs - name: HF_HUB_DISABLE_TELEMETRY value: \"1\" - name: TOKENIZERS_PARALLELISM value: \"true\" # Thread caps to avoid oversubscription; align to CPU limits below - name: OMP_NUM_THREADS value: \"4\" - name: MKL_NUM_THREADS value: \"4\" - name: NUMEXPR_MAX_THREADS value: \"4\" resources: requests: cpu: \"2\" memory: \"4Gi\" ephemeral-storage: \"5Gi\" limits: cpu: \"4\" memory: \"6Gi\" ephemeral-storage: \"20Gi\" volumeMounts: - name: host mountPath: /mnt/project volumes: - name: host hostPath: path: /mnt/project type: Directory - name: hf-cache hostPath: path: /mnt/hf-cache # create once on the node type: DirectoryOrCreate You can switch the container to the purpose-built image from training/Dockerfile later; the above installs deps inline to keep the lab minimal. k8s/20-train/job-merge-model.yaml Merges the adapter and creates a model.tgz . Provide RUN_ID (the timestamped folder created by the training job). apiVersion: batch/v1 kind: Job metadata: name: atharva-merge-model namespace: atharva-ml spec: template: spec: nodeName: llmops-kind-worker restartPolicy: Never containers: - name: merge image: schoolofdevops/lora-build-python:3.11-slim command: [\"bash\",\"-lc\"] args: - | python /mnt/project/atharva-dental-assistant/training/merge_lora.py env: - name: BASE_MODEL value: \"HuggingFaceTB/SmolLM2-135M-Instruct\" - name: RUN_ID value: \"REPLACE_WITH_RUN_ID\" volumeMounts: - name: host mountPath: /mnt/project volumes: - name: host hostPath: { path: /mnt/project, type: Directory } scripts/train_lora.sh #!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/20-train/job-train-lora.yaml kubectl -n atharva-ml wait --for=condition=complete job/atharva-train-lora --timeout=12h kubectl -n atharva-ml logs job/atharva-train-lora echo \"Artifacts under artifacts/train/<run-id>/\" After it finishes, list the timestamped folder: ls -1 artifacts/train \u2192 copy the folder name (e.g., 20251001-1130xx ). scripts/merge_model.sh #!/usr/bin/env bash set -euo pipefail if [ $# -lt 1 ]; then echo \"Usage: $0 <RUN_ID>\" exit 1 fi RUN_ID=\"$1\" # Patch the manifest with the RUN_ID and apply tmp=$(mktemp) sed \"s/REPLACE_WITH_RUN_ID/${RUN_ID}/g\" k8s/20-train/job-merge-model.yaml > \"$tmp\" kubectl apply -f \"$tmp\" kubectl -n atharva-ml wait --for=condition=complete job/atharva-merge-model --timeout=60m kubectl -n atharva-ml logs job/atharva-merge-model echo \"Merged model at artifacts/train/${RUN_ID}/merged-model\" #echo \"Tarball at artifacts/train/${RUN_ID}/model.tgz\" \ud83e\uddea Run the lab # 0) (If not done) Ensure Lab 1\u2019s data exists under datasets/training/{train,val}.jsonl # 1) Load Base Image to a node docker image pull schoolofdevops/lora-build-python:3.11-slim kind load docker-image --name llmops-kind schoolofdevops/lora-build-python:3.11-slim --nodes llmops-kind-worker # 2) Start fine-tuning (CPU). This can take a while; steps are small on purpose. bash scripts/train_lora.sh # 3) Find the run-id (timestamp folder) created by the job: ls -1 artifacts/train # 4) Merge adapters into base & create tarball bash scripts/merge_model.sh <RUN_ID> # 5) Inspect outputs tree artifacts/train/<RUN_ID> | sed -n '1,200p' You should see: lora_adapter/ (adapter weights), tokenizer/ , run.json , merged-model/ (Transformers folder with config.json , model.safetensors , tokenizer files), model.tgz (for OCI packaging in the next lab). Lab Summary This is what we accomplished in this lab Fine-tuned SmolLM2-135M on Kubernetes (CPU) with LoRA to learn Atharva\u2019s style: concise steps, safety tone, ask-back, and always include Source: . Produced reproducible artifacts under artifacts/train/<run-id>/ . Merged adapters \u2192 a single folder that vLLM can serve. Created a tarball ready to be wrapped into an OCI image and mounted via ImageVolumes later. courses/llmops/labs/v1","title":"Lab 02 - Fine-tuning a Model with LoRA"},{"location":"lab02/#lab-2-cpu-lora-fine-tuning-smollm2-135m","text":"Amazing\u2014here\u2019s Lab 2: CPU LoRA fine-tuning (SmolLM2-135M) , ready to drop into your repo and run on the KIND cluster from Lab 0. This lab: fine-tunes HuggingFaceTB/SmolLM2-135M-Instruct on CPU with PEFT LoRA , using the synthetic chat JSONL you generated in Lab 1, saves LoRA adapters + checkpoints under artifacts/train/<run-id>/ , merges the LoRA into base weights for serving, produces a Transformers-style model folder , and a tarball you\u2019ll later wrap into an OCI image for ImageVolumes.","title":"Lab 2: CPU LoRA fine-tuning (SmolLM2-135M)"},{"location":"lab02/#files-added-in-this-lab","text":"atharva-dental-assistant/ \u251c\u2500 training/ \u2502 \u251c\u2500 Dockerfile \u2502 \u251c\u2500 train_lora.py \u2502 \u251c\u2500 merge_lora.py \u2502 \u2514\u2500 prompt_utils.py \u251c\u2500 k8s/ \u2502 \u2514\u2500 20-train/ \u2502 \u251c\u2500 job-train-lora.yaml \u2502 \u2514\u2500 job-merge-model.yaml \u2514\u2500 scripts/ \u251c\u2500 train_lora.sh \u2514\u2500 merge_model.sh Assumes your repo still lives under ./project/atharva-dental-assistant on the host (mounted into nodes at /mnt/project/atharva-dental-assistant per Lab 0). # make sure you are in the right project path project/atharva-dental-assistant # create directories you would need as part of this lab mkdir training k8s/20-train","title":"\ud83d\udcc1 Files added in this lab"},{"location":"lab02/#trainingdockerfile","text":"CPU-only image with pinned libs. (No CUDA; uses PyTorch CPU wheel.) File : training/Dockerfile #training/Dockerfile FROM python:3.11-slim ENV DEBIAN_FRONTEND=noninteractive PIP_DISABLE_PIP_VERSION_CHECK=1 \\ PYTHONDONTWRITEBYTECODE=1 PYTHONUNBUFFERED=1 RUN apt-get update && apt-get install -y --no-install-recommends \\ git curl ca-certificates build-essential && \\ rm -rf /var/lib/apt/lists/* # Install CPU PyTorch + Transformers/PEFT/Datasets/Accelerate RUN pip install --no-cache-dir \"torch==2.3.1\" \\ \"transformers==4.43.3\" \"peft==0.12.0\" \"datasets==2.20.0\" \\ \"accelerate==0.33.0\" \"tqdm==4.66.5\" \"bitsandbytes==0.43.1\" \\ \"sentencepiece==0.2.0\" \"numpy==1.26.4\" \"pydantic==2.8.2\" WORKDIR /workspace COPY training/train_lora.py training/merge_lora.py training/prompt_utils.py /workspace/ bitsandbytes is optional; it will no-op on CPU (kept for parity if you demonstrate QLoRA later).","title":"training/Dockerfile"},{"location":"lab02/#trainingprompt_utilspy","text":"Utilities to turn our messages (from JSONL) into tokenizable text. We try tokenizer.apply_chat_template (preferred) and fallback to a simple template. # training/prompt_utils.py from typing import List, Dict DEFAULT_SYSTEM = ( \"You are Atharva Dental Clinic assistant in Pune, India (INR). \" \"Always use INR (\u20b9) as a currency for prices and cost ranges, \" \"Be concise, safety-minded, ask follow-ups if info is missing, \" \"Consider the context provided and derive answered based on that, \" \"and always include a final 'Source:' line citing file#section.\" ) def to_chat(messages: List[Dict], default_system: str = DEFAULT_SYSTEM): \"\"\" Ensure we have system\u2192user\u2192assistant message order for a single-turn sample. Our dataset already stores messages with roles. We enforce one assistant turn. \"\"\" sys_seen = any(m[\"role\"] == \"system\" for m in messages) msgs = [] if not sys_seen: msgs.append({\"role\": \"system\", \"content\": default_system}) msgs.extend(messages) # Basic guard: keep only first assistant answer for label masking out, assistant_added = [], False for m in msgs: if m[\"role\"] == \"assistant\": if assistant_added: # drop extra assistant turns for simplicity continue assistant_added = True out.append(m) return out def simple_template(messages: List[Dict]) -> str: \"\"\" Fallback formatting if tokenizer has no chat template. \"\"\" lines = [] for m in messages: role = m[\"role\"] prefix = {\"system\":\"[SYS]\", \"user\":\"[USER]\", \"assistant\":\"[ASSISTANT]\"}.get(role, f\"[{role.upper()}]\") lines.append(f\"{prefix}\\n{m['content'].strip()}\\n\") # Ensure the string ends with assistant text (trainer expects labels on last turn) return \"\\n\".join(lines).strip()","title":"training/prompt_utils.py"},{"location":"lab02/#trainingtrain_lorapy","text":"Loads datasets/training/train.jsonl and val.jsonl Uses chat template if model has one; otherwise falls back Masks labels so loss is computed only on the assistant span Trains with Trainer on CPU with your hyperparams Saves adapter + tokenizer + minimal run.json # training/train_lora.py (fast demo edition) import os, json, time, math, random from pathlib import Path from dataclasses import dataclass from typing import Dict, List, Any import torch from datasets import load_dataset from transformers import (AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments) from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training from prompt_utils import to_chat, simple_template, DEFAULT_SYSTEM BASE_DIR = Path(\"/mnt/project/atharva-dental-assistant\") DATA_DIR = BASE_DIR / \"datasets\" / \"training\" # ------------------------------ # Demo-friendly defaults (override via env) # ------------------------------ BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"HuggingFaceTB/SmolLM2-135M-Instruct\") MAX_SEQ_LEN = int(os.environ.get(\"MAX_SEQ_LEN\", \"256\")) # \u2193 from 512 LORA_R = int(os.environ.get(\"LORA_R\", \"4\")) # \u2193 from 8 LORA_ALPHA = int(os.environ.get(\"LORA_ALPHA\", \"8\")) # \u2193 from 16 LORA_DROPOUT = float(os.environ.get(\"LORA_DROPOUT\", \"0.05\")) LR = float(os.environ.get(\"LR\", \"2e-4\")) WARMUP_RATIO = float(os.environ.get(\"WARMUP_RATIO\", \"0.02\")) BATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", \"1\")) GRAD_ACCUM = int(os.environ.get(\"GRAD_ACCUM\", \"4\")) # \u2193 from 8 MAX_STEPS = int(os.environ.get(\"MAX_STEPS\", \"80\")) # \u2193 from 400 (\u22485\u201310 min) # Optional dataset subsample for speed (0 = use all) DEMO_MAX_TRAIN_SAMPLES = int(os.environ.get(\"DEMO_MAX_TRAIN_SAMPLES\", \"0\")) DEMO_MAX_VAL_SAMPLES = int(os.environ.get(\"DEMO_MAX_VAL_SAMPLES\", \"0\")) OUTPUT_ROOT = BASE_DIR / \"artifacts\" / \"train\" / time.strftime(\"%Y%m%d-%H%M%S\") # Use all CPU cores for a faster demo torch.set_num_threads(max(1, os.cpu_count())) print(f\"Base model: {BASE_MODEL}\") print(f\"Output dir: {OUTPUT_ROOT}\") tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True) if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token model = AutoModelForCausalLM.from_pretrained( BASE_MODEL, torch_dtype=torch.float32, device_map=None ) model = prepare_model_for_kbit_training(model) # safe on CPU # Trim LoRA to attention projections only (fewer trainable params) peft_cfg = LoraConfig( r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"], lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\" ) model = get_peft_model(model, peft_cfg) def build_example(messages: List[Dict[str, str]]) -> Dict[str, Any]: msgs = to_chat(messages, DEFAULT_SYSTEM) use_chat_template = hasattr(tokenizer, \"apply_chat_template\") text_prompt = ( tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False) if use_chat_template else simple_template(msgs) ) # Find assistant content for label masking assistant_text = [m[\"content\"] for m in msgs if m[\"role\"]==\"assistant\"][-1] _ = assistant_text.strip() tok = tokenizer(text_prompt, truncation=True, max_length=MAX_SEQ_LEN, padding=False, return_tensors=None) prefix_msgs = [m for m in msgs if m[\"role\"]!=\"assistant\"] prefix_text = ( tokenizer.apply_chat_template(prefix_msgs, tokenize=False, add_generation_prompt=True) if use_chat_template else simple_template(prefix_msgs) + \"\\n[ASSISTANT]\\n\" ) prefix_tok = tokenizer(prefix_text, truncation=True, max_length=MAX_SEQ_LEN, padding=False, return_tensors=None) input_ids = tok[\"input_ids\"] labels = input_ids.copy() mask_len = min(len(prefix_tok[\"input_ids\"]), len(labels)) labels[:mask_len] = [-100] * mask_len return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": [1]*len(input_ids)} def load_jsonl(path: Path): for line in path.read_text(encoding=\"utf-8\").splitlines(): if not line.strip(): continue yield json.loads(line) # ------------------------------ # Load + optional subsample # ------------------------------ train_records = list(load_jsonl(DATA_DIR/\"train.jsonl\")) val_records = list(load_jsonl(DATA_DIR/\"val.jsonl\")) if DEMO_MAX_TRAIN_SAMPLES > 0 and len(train_records) > DEMO_MAX_TRAIN_SAMPLES: random.seed(42) train_records = random.sample(train_records, DEMO_MAX_TRAIN_SAMPLES) if DEMO_MAX_VAL_SAMPLES > 0 and len(val_records) > DEMO_MAX_VAL_SAMPLES: random.seed(123) val_records = random.sample(val_records, DEMO_MAX_VAL_SAMPLES) train_ds = [build_example(rec[\"messages\"]) for rec in train_records] val_ds = [build_example(rec[\"messages\"]) for rec in val_records] @dataclass class Collator: pad_token_id: int = tokenizer.pad_token_id def __call__(self, batch): maxlen = max(len(x[\"input_ids\"]) for x in batch) input_ids, labels, attn = [], [], [] for x in batch: pad = [self.pad_token_id] * (maxlen - len(x[\"input_ids\"])) maskpd = [0] * (maxlen - len(x[\"attention_mask\"])) lblpd = [-100] * (maxlen - len(x[\"labels\"])) # \u2190 fixed missing ')' input_ids.append(x[\"input_ids\"] + pad) labels.append(x[\"labels\"] + lblpd) attn.append(x[\"attention_mask\"] + maskpd) return { \"input_ids\": torch.tensor(input_ids, dtype=torch.long), \"labels\": torch.tensor(labels, dtype=torch.long), \"attention_mask\": torch.tensor(attn, dtype=torch.long), } OUTPUT_ROOT.mkdir(parents=True, exist_ok=True) # ------------------------------ # Training args: no eval during train, single final save, fewer steps # ------------------------------ args = TrainingArguments( output_dir=str(OUTPUT_ROOT), per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=1, gradient_accumulation_steps=GRAD_ACCUM, learning_rate=LR, warmup_ratio=WARMUP_RATIO, max_steps=MAX_STEPS, lr_scheduler_type=\"cosine\", logging_steps=50, evaluation_strategy=\"no\", # \u2190 no eval to save time save_steps=10_000_000, # \u2190 avoid mid-run checkpoints save_total_limit=1, # \u2190 keep only final bf16=False, fp16=False, dataloader_num_workers=0, report_to=\"none\" ) # Helpful run summary N = len(train_ds) steps_per_epoch = max(1, math.ceil(N / (BATCH_SIZE * GRAD_ACCUM))) est_epochs = args.max_steps / steps_per_epoch print(f\"Train examples: {N}, steps/epoch: {steps_per_epoch}, \" f\"optimizer steps: {args.max_steps}, ~epochs: {est_epochs:.2f}\") trainer = Trainer( model=model, args=args, train_dataset=train_ds, eval_dataset=None, # no eval during training data_collator=Collator(), ) trainer.train() # Save adapter + tokenizer model.save_pretrained(str(OUTPUT_ROOT / \"lora_adapter\")) tokenizer.save_pretrained(str(OUTPUT_ROOT / \"tokenizer\")) # Run manifest (OUTPUT_ROOT / \"run.json\").write_text(json.dumps({ \"base_model\": BASE_MODEL, \"max_seq_len\": MAX_SEQ_LEN, \"lora\": {\"r\": LORA_R, \"alpha\": LORA_ALPHA, \"dropout\": LORA_DROPOUT}, \"lr\": LR, \"warmup_ratio\": WARMUP_RATIO, \"batch\": BATCH_SIZE, \"grad_accum\": GRAD_ACCUM, \"max_steps\": MAX_STEPS, \"demo_max_train_samples\": DEMO_MAX_TRAIN_SAMPLES, \"demo_max_val_samples\": DEMO_MAX_VAL_SAMPLES }, indent=2), encoding=\"utf-8\") print(f\"Training complete. Artifacts at {OUTPUT_ROOT}\")","title":"training/train_lora.py"},{"location":"lab02/#trainingmerge_lorapy","text":"Merges the adapter into the base model and writes a Transformers folder you can point vLLM at later. Also creates a model.tgz tarball for OCI packaging in the next lab. # training/merge_lora.py import os, json, shutil, tarfile from pathlib import Path import torch from transformers import AutoModelForCausalLM, AutoTokenizer from peft import PeftModel BASE_DIR = Path(\"/mnt/project/atharva-dental-assistant\") ART_ROOT = BASE_DIR / \"artifacts\" / \"train\" BASE_MODEL = os.environ.get(\"BASE_MODEL\", \"HuggingFaceTB/SmolLM2-135M-Instruct\") RUN_ID = os.environ[\"RUN_ID\"] # e.g., 20250928-121314 (folder under artifacts/train) run_dir = ART_ROOT / RUN_ID adapter_dir = run_dir / \"lora_adapter\" tok_dir = run_dir / \"tokenizer\" out_dir = run_dir / \"merged-model\" assert adapter_dir.exists(), f\"Missing adapter at {adapter_dir}\" print(f\"Loading base {BASE_MODEL} and merging adapter from {adapter_dir}\") base = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.float32, device_map=None) merged = PeftModel.from_pretrained(base, str(adapter_dir)) merged = merged.merge_and_unload() # apply LoRA into base weights tok = AutoTokenizer.from_pretrained(tok_dir if tok_dir.exists() else BASE_MODEL, use_fast=True) out_dir.mkdir(parents=True, exist_ok=True) merged.save_pretrained(out_dir) tok.save_pretrained(out_dir) # Create a tarball for OCI packaging in Lab 3 #tgz_path = run_dir / \"model.tgz\" #with tarfile.open(tgz_path, \"w:gz\") as tar: # tar.add(out_dir, arcname=\"model\") #print(f\"Merged model saved at {out_dir}, tarball at {tgz_path}\")","title":"training/merge_lora.py"},{"location":"lab02/#k8s20-trainjob-train-lorayaml","text":"Runs the training container mounting your repo path. Adjust MAX_STEPS to 300\u2013600. apiVersion: batch/v1 kind: Job metadata: name: atharva-train-lora namespace: atharva-ml spec: ttlSecondsAfterFinished: 7200 # auto-clean up pod 2h after completion template: spec: nodeName: llmops-kind-worker restartPolicy: Never containers: - name: train image: schoolofdevops/lora-build-python:3.11-slim imagePullPolicy: IfNotPresent command: [\"bash\",\"-lc\"] args: - | python /mnt/project/atharva-dental-assistant/training/train_lora.py env: - name: BASE_MODEL value: \"HuggingFaceTB/SmolLM2-135M-Instruct\" - name: MAX_SEQ_LEN value: \"256\" - name: LORA_R value: \"4\" - name: LORA_ALPHA value: \"8\" - name: LORA_DROPOUT value: \"0.05\" - name: LR value: \"2e-4\" - name: WARMUP_RATIO value: \"0.02\" - name: BATCH_SIZE value: \"1\" - name: GRAD_ACCUM value: \"1\" - name: MAX_STEPS value: \"80\" - name: DEMO_MAX_TRAIN_SAMPLES value: \"0\" - name: DEMO_MAX_VAL_SAMPLES value: \"0\" - name: HF_HOME value: \"/cache/hf\" # model/dataset cache across runs - name: HF_HUB_DISABLE_TELEMETRY value: \"1\" - name: TOKENIZERS_PARALLELISM value: \"true\" # Thread caps to avoid oversubscription; align to CPU limits below - name: OMP_NUM_THREADS value: \"4\" - name: MKL_NUM_THREADS value: \"4\" - name: NUMEXPR_MAX_THREADS value: \"4\" resources: requests: cpu: \"2\" memory: \"4Gi\" ephemeral-storage: \"5Gi\" limits: cpu: \"4\" memory: \"6Gi\" ephemeral-storage: \"20Gi\" volumeMounts: - name: host mountPath: /mnt/project volumes: - name: host hostPath: path: /mnt/project type: Directory - name: hf-cache hostPath: path: /mnt/hf-cache # create once on the node type: DirectoryOrCreate You can switch the container to the purpose-built image from training/Dockerfile later; the above installs deps inline to keep the lab minimal.","title":"k8s/20-train/job-train-lora.yaml"},{"location":"lab02/#k8s20-trainjob-merge-modelyaml","text":"Merges the adapter and creates a model.tgz . Provide RUN_ID (the timestamped folder created by the training job). apiVersion: batch/v1 kind: Job metadata: name: atharva-merge-model namespace: atharva-ml spec: template: spec: nodeName: llmops-kind-worker restartPolicy: Never containers: - name: merge image: schoolofdevops/lora-build-python:3.11-slim command: [\"bash\",\"-lc\"] args: - | python /mnt/project/atharva-dental-assistant/training/merge_lora.py env: - name: BASE_MODEL value: \"HuggingFaceTB/SmolLM2-135M-Instruct\" - name: RUN_ID value: \"REPLACE_WITH_RUN_ID\" volumeMounts: - name: host mountPath: /mnt/project volumes: - name: host hostPath: { path: /mnt/project, type: Directory }","title":"k8s/20-train/job-merge-model.yaml"},{"location":"lab02/#scriptstrain_lorash","text":"#!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/20-train/job-train-lora.yaml kubectl -n atharva-ml wait --for=condition=complete job/atharva-train-lora --timeout=12h kubectl -n atharva-ml logs job/atharva-train-lora echo \"Artifacts under artifacts/train/<run-id>/\" After it finishes, list the timestamped folder: ls -1 artifacts/train \u2192 copy the folder name (e.g., 20251001-1130xx ).","title":"scripts/train_lora.sh"},{"location":"lab02/#scriptsmerge_modelsh","text":"#!/usr/bin/env bash set -euo pipefail if [ $# -lt 1 ]; then echo \"Usage: $0 <RUN_ID>\" exit 1 fi RUN_ID=\"$1\" # Patch the manifest with the RUN_ID and apply tmp=$(mktemp) sed \"s/REPLACE_WITH_RUN_ID/${RUN_ID}/g\" k8s/20-train/job-merge-model.yaml > \"$tmp\" kubectl apply -f \"$tmp\" kubectl -n atharva-ml wait --for=condition=complete job/atharva-merge-model --timeout=60m kubectl -n atharva-ml logs job/atharva-merge-model echo \"Merged model at artifacts/train/${RUN_ID}/merged-model\" #echo \"Tarball at artifacts/train/${RUN_ID}/model.tgz\"","title":"scripts/merge_model.sh"},{"location":"lab02/#run-the-lab","text":"# 0) (If not done) Ensure Lab 1\u2019s data exists under datasets/training/{train,val}.jsonl # 1) Load Base Image to a node docker image pull schoolofdevops/lora-build-python:3.11-slim kind load docker-image --name llmops-kind schoolofdevops/lora-build-python:3.11-slim --nodes llmops-kind-worker # 2) Start fine-tuning (CPU). This can take a while; steps are small on purpose. bash scripts/train_lora.sh # 3) Find the run-id (timestamp folder) created by the job: ls -1 artifacts/train # 4) Merge adapters into base & create tarball bash scripts/merge_model.sh <RUN_ID> # 5) Inspect outputs tree artifacts/train/<RUN_ID> | sed -n '1,200p' You should see: lora_adapter/ (adapter weights), tokenizer/ , run.json , merged-model/ (Transformers folder with config.json , model.safetensors , tokenizer files), model.tgz (for OCI packaging in the next lab).","title":"\ud83e\uddea Run the lab"},{"location":"lab02/#lab-summary","text":"This is what we accomplished in this lab Fine-tuned SmolLM2-135M on Kubernetes (CPU) with LoRA to learn Atharva\u2019s style: concise steps, safety tone, ask-back, and always include Source: . Produced reproducible artifacts under artifacts/train/<run-id>/ . Merged adapters \u2192 a single folder that vLLM can serve. Created a tarball ready to be wrapped into an OCI image and mounted via ImageVolumes later.","title":"Lab Summary"},{"location":"lab02/#coursesllmopslabsv1","text":"","title":"courses/llmops/labs/v1"},{"location":"lab03/","text":"Lab 3 - Packaging Model as OCI Image Awesome\u2014moving into Lab 3: Package the merged model as an OCI artifact and mount it via ImageVolumes . This lab gives you production-like model delivery : immutable, versioned, and ArgoCD/GitOps-friendly later. Lab 3 \u2014 Model as OCI Artifact + ImageVolume mount Goal Turn the merged model from Lab 2 into an OCI image (no app code, just /model files). Push it to a local registry accessible by your KIND nodes. Mount it into a pod using ImageVolumes and verify the files. Works with the cluster you created in Lab 0 (ImageVolume feature-gate already enabled). What you\u2019ll add in this lab atharva-dental-assistant/ \u251c\u2500 training/ \u2502 \u2514\u2500 Dockerfile.model-asset # builds an image containing /model \u251c\u2500 k8s/ \u2502 \u2514\u2500 30-model/ \u2502 \u251c\u2500 model-mount-check.yaml # Pod that mounts the model via ImageVolume \u2502 \u2514\u2500 (optional) cosign-verify-job.yaml \u2514\u2500 scripts/ \u251c\u2500 start_local_registry.sh # runs registry at kind-registry:5001 and nets it to KIND \u251c\u2500 build_model_image.sh # builds + tags image from artifacts/train/<RUN_ID>/merged-model \u2514\u2500 model_mount_smoke.sh # applies Pod, inspects /model cd project/atharva-dental-assistant mkdir k8s/30-model 1) Sign up to DockerHub Registry Sign up to https://hub.docker.com/ if you haven\u2019t already. 2) Build a model \u201casset\u201d image (only the weights) We\u2019ll COPY the merged model folder (created in Lab 2) into /model in an image and push it to kind-registry:5001 . training/Dockerfile.model-asset # Minimal base; just hosts files at /model FROM alpine:3.20 RUN adduser -D -H -s /sbin/nologin model && mkdir -p /model && chown -R model /model # Copy the merged Transformers folder produced by Lab 2 # (Contains config.json, .safetensors, tokenizer files, etc.) COPY artifacts/train/REPLACE_RUN_ID/merged-model/ /model/ USER model We\u2019ll patch REPLACE_RUN_ID at build time. scripts/build_model_image.sh #!/usr/bin/env bash set -euo pipefail if [ $# -lt 2 ]; then echo \"Usage: $0 <RUN_ID> <TAG> e.g. $0 20251001-113045 v1\" exit 1 fi RUN_ID=\"$1\" USER=\"$2\" TAG=\"$3\" IMG=\"${USER}/smollm2-135m-merged:${TAG}\" # Safety checks [ -d \"artifacts/train/${RUN_ID}/merged-model\" ] || { echo \"Merged model folder not found for RUN_ID=${RUN_ID}\"; exit 1; } # Create a temp Dockerfile with RUN_ID patched TMP_DF=$(mktemp) sed \"s|REPLACE_RUN_ID|${RUN_ID}|g\" training/Dockerfile.model-asset > \"$TMP_DF\" echo \"==> Building model asset image: ${IMG}\" docker build -f \"$TMP_DF\" -t \"${IMG}\" . # Optional, enable if you want to push this image to DockerHub #echo \"==> Pushing to local registry ${IMG}\" #docker push \"${IMG}\" echo \"Done. Image: ${IMG}\" Run it: # Example bash scripts/build_model_image.sh <RUN_ID> <DOCKERHUB_USERNAME> v1 # e.g. bash scripts/build_model_image.sh 20251001-113045 initcron v1 You should now have an image kind-registry:5001/atharva/smollm2-135m-merged:v1 in the local registry. Load this image to llmops-kind-worker node kind load docker-image --name llmops-kind xxxxxx/smollm2-135m-merged:v2 --nodes llmops-kind-worker where, replace xxxxxx with your DockerHub ID 3) Test Mount the model via ImageVolume This Pod mounts the image\u2019s /model directory read-only at /model in the container. We then ls -lah /model . k8s/30-model/model-mount-check.yaml apiVersion: v1 kind: Pod metadata: name: model-mount-check namespace: atharva-ml spec: restartPolicy: Never nodeName: llmops-kind-worker containers: - name: inspect image: debian:12-slim command: [\"bash\",\"-lc\",\"ls -lah /model/model && head -n 50 /model/model/config.json || true && sleep 5\"] volumeMounts: - name: model mountPath: /model readOnly: true volumes: - name: model image: reference: xxxxxx/smollm2-135m-merged:v1 pullPolicy: IfNotPresent where, replace xxxxxx with actual dockerhub user/org, update tag (e.g. v1) if necessary. If you used a different tag or repo path, update reference: accordingly. scripts/model_mount_smoke.sh #!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/30-model/model-mount-check.yaml echo \"Waiting for Pod to complete...\" kubectl -n atharva-ml wait --for=condition=Ready pod/model-mount-check --timeout=120s || true kubectl -n atharva-ml logs pod/model-mount-check Run it: bash scripts/model_mount_smoke.sh Expected output (truncated): /model total 120M -rw-r--r-- 1 model model 1.2K config.json -rw-r--r-- 1 model model 512 tokenizer_config.json -rw-r--r-- 1 model model 96 tokenizer.json -rw-r--r-- 1 model model 120 model.safetensors ... { \"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", ... } (Your config.json fields will reflect SmolLM2.) Lab Summary This is what we accomplished in this lab Stood up a local OCI registry that KIND nodes can reach ( kind-registry:5001 ). Packaged your merged Transformers model into an immutable image . Mounted it with ImageVolumes \u2014the exact pattern you\u2019ll use for vLLM in KServe. (Optional) Practiced image signing to prepare for a secure rollout. courses/llmops/labs/v1","title":"Lab 03 - Packaging Model as OCI Image"},{"location":"lab03/#lab-3-packaging-model-as-oci-image","text":"Awesome\u2014moving into Lab 3: Package the merged model as an OCI artifact and mount it via ImageVolumes . This lab gives you production-like model delivery : immutable, versioned, and ArgoCD/GitOps-friendly later.","title":"Lab 3 - Packaging Model as OCI Image"},{"location":"lab03/#lab-3-model-as-oci-artifact-imagevolume-mount","text":"","title":"Lab 3 \u2014 Model as OCI Artifact + ImageVolume mount"},{"location":"lab03/#goal","text":"Turn the merged model from Lab 2 into an OCI image (no app code, just /model files). Push it to a local registry accessible by your KIND nodes. Mount it into a pod using ImageVolumes and verify the files. Works with the cluster you created in Lab 0 (ImageVolume feature-gate already enabled).","title":"Goal"},{"location":"lab03/#what-youll-add-in-this-lab","text":"atharva-dental-assistant/ \u251c\u2500 training/ \u2502 \u2514\u2500 Dockerfile.model-asset # builds an image containing /model \u251c\u2500 k8s/ \u2502 \u2514\u2500 30-model/ \u2502 \u251c\u2500 model-mount-check.yaml # Pod that mounts the model via ImageVolume \u2502 \u2514\u2500 (optional) cosign-verify-job.yaml \u2514\u2500 scripts/ \u251c\u2500 start_local_registry.sh # runs registry at kind-registry:5001 and nets it to KIND \u251c\u2500 build_model_image.sh # builds + tags image from artifacts/train/<RUN_ID>/merged-model \u2514\u2500 model_mount_smoke.sh # applies Pod, inspects /model cd project/atharva-dental-assistant mkdir k8s/30-model","title":"What you\u2019ll add in this lab"},{"location":"lab03/#1-sign-up-to-dockerhub-registry","text":"Sign up to https://hub.docker.com/ if you haven\u2019t already.","title":"1) Sign up to DockerHub Registry"},{"location":"lab03/#2-build-a-model-asset-image-only-the-weights","text":"We\u2019ll COPY the merged model folder (created in Lab 2) into /model in an image and push it to kind-registry:5001 .","title":"2) Build a model \u201casset\u201d image (only the weights)"},{"location":"lab03/#trainingdockerfilemodel-asset","text":"# Minimal base; just hosts files at /model FROM alpine:3.20 RUN adduser -D -H -s /sbin/nologin model && mkdir -p /model && chown -R model /model # Copy the merged Transformers folder produced by Lab 2 # (Contains config.json, .safetensors, tokenizer files, etc.) COPY artifacts/train/REPLACE_RUN_ID/merged-model/ /model/ USER model We\u2019ll patch REPLACE_RUN_ID at build time.","title":"training/Dockerfile.model-asset"},{"location":"lab03/#scriptsbuild_model_imagesh","text":"#!/usr/bin/env bash set -euo pipefail if [ $# -lt 2 ]; then echo \"Usage: $0 <RUN_ID> <TAG> e.g. $0 20251001-113045 v1\" exit 1 fi RUN_ID=\"$1\" USER=\"$2\" TAG=\"$3\" IMG=\"${USER}/smollm2-135m-merged:${TAG}\" # Safety checks [ -d \"artifacts/train/${RUN_ID}/merged-model\" ] || { echo \"Merged model folder not found for RUN_ID=${RUN_ID}\"; exit 1; } # Create a temp Dockerfile with RUN_ID patched TMP_DF=$(mktemp) sed \"s|REPLACE_RUN_ID|${RUN_ID}|g\" training/Dockerfile.model-asset > \"$TMP_DF\" echo \"==> Building model asset image: ${IMG}\" docker build -f \"$TMP_DF\" -t \"${IMG}\" . # Optional, enable if you want to push this image to DockerHub #echo \"==> Pushing to local registry ${IMG}\" #docker push \"${IMG}\" echo \"Done. Image: ${IMG}\" Run it: # Example bash scripts/build_model_image.sh <RUN_ID> <DOCKERHUB_USERNAME> v1 # e.g. bash scripts/build_model_image.sh 20251001-113045 initcron v1 You should now have an image kind-registry:5001/atharva/smollm2-135m-merged:v1 in the local registry. Load this image to llmops-kind-worker node kind load docker-image --name llmops-kind xxxxxx/smollm2-135m-merged:v2 --nodes llmops-kind-worker where, replace xxxxxx with your DockerHub ID","title":"scripts/build_model_image.sh"},{"location":"lab03/#3-test-mount-the-model-via-imagevolume","text":"This Pod mounts the image\u2019s /model directory read-only at /model in the container. We then ls -lah /model .","title":"3) Test Mount the model via ImageVolume"},{"location":"lab03/#k8s30-modelmodel-mount-checkyaml","text":"apiVersion: v1 kind: Pod metadata: name: model-mount-check namespace: atharva-ml spec: restartPolicy: Never nodeName: llmops-kind-worker containers: - name: inspect image: debian:12-slim command: [\"bash\",\"-lc\",\"ls -lah /model/model && head -n 50 /model/model/config.json || true && sleep 5\"] volumeMounts: - name: model mountPath: /model readOnly: true volumes: - name: model image: reference: xxxxxx/smollm2-135m-merged:v1 pullPolicy: IfNotPresent where, replace xxxxxx with actual dockerhub user/org, update tag (e.g. v1) if necessary. If you used a different tag or repo path, update reference: accordingly.","title":"k8s/30-model/model-mount-check.yaml"},{"location":"lab03/#scriptsmodel_mount_smokesh","text":"#!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/30-model/model-mount-check.yaml echo \"Waiting for Pod to complete...\" kubectl -n atharva-ml wait --for=condition=Ready pod/model-mount-check --timeout=120s || true kubectl -n atharva-ml logs pod/model-mount-check Run it: bash scripts/model_mount_smoke.sh Expected output (truncated): /model total 120M -rw-r--r-- 1 model model 1.2K config.json -rw-r--r-- 1 model model 512 tokenizer_config.json -rw-r--r-- 1 model model 96 tokenizer.json -rw-r--r-- 1 model model 120 model.safetensors ... { \"architectures\": [\"MistralForCausalLM\"], \"model_type\": \"mistral\", ... } (Your config.json fields will reflect SmolLM2.)","title":"scripts/model_mount_smoke.sh"},{"location":"lab03/#lab-summary","text":"This is what we accomplished in this lab Stood up a local OCI registry that KIND nodes can reach ( kind-registry:5001 ). Packaged your merged Transformers model into an immutable image . Mounted it with ImageVolumes \u2014the exact pattern you\u2019ll use for vLLM in KServe. (Optional) Practiced image signing to prepare for a secure rollout.","title":"Lab Summary"},{"location":"lab03/#coursesllmopslabsv1","text":"","title":"courses/llmops/labs/v1"},{"location":"lab04/","text":"Lab 4 \u2014 vLLM Serving (KServe RawDeployment) + RAG-aware Chat API Goal Serve the merged SmolLM2-135M from Lab 2 via vLLM . Mount the model snapshot with ImageVolumes . Deploy a Chat API (FastAPI) that: calls the Retriever (Lab 1) for top-k context, composes a safe system prompt (Pune/INR, clinic context), calls vLLM\u2019s OpenAI-compatible endpoint, returns answer + citations + basic timing/token stats. Namespaces: atharva-ml : vLLM server atharva-app : Chat API (or keep all in atharva-ml if you prefer) Repo additions (this lab) atharva-dental-assistant/ \u251c\u2500 serving/ \u2502 \u251c\u2500 chat_api.py \u2502 \u2514\u2500 prompt_templates.py \u251c\u2500 k8s/ \u2502 \u2514\u2500 40-serve/ \u2502 \u251c\u2500 rawdeployment-vllm.yaml # KServe RawDeployment (preferred) \u2502 \u251c\u2500 svc-vllm.yaml \u2502 \u251c\u2500 deploy-chat-api.yaml \u2502 \u251c\u2500 svc-chat-api.yaml \u2502 \u2514\u2500 cm-chat-api.yaml \u2514\u2500 scripts/ \u251c\u2500 deploy_vllm.sh \u251c\u2500 deploy_chat_api.sh \u2514\u2500 smoke_e2e.sh Assumes you already built & pushed your model image to the local registry in Lab 3 as: kind-registry:5001/atharva/smollm2-135m-merged:v1 cd project/atharva-dental-assistant # Pull a vLLM image for CPU Inference docker image pull schoolofdevops/vllm-cpu-nonuma:0.9.1 # Load it onto KinD cluster kind load docker-image --name llmops-kind schoolofdevops/vllm-cpu-nonuma:0.9.1 --nodes llmops-kind-worker 1) vLLM with ImageVolume (KServe RawDeployment) k8s/40-serve/rawdeployment-vllm.yaml apiVersion: serving.kserve.io/v1beta1 kind: InferenceService metadata: name: atharva-vllm namespace: atharva-ml annotations: autoscaling.knative.dev/metric: \"concurrency\" autoscaling.knative.dev/target: \"1\" spec: predictor: minReplicas: 1 maxReplicas: 1 containerConcurrency: 1 containers: - name: vllm image: schoolofdevops/vllm-cpu-nonuma:0.9.1 args: - --model=/models/model - --host=0.0.0.0 - --port=8000 # KServe/Knative-friendly - --max-model-len=2048 - --served-model-name=smollm2-135m-atharva - --dtype=float16 # keeps RAM lower on CPU for this tiny model - --disable-frontend-multiprocessing - --max-num-seqs=1 # clamp engine concurrency (OOM guard) - --swap-space=0.5 # GiB reserved for CPU KV cache (fits small pod) env: - name: VLLM_TARGET_DEVICE value: \"cpu\" - name: VLLM_CPU_KVCACHE_SPACE value: \"1\" - name: OMP_NUM_THREADS value: \"2\" - name: OPENBLAS_NUM_THREADS value: \"1\" - name: MKL_NUM_THREADS value: \"1\" - name: VLLM_CPU_OMP_THREADS_BIND value: \"0-1\" # avoid NUMA auto-binding path ports: - name: http1 containerPort: 8000 resources: requests: cpu: \"2\" memory: \"2Gi\" limits: cpu: \"2\" memory: \"3Gi\" # bump to 4Gi if you still see OOM readinessProbe: httpGet: path: /health port: 8000 initialDelaySeconds: 5 periodSeconds: 5 livenessProbe: httpGet: path: /health port: 8000 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: model mountPath: /models readOnly: true # Pinning to a node is fine; nodeSelector is usually nicer than nodeName: # nodeSelector: # kubernetes.io/hostname: llmops-kind-worker nodeName: llmops-kind-worker volumes: - name: model image: reference: initcron/smollm2-135m-merged:v3 pullPolicy: IfNotPresent where, replace initcron/smollm2-135m-merged:v2 with actual tag. If your cluster CRD doesn\u2019t support RawDeployment , you can temporarily deploy a plain Deployment (fallback) with the same container/volume spec . Keep going with the rest of the lab; the Chat API doesn\u2019t care how vLLM is deployed as long as the Service is up. k8s/40-serve/svc-vllm.yaml apiVersion: v1 kind: Service metadata: name: atharva-vllm namespace: atharva-ml spec: type: NodePort selector: # KServe RawDeployment pods are labeled with app=<deployment name> by default app: isvc.atharva-vllm-predictor ports: - name: http port: 8000 targetPort: 8000 nodePort: 30200 2) Chat API (RAG \u2192 prompt \u2192 vLLM) mkdir serving serving/prompt_templates.py SYSTEM_PROMPT = ( \"You are Atharva Dental Clinic assistant based in Pune, India. \" \"Respond in concise steps, use INR as currency for any prices or costs, be safety-minded, first try to find the answer from the context provided here.\" \"ask for missing info when necessary, and ALWAYS include a final 'Source:' line \" \"citing file#section for facts derived from context.\\n\" \"If the question indicates emergency red flags (uncontrolled bleeding, facial swelling, high fever, trauma), \" \"urge immediate contact with the clinic's emergency number.\\n\" ) def _label(meta: dict) -> str: did = (meta or {}).get(\"doc_id\") sec = (meta or {}).get(\"section\") if not did: return \"unknown\" return f\"{did}#{sec}\" if sec and sec != \"full\" else did def _render_context_block(retrieved_hits: list[dict]) -> str: \"\"\" Render only label + text, no Python dicts. \"\"\" blocks: list[str] = [] for h in retrieved_hits: meta = h.get(\"meta\") or {} label = _label(meta) text = (h.get(\"text\") or meta.get(\"text\") or \"\").strip() if not text: continue blocks.append(f\"### {label}\\n{text}\") return \"\\n\\n\".join(blocks).strip() def build_messages(user_q: str, retrieved_hits: list[dict]) -> list[dict]: context_block = _render_context_block(retrieved_hits) system = SYSTEM_PROMPT + \"\\nContext snippets:\\n\" + (context_block if context_block else \"(none)\") return [ {\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user_q.strip()}, ] serving/chat_api.py import os import time import httpx from fastapi import FastAPI, Query from pydantic import BaseModel from typing import List, Dict, Any from prompt_templates import build_messages RETRIEVER_URL = os.getenv(\"RETRIEVER_URL\", \"http://atharva-retriever.atharva-ml.svc.cluster.local:8001\") VLLM_URL = os.getenv(\"VLLM_URL\", \"http://atharva-vllm.atharva-ml.svc.cluster.local:8000\") MODEL_NAME = os.getenv(\"MODEL_NAME\", \"smollm2-135m-atharva\") MAX_CTX_SNIPPETS = int(os.getenv(\"MAX_CTX_SNIPPETS\", \"3\")) MAX_CTX_CHARS = int(os.getenv(\"MAX_CTX_CHARS\", \"2400\")) app = FastAPI() class ChatRequest(BaseModel): question: str k: int = 4 max_tokens: int = 200 temperature: float = 0.1 debug: bool = False # <\u2014 when true, include prompt/messages in response def _label(meta: Dict[str, Any]) -> str: did = (meta or {}).get(\"doc_id\") sec = (meta or {}).get(\"section\") if not did: return \"unknown\" return f\"{did}#{sec}\" if sec and sec != \"full\" else did def _collect_citations(hits: List[Dict[str, Any]]) -> List[str]: seen, out = set(), [] for h in hits: lab = _label(h.get(\"meta\")) if lab not in seen: seen.add(lab); out.append(lab) return out def _normalize_hits(hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]: # Drop recent_queries from grounding context (they\u2019re noisy) filt = [] for h in hits: did = ((h.get(\"meta\") or {}).get(\"doc_id\") or \"\").lower() if did.startswith(\"recent_queries.jsonl\"): continue filt.append(h) # Prefer those with text first filt.sort(key=lambda h: (h.get(\"text\") is None), reverse=False) # Dedup by label seen, dedup = set(), [] for h in filt: lab = _label(h.get(\"meta\")) if lab in seen: continue seen.add(lab); dedup.append(h) # Trim by count and char budget total = 0; trimmed = [] for h in dedup: txt = h.get(\"text\") or (h.get(\"meta\") or {}).get(\"text\") or \"\" if len(trimmed) < MAX_CTX_SNIPPETS and total + len(txt) <= MAX_CTX_CHARS: trimmed.append(h); total += len(txt) if len(trimmed) >= MAX_CTX_SNIPPETS: break return trimmed def _strip_existing_source(txt: str) -> str: lines = txt.rstrip().splitlines() kept = [ln for ln in lines if not ln.strip().lower().startswith(\"source:\")] return \"\\n\".join(kept).rstrip() @app.get(\"/health\") def health(): return {\"ok\": True, \"retriever\": RETRIEVER_URL, \"vllm\": VLLM_URL} @app.get(\"/dryrun\") def dryrun(q: str = Query(..., alias=\"question\"), k: int = 4): \"\"\"Build exactly what /chat would send to vLLM, but don\u2019t call vLLM.\"\"\" with httpx.Client(timeout=30) as cx: r = cx.post(f\"{RETRIEVER_URL}/search\", json={\"query\": q, \"k\": k}) r.raise_for_status() raw_hits = r.json().get(\"hits\", []) ctx_hits = _normalize_hits(raw_hits) citations = _collect_citations(ctx_hits) messages = build_messages(q, ctx_hits) # Also surface the precise snippets we used (label + text) used_snippets = [] for h in ctx_hits: meta = h.get(\"meta\") or {} used_snippets.append({ \"label\": _label(meta), \"text\": h.get(\"text\") or meta.get(\"text\") or \"\" }) return { \"question\": q, \"citations\": citations, \"used_snippets\": used_snippets, # what the model will actually see \"messages\": messages, # the exact OpenAI Chat payload \"note\": \"This is a dry run; no LLM call was made.\" } @app.post(\"/chat\") def chat(req: ChatRequest): t0 = time.time() # 1) retrieve with httpx.Client(timeout=30) as cx: r = cx.post(f\"{RETRIEVER_URL}/search\", json={\"query\": req.question, \"k\": req.k}) r.raise_for_status() raw_hits = r.json().get(\"hits\", []) # 2) normalize + citations ctx_hits = _normalize_hits(raw_hits) citations = _collect_citations(ctx_hits) # 3) build messages with actual snippet text messages = build_messages(req.question, ctx_hits) # 4) call vLLM (OpenAI-compatible) temperature = max(0.0, min(req.temperature, 0.5)) max_tokens = min(req.max_tokens, 256) payload = { \"model\": MODEL_NAME, \"messages\": messages, \"temperature\": temperature, \"top_p\": 0.9, \"max_tokens\": max_tokens, \"stream\": False, } with httpx.Client(timeout=120) as cx: rr = cx.post(f\"{VLLM_URL}/v1/chat/completions\", json=payload) rr.raise_for_status() data = rr.json() content = data[\"choices\"][0][\"message\"][\"content\"] usage = data.get(\"usage\", {}) dt = time.time() - t0 content = _strip_existing_source(content) content = content + (\"\\nSource: \" + \"; \".join(citations) if citations else \"\\nSource: (none)\") resp = { \"answer\": content, \"citations\": citations, \"latency_seconds\": round(dt, 3), \"usage\": usage, } # 5) optional debug payload so you can inspect exactly what was sent if req.debug: used_snippets = [] for h in ctx_hits: meta = h.get(\"meta\") or {} used_snippets.append({ \"label\": _label(meta), \"text\": h.get(\"text\") or meta.get(\"text\") or \"\" }) resp[\"debug\"] = { \"messages\": messages, # exact system+user messages sent \"used_snippets\": used_snippets, \"raw_hits\": raw_hits[:10], # original retriever output (trimmed) \"payload_model\": MODEL_NAME, \"payload_temperature\": temperature, \"payload_max_tokens\": max_tokens, } return resp 3) K8s manifests for Chat API k8s/40-serve/cm-chat-api.yaml apiVersion: v1 kind: ConfigMap metadata: name: chat-api-config namespace: atharva-app data: RETRIEVER_URL: \"http://atharva-retriever.atharva-ml.svc.cluster.local:8001\" VLLM_URL: \"http://atharva-vllm.atharva-ml.svc.cluster.local:8000\" MODEL_NAME: \"smollm2-135m-atharva\" k8s/40-serve/deploy-chat-api.yaml apiVersion: apps/v1 kind: Deployment metadata: name: atharva-chat-api namespace: atharva-app spec: replicas: 1 selector: matchLabels: { app: atharva-chat-api } template: metadata: labels: { app: atharva-chat-api } spec: containers: - name: api image: python:3.11-slim workingDir: /workspace command: [\"bash\",\"-lc\"] args: - | pip install --no-cache-dir fastapi==0.112.2 uvicorn==0.30.6 httpx==0.27.2 uvicorn chat_api:app --host 0.0.0.0 --port 8080 --proxy-headers envFrom: - configMapRef: { name: chat-api-config } volumeMounts: - name: host mountPath: /workspace subPath: atharva-dental-assistant/serving ports: [{ containerPort: 8080 }] readinessProbe: httpGet: { path: /health, port: 8080 } initialDelaySeconds: 5 periodSeconds: 10 volumes: - name: host hostPath: { path: /mnt/project, type: Directory } k8s/40-serve/svc-chat-api.yaml apiVersion: v1 kind: Service metadata: name: atharva-chat-api namespace: atharva-app spec: type: NodePort selector: { app: atharva-chat-api } ports: - name: http port: 8080 targetPort: 8080 nodePort: 30300 (Optional) You can add an Ingress later; for local dev we\u2019ll use NodePort . 4) Helper scripts scripts/deploy_vllm.sh #!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/40-serve/rawdeployment-vllm.yaml kubectl apply -f k8s/40-serve/svc-vllm.yaml echo \"Waiting for vLLM Service endpoints...\" kubectl -n atharva-ml rollout status deploy/atharva-vllm-predictor --timeout=300s || true kubectl -n atharva-ml get pods -l app=vllm -o wide kubectl -n atharva-ml get svc atharva-vllm If the rollout status line errors (RawDeployment creates the Deployment; sometimes the generated name differs), don\u2019t worry\u2014check pods with the label app=vllm . If your cluster / KServe version behaves differently, just kubectl -n atharva-ml get deploy,pod to see the actual names. scripts/deploy_chat_api.sh #!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/40-serve/cm-chat-api.yaml kubectl apply -f k8s/40-serve/deploy-chat-api.yaml kubectl apply -f k8s/40-serve/svc-chat-api.yaml kubectl -n atharva-app rollout status deploy/atharva-chat-api --timeout=180s kubectl -n atharva-app get svc atharva-chat-api scripts/smoke_e2e.sh #!/usr/bin/env bash set -euo pipefail CHAT_HOST=\"${CHAT_HOST:-127.0.0.1}\" CHAT_PORT=\"${CHAT_PORT:-30300}\" VLLM_HOST=\"${VLLM_HOST:-127.0.0.1}\" VLLM_PORT=\"${VLLM_PORT:-30200}\" chat_url=\"http://${CHAT_HOST}:${CHAT_PORT}\" vllm_url=\"http://${VLLM_HOST}:${VLLM_PORT}\" echo \"=== End-to-End Test: Chat API -> Retriever -> vLLM ===\" echo \"Chat API: ${chat_url} vLLM: ${vllm_url}\" echo # 1) Health checks echo \"[1/4] Health: Chat API\" curl -sf \"${chat_url}/health\" | jq . || { echo \"Chat API health failed\"; exit 1; } echo echo \"[2/4] Health: vLLM (OpenAI models)\" curl -sf \"${vllm_url}/v1/models\" | jq '.data[0] // {}' || { echo \"vLLM models failed\"; exit 1; } echo # 2) Helper to run a chat and extract key fields ask() { local q=\"$1\"; local k=\"${2:-4}\"; local max_tokens=\"${3:-256}\"; local temp=\"${4:-0.3}\" echo \"Q: $q\" resp=\"$(curl -s -X POST \"${chat_url}/chat\" \\ -H 'content-type: application/json' \\ -d \"{\\\"question\\\":\\\"${q}\\\",\\\"k\\\":${k},\\\"max_tokens\\\":${max_tokens},\\\"temperature\\\":${temp}}\")\" # Pretty summary echo \"$resp\" | jq -r ' . as $r | \"\u2500 Answer \u2500\\n\" + ($r.answer // \"<no answer>\") + \"\\n\\n\" + \"\u2500 Citations \u2500\\n\" + ((($r.citations // [])|join(\"\\n\")) // \"<none>\") + \"\\n\\n\" + \"\u2500 Stats \u2500\\n\" + (\"latency_seconds: \" + (($r.latency_seconds // 0)|tostring)) + \"\\n\" + (\"prompt_tokens: \" + (($r.usage.prompt_tokens // 0)|tostring)) + \"\\n\" + (\"completion_tokens:\" + (($r.usage.completion_tokens // 0)|tostring)) + \"\\n\" ' echo \"-------------------------------------------\" } echo \"[3/4] Functional E2E prompts\" ask \"Are you open on Sundays ?\" ask \"How long does scaling take and what aftercare is needed?\" ask \"What is the typical cost range for a root canal and crown?\" 4 256 0.2 ask \"My face is badly swollen and I have a high fever after an extraction. What should I do?\" 4 192 0.1 # 3) Optional: short latency/tokens smoke loop echo echo \"[4/4] Throughput smoke (3 quick runs)\" for i in 1 2 3; do curl -s -X POST \"${chat_url}/chat\" \\ -H 'content-type: application/json' \\ -d '{\"question\":\"Is next-day pain after RCT normal? Suggest aftercare.\",\"k\":3,\"max_tokens\":192}' \\ | jq -r '\"run=\\($i) lat=\\(.latency_seconds)s tokens=(p:\\(.usage.prompt_tokens // 0), c:\\(.usage.completion_tokens // 0))\"' --arg i \"$i\" done echo echo \"\u2705 E2E complete.\" 5) Install Kserve Install and validate kserve using # Setup Cert Manager kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.0/cert-manager.yaml # Validate cert-manager components are running. kubectl get pods -n cert-manager -w # wait till cert-manager pods are up # Install Kserver CRDs helm install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd --version v0.15.2 --namespace kserve --create-namespace # Validate CRDs are deployed helm list -A # Install Kserver helm install kserve oci://ghcr.io/kserve/charts/kserve --version v0.15.2 \\ --namespace kserve \\ --set kserve.controller.deploymentMode=RawDeployment # Validate kserver is deployed helm list -A kubectl wait --for=condition=Available -n kserve deploy/kserve-controller-manager --timeout=300s kubectl get pods -n kserve 5) Run the lab From repo root: # 1) Deploy vLLM (KServe RawDeployment) that mounts the model via ImageVolume bash scripts/deploy_vllm.sh # 2) Deploy the Chat API (RAG \u2192 prompt \u2192 vLLM) bash scripts/deploy_chat_api.sh # 3) End-to-end smoke test bash scripts/smoke_e2e.sh Quick local validation 1) Model list curl -s http://127.0.0.1:30200/v1/models | jq . 2) chat API Test curl -s -X POST http://127.0.0.1:30300/chat -H \"content-type: application/json\" \\ -d '{\"question\":\"Are you open on Sundays?\",\"k\":4}' | jq . curl -s -X POST http://127.0.0.1:30300/chat -H \"content-type: application/json\" \\ -d '{\"question\":\"Whats the price range for Root Canal Therapy?\",\"k\":4}' | jq . 3) To see whats being sent to the LLM curl -s -X POST http://127.0.0.1:30300/chat \\\\n -H 'content-type: application/json' \\\\n -d '{\"question\":\"How long does scaling take and what aftercare is needed?\",\"k\":4,\"debug\":true}' \\\\n | jq -r '.debug.messages[0].content' curl -s -X POST http://127.0.0.1:30300/chat \\\\n -H 'content-type: application/json' \\\\n -d '{\"question\":\"Whats the price range for Root Canal Therapy?\",\"k\":4,\"debug\":true}' \\\\n | jq -r '.debug.messages[0].content' Expect answers in Atharva\u2019s concise, safety-minded style with a trailing Source: line and citations like treatments.json#TX-RCT-01 . Lab Summary This is what we accomplished in this lab Served your fine-tuned SmolLM2-135M using vLLM with model weights mounted from an ImageVolume . Exposed vLLM via a Service and wired a Chat API that performs RAG \u2192 prompt \u2192 LLM . Verified end-to-end responses and citations.","title":"Lab 04 - Serving with Kserve and vLLM"},{"location":"lab04/#lab-4-vllm-serving-kserve-rawdeployment-rag-aware-chat-api","text":"","title":"Lab 4 \u2014 vLLM Serving (KServe RawDeployment) + RAG-aware Chat API"},{"location":"lab04/#goal","text":"Serve the merged SmolLM2-135M from Lab 2 via vLLM . Mount the model snapshot with ImageVolumes . Deploy a Chat API (FastAPI) that: calls the Retriever (Lab 1) for top-k context, composes a safe system prompt (Pune/INR, clinic context), calls vLLM\u2019s OpenAI-compatible endpoint, returns answer + citations + basic timing/token stats. Namespaces: atharva-ml : vLLM server atharva-app : Chat API (or keep all in atharva-ml if you prefer)","title":"Goal"},{"location":"lab04/#repo-additions-this-lab","text":"atharva-dental-assistant/ \u251c\u2500 serving/ \u2502 \u251c\u2500 chat_api.py \u2502 \u2514\u2500 prompt_templates.py \u251c\u2500 k8s/ \u2502 \u2514\u2500 40-serve/ \u2502 \u251c\u2500 rawdeployment-vllm.yaml # KServe RawDeployment (preferred) \u2502 \u251c\u2500 svc-vllm.yaml \u2502 \u251c\u2500 deploy-chat-api.yaml \u2502 \u251c\u2500 svc-chat-api.yaml \u2502 \u2514\u2500 cm-chat-api.yaml \u2514\u2500 scripts/ \u251c\u2500 deploy_vllm.sh \u251c\u2500 deploy_chat_api.sh \u2514\u2500 smoke_e2e.sh Assumes you already built & pushed your model image to the local registry in Lab 3 as: kind-registry:5001/atharva/smollm2-135m-merged:v1 cd project/atharva-dental-assistant # Pull a vLLM image for CPU Inference docker image pull schoolofdevops/vllm-cpu-nonuma:0.9.1 # Load it onto KinD cluster kind load docker-image --name llmops-kind schoolofdevops/vllm-cpu-nonuma:0.9.1 --nodes llmops-kind-worker","title":"Repo additions (this lab)"},{"location":"lab04/#1-vllm-with-imagevolume-kserve-rawdeployment","text":"","title":"1) vLLM with ImageVolume (KServe RawDeployment)"},{"location":"lab04/#k8s40-serverawdeployment-vllmyaml","text":"apiVersion: serving.kserve.io/v1beta1 kind: InferenceService metadata: name: atharva-vllm namespace: atharva-ml annotations: autoscaling.knative.dev/metric: \"concurrency\" autoscaling.knative.dev/target: \"1\" spec: predictor: minReplicas: 1 maxReplicas: 1 containerConcurrency: 1 containers: - name: vllm image: schoolofdevops/vllm-cpu-nonuma:0.9.1 args: - --model=/models/model - --host=0.0.0.0 - --port=8000 # KServe/Knative-friendly - --max-model-len=2048 - --served-model-name=smollm2-135m-atharva - --dtype=float16 # keeps RAM lower on CPU for this tiny model - --disable-frontend-multiprocessing - --max-num-seqs=1 # clamp engine concurrency (OOM guard) - --swap-space=0.5 # GiB reserved for CPU KV cache (fits small pod) env: - name: VLLM_TARGET_DEVICE value: \"cpu\" - name: VLLM_CPU_KVCACHE_SPACE value: \"1\" - name: OMP_NUM_THREADS value: \"2\" - name: OPENBLAS_NUM_THREADS value: \"1\" - name: MKL_NUM_THREADS value: \"1\" - name: VLLM_CPU_OMP_THREADS_BIND value: \"0-1\" # avoid NUMA auto-binding path ports: - name: http1 containerPort: 8000 resources: requests: cpu: \"2\" memory: \"2Gi\" limits: cpu: \"2\" memory: \"3Gi\" # bump to 4Gi if you still see OOM readinessProbe: httpGet: path: /health port: 8000 initialDelaySeconds: 5 periodSeconds: 5 livenessProbe: httpGet: path: /health port: 8000 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: model mountPath: /models readOnly: true # Pinning to a node is fine; nodeSelector is usually nicer than nodeName: # nodeSelector: # kubernetes.io/hostname: llmops-kind-worker nodeName: llmops-kind-worker volumes: - name: model image: reference: initcron/smollm2-135m-merged:v3 pullPolicy: IfNotPresent where, replace initcron/smollm2-135m-merged:v2 with actual tag. If your cluster CRD doesn\u2019t support RawDeployment , you can temporarily deploy a plain Deployment (fallback) with the same container/volume spec . Keep going with the rest of the lab; the Chat API doesn\u2019t care how vLLM is deployed as long as the Service is up.","title":"k8s/40-serve/rawdeployment-vllm.yaml"},{"location":"lab04/#k8s40-servesvc-vllmyaml","text":"apiVersion: v1 kind: Service metadata: name: atharva-vllm namespace: atharva-ml spec: type: NodePort selector: # KServe RawDeployment pods are labeled with app=<deployment name> by default app: isvc.atharva-vllm-predictor ports: - name: http port: 8000 targetPort: 8000 nodePort: 30200","title":"k8s/40-serve/svc-vllm.yaml"},{"location":"lab04/#2-chat-api-rag-prompt-vllm","text":"mkdir serving","title":"2) Chat API (RAG \u2192 prompt \u2192 vLLM)"},{"location":"lab04/#servingprompt_templatespy","text":"SYSTEM_PROMPT = ( \"You are Atharva Dental Clinic assistant based in Pune, India. \" \"Respond in concise steps, use INR as currency for any prices or costs, be safety-minded, first try to find the answer from the context provided here.\" \"ask for missing info when necessary, and ALWAYS include a final 'Source:' line \" \"citing file#section for facts derived from context.\\n\" \"If the question indicates emergency red flags (uncontrolled bleeding, facial swelling, high fever, trauma), \" \"urge immediate contact with the clinic's emergency number.\\n\" ) def _label(meta: dict) -> str: did = (meta or {}).get(\"doc_id\") sec = (meta or {}).get(\"section\") if not did: return \"unknown\" return f\"{did}#{sec}\" if sec and sec != \"full\" else did def _render_context_block(retrieved_hits: list[dict]) -> str: \"\"\" Render only label + text, no Python dicts. \"\"\" blocks: list[str] = [] for h in retrieved_hits: meta = h.get(\"meta\") or {} label = _label(meta) text = (h.get(\"text\") or meta.get(\"text\") or \"\").strip() if not text: continue blocks.append(f\"### {label}\\n{text}\") return \"\\n\\n\".join(blocks).strip() def build_messages(user_q: str, retrieved_hits: list[dict]) -> list[dict]: context_block = _render_context_block(retrieved_hits) system = SYSTEM_PROMPT + \"\\nContext snippets:\\n\" + (context_block if context_block else \"(none)\") return [ {\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user_q.strip()}, ]","title":"serving/prompt_templates.py"},{"location":"lab04/#servingchat_apipy","text":"import os import time import httpx from fastapi import FastAPI, Query from pydantic import BaseModel from typing import List, Dict, Any from prompt_templates import build_messages RETRIEVER_URL = os.getenv(\"RETRIEVER_URL\", \"http://atharva-retriever.atharva-ml.svc.cluster.local:8001\") VLLM_URL = os.getenv(\"VLLM_URL\", \"http://atharva-vllm.atharva-ml.svc.cluster.local:8000\") MODEL_NAME = os.getenv(\"MODEL_NAME\", \"smollm2-135m-atharva\") MAX_CTX_SNIPPETS = int(os.getenv(\"MAX_CTX_SNIPPETS\", \"3\")) MAX_CTX_CHARS = int(os.getenv(\"MAX_CTX_CHARS\", \"2400\")) app = FastAPI() class ChatRequest(BaseModel): question: str k: int = 4 max_tokens: int = 200 temperature: float = 0.1 debug: bool = False # <\u2014 when true, include prompt/messages in response def _label(meta: Dict[str, Any]) -> str: did = (meta or {}).get(\"doc_id\") sec = (meta or {}).get(\"section\") if not did: return \"unknown\" return f\"{did}#{sec}\" if sec and sec != \"full\" else did def _collect_citations(hits: List[Dict[str, Any]]) -> List[str]: seen, out = set(), [] for h in hits: lab = _label(h.get(\"meta\")) if lab not in seen: seen.add(lab); out.append(lab) return out def _normalize_hits(hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]: # Drop recent_queries from grounding context (they\u2019re noisy) filt = [] for h in hits: did = ((h.get(\"meta\") or {}).get(\"doc_id\") or \"\").lower() if did.startswith(\"recent_queries.jsonl\"): continue filt.append(h) # Prefer those with text first filt.sort(key=lambda h: (h.get(\"text\") is None), reverse=False) # Dedup by label seen, dedup = set(), [] for h in filt: lab = _label(h.get(\"meta\")) if lab in seen: continue seen.add(lab); dedup.append(h) # Trim by count and char budget total = 0; trimmed = [] for h in dedup: txt = h.get(\"text\") or (h.get(\"meta\") or {}).get(\"text\") or \"\" if len(trimmed) < MAX_CTX_SNIPPETS and total + len(txt) <= MAX_CTX_CHARS: trimmed.append(h); total += len(txt) if len(trimmed) >= MAX_CTX_SNIPPETS: break return trimmed def _strip_existing_source(txt: str) -> str: lines = txt.rstrip().splitlines() kept = [ln for ln in lines if not ln.strip().lower().startswith(\"source:\")] return \"\\n\".join(kept).rstrip() @app.get(\"/health\") def health(): return {\"ok\": True, \"retriever\": RETRIEVER_URL, \"vllm\": VLLM_URL} @app.get(\"/dryrun\") def dryrun(q: str = Query(..., alias=\"question\"), k: int = 4): \"\"\"Build exactly what /chat would send to vLLM, but don\u2019t call vLLM.\"\"\" with httpx.Client(timeout=30) as cx: r = cx.post(f\"{RETRIEVER_URL}/search\", json={\"query\": q, \"k\": k}) r.raise_for_status() raw_hits = r.json().get(\"hits\", []) ctx_hits = _normalize_hits(raw_hits) citations = _collect_citations(ctx_hits) messages = build_messages(q, ctx_hits) # Also surface the precise snippets we used (label + text) used_snippets = [] for h in ctx_hits: meta = h.get(\"meta\") or {} used_snippets.append({ \"label\": _label(meta), \"text\": h.get(\"text\") or meta.get(\"text\") or \"\" }) return { \"question\": q, \"citations\": citations, \"used_snippets\": used_snippets, # what the model will actually see \"messages\": messages, # the exact OpenAI Chat payload \"note\": \"This is a dry run; no LLM call was made.\" } @app.post(\"/chat\") def chat(req: ChatRequest): t0 = time.time() # 1) retrieve with httpx.Client(timeout=30) as cx: r = cx.post(f\"{RETRIEVER_URL}/search\", json={\"query\": req.question, \"k\": req.k}) r.raise_for_status() raw_hits = r.json().get(\"hits\", []) # 2) normalize + citations ctx_hits = _normalize_hits(raw_hits) citations = _collect_citations(ctx_hits) # 3) build messages with actual snippet text messages = build_messages(req.question, ctx_hits) # 4) call vLLM (OpenAI-compatible) temperature = max(0.0, min(req.temperature, 0.5)) max_tokens = min(req.max_tokens, 256) payload = { \"model\": MODEL_NAME, \"messages\": messages, \"temperature\": temperature, \"top_p\": 0.9, \"max_tokens\": max_tokens, \"stream\": False, } with httpx.Client(timeout=120) as cx: rr = cx.post(f\"{VLLM_URL}/v1/chat/completions\", json=payload) rr.raise_for_status() data = rr.json() content = data[\"choices\"][0][\"message\"][\"content\"] usage = data.get(\"usage\", {}) dt = time.time() - t0 content = _strip_existing_source(content) content = content + (\"\\nSource: \" + \"; \".join(citations) if citations else \"\\nSource: (none)\") resp = { \"answer\": content, \"citations\": citations, \"latency_seconds\": round(dt, 3), \"usage\": usage, } # 5) optional debug payload so you can inspect exactly what was sent if req.debug: used_snippets = [] for h in ctx_hits: meta = h.get(\"meta\") or {} used_snippets.append({ \"label\": _label(meta), \"text\": h.get(\"text\") or meta.get(\"text\") or \"\" }) resp[\"debug\"] = { \"messages\": messages, # exact system+user messages sent \"used_snippets\": used_snippets, \"raw_hits\": raw_hits[:10], # original retriever output (trimmed) \"payload_model\": MODEL_NAME, \"payload_temperature\": temperature, \"payload_max_tokens\": max_tokens, } return resp","title":"serving/chat_api.py"},{"location":"lab04/#3-k8s-manifests-for-chat-api","text":"","title":"3) K8s manifests for Chat API"},{"location":"lab04/#k8s40-servecm-chat-apiyaml","text":"apiVersion: v1 kind: ConfigMap metadata: name: chat-api-config namespace: atharva-app data: RETRIEVER_URL: \"http://atharva-retriever.atharva-ml.svc.cluster.local:8001\" VLLM_URL: \"http://atharva-vllm.atharva-ml.svc.cluster.local:8000\" MODEL_NAME: \"smollm2-135m-atharva\"","title":"k8s/40-serve/cm-chat-api.yaml"},{"location":"lab04/#k8s40-servedeploy-chat-apiyaml","text":"apiVersion: apps/v1 kind: Deployment metadata: name: atharva-chat-api namespace: atharva-app spec: replicas: 1 selector: matchLabels: { app: atharva-chat-api } template: metadata: labels: { app: atharva-chat-api } spec: containers: - name: api image: python:3.11-slim workingDir: /workspace command: [\"bash\",\"-lc\"] args: - | pip install --no-cache-dir fastapi==0.112.2 uvicorn==0.30.6 httpx==0.27.2 uvicorn chat_api:app --host 0.0.0.0 --port 8080 --proxy-headers envFrom: - configMapRef: { name: chat-api-config } volumeMounts: - name: host mountPath: /workspace subPath: atharva-dental-assistant/serving ports: [{ containerPort: 8080 }] readinessProbe: httpGet: { path: /health, port: 8080 } initialDelaySeconds: 5 periodSeconds: 10 volumes: - name: host hostPath: { path: /mnt/project, type: Directory }","title":"k8s/40-serve/deploy-chat-api.yaml"},{"location":"lab04/#k8s40-servesvc-chat-apiyaml","text":"apiVersion: v1 kind: Service metadata: name: atharva-chat-api namespace: atharva-app spec: type: NodePort selector: { app: atharva-chat-api } ports: - name: http port: 8080 targetPort: 8080 nodePort: 30300 (Optional) You can add an Ingress later; for local dev we\u2019ll use NodePort .","title":"k8s/40-serve/svc-chat-api.yaml"},{"location":"lab04/#4-helper-scripts","text":"","title":"4) Helper scripts"},{"location":"lab04/#scriptsdeploy_vllmsh","text":"#!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/40-serve/rawdeployment-vllm.yaml kubectl apply -f k8s/40-serve/svc-vllm.yaml echo \"Waiting for vLLM Service endpoints...\" kubectl -n atharva-ml rollout status deploy/atharva-vllm-predictor --timeout=300s || true kubectl -n atharva-ml get pods -l app=vllm -o wide kubectl -n atharva-ml get svc atharva-vllm If the rollout status line errors (RawDeployment creates the Deployment; sometimes the generated name differs), don\u2019t worry\u2014check pods with the label app=vllm . If your cluster / KServe version behaves differently, just kubectl -n atharva-ml get deploy,pod to see the actual names.","title":"scripts/deploy_vllm.sh"},{"location":"lab04/#scriptsdeploy_chat_apish","text":"#!/usr/bin/env bash set -euo pipefail kubectl apply -f k8s/40-serve/cm-chat-api.yaml kubectl apply -f k8s/40-serve/deploy-chat-api.yaml kubectl apply -f k8s/40-serve/svc-chat-api.yaml kubectl -n atharva-app rollout status deploy/atharva-chat-api --timeout=180s kubectl -n atharva-app get svc atharva-chat-api","title":"scripts/deploy_chat_api.sh"},{"location":"lab04/#scriptssmoke_e2esh","text":"#!/usr/bin/env bash set -euo pipefail CHAT_HOST=\"${CHAT_HOST:-127.0.0.1}\" CHAT_PORT=\"${CHAT_PORT:-30300}\" VLLM_HOST=\"${VLLM_HOST:-127.0.0.1}\" VLLM_PORT=\"${VLLM_PORT:-30200}\" chat_url=\"http://${CHAT_HOST}:${CHAT_PORT}\" vllm_url=\"http://${VLLM_HOST}:${VLLM_PORT}\" echo \"=== End-to-End Test: Chat API -> Retriever -> vLLM ===\" echo \"Chat API: ${chat_url} vLLM: ${vllm_url}\" echo # 1) Health checks echo \"[1/4] Health: Chat API\" curl -sf \"${chat_url}/health\" | jq . || { echo \"Chat API health failed\"; exit 1; } echo echo \"[2/4] Health: vLLM (OpenAI models)\" curl -sf \"${vllm_url}/v1/models\" | jq '.data[0] // {}' || { echo \"vLLM models failed\"; exit 1; } echo # 2) Helper to run a chat and extract key fields ask() { local q=\"$1\"; local k=\"${2:-4}\"; local max_tokens=\"${3:-256}\"; local temp=\"${4:-0.3}\" echo \"Q: $q\" resp=\"$(curl -s -X POST \"${chat_url}/chat\" \\ -H 'content-type: application/json' \\ -d \"{\\\"question\\\":\\\"${q}\\\",\\\"k\\\":${k},\\\"max_tokens\\\":${max_tokens},\\\"temperature\\\":${temp}}\")\" # Pretty summary echo \"$resp\" | jq -r ' . as $r | \"\u2500 Answer \u2500\\n\" + ($r.answer // \"<no answer>\") + \"\\n\\n\" + \"\u2500 Citations \u2500\\n\" + ((($r.citations // [])|join(\"\\n\")) // \"<none>\") + \"\\n\\n\" + \"\u2500 Stats \u2500\\n\" + (\"latency_seconds: \" + (($r.latency_seconds // 0)|tostring)) + \"\\n\" + (\"prompt_tokens: \" + (($r.usage.prompt_tokens // 0)|tostring)) + \"\\n\" + (\"completion_tokens:\" + (($r.usage.completion_tokens // 0)|tostring)) + \"\\n\" ' echo \"-------------------------------------------\" } echo \"[3/4] Functional E2E prompts\" ask \"Are you open on Sundays ?\" ask \"How long does scaling take and what aftercare is needed?\" ask \"What is the typical cost range for a root canal and crown?\" 4 256 0.2 ask \"My face is badly swollen and I have a high fever after an extraction. What should I do?\" 4 192 0.1 # 3) Optional: short latency/tokens smoke loop echo echo \"[4/4] Throughput smoke (3 quick runs)\" for i in 1 2 3; do curl -s -X POST \"${chat_url}/chat\" \\ -H 'content-type: application/json' \\ -d '{\"question\":\"Is next-day pain after RCT normal? Suggest aftercare.\",\"k\":3,\"max_tokens\":192}' \\ | jq -r '\"run=\\($i) lat=\\(.latency_seconds)s tokens=(p:\\(.usage.prompt_tokens // 0), c:\\(.usage.completion_tokens // 0))\"' --arg i \"$i\" done echo echo \"\u2705 E2E complete.\"","title":"scripts/smoke_e2e.sh"},{"location":"lab04/#5-install-kserve","text":"Install and validate kserve using # Setup Cert Manager kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.0/cert-manager.yaml # Validate cert-manager components are running. kubectl get pods -n cert-manager -w # wait till cert-manager pods are up # Install Kserver CRDs helm install kserve-crd oci://ghcr.io/kserve/charts/kserve-crd --version v0.15.2 --namespace kserve --create-namespace # Validate CRDs are deployed helm list -A # Install Kserver helm install kserve oci://ghcr.io/kserve/charts/kserve --version v0.15.2 \\ --namespace kserve \\ --set kserve.controller.deploymentMode=RawDeployment # Validate kserver is deployed helm list -A kubectl wait --for=condition=Available -n kserve deploy/kserve-controller-manager --timeout=300s kubectl get pods -n kserve","title":"5) Install Kserve"},{"location":"lab04/#5-run-the-lab","text":"From repo root: # 1) Deploy vLLM (KServe RawDeployment) that mounts the model via ImageVolume bash scripts/deploy_vllm.sh # 2) Deploy the Chat API (RAG \u2192 prompt \u2192 vLLM) bash scripts/deploy_chat_api.sh # 3) End-to-end smoke test bash scripts/smoke_e2e.sh","title":"5) Run the lab"},{"location":"lab04/#quick-local-validation","text":"","title":"Quick local validation"},{"location":"lab04/#1-model-list","text":"curl -s http://127.0.0.1:30200/v1/models | jq .","title":"1) Model list"},{"location":"lab04/#2-chat-api-test","text":"curl -s -X POST http://127.0.0.1:30300/chat -H \"content-type: application/json\" \\ -d '{\"question\":\"Are you open on Sundays?\",\"k\":4}' | jq . curl -s -X POST http://127.0.0.1:30300/chat -H \"content-type: application/json\" \\ -d '{\"question\":\"Whats the price range for Root Canal Therapy?\",\"k\":4}' | jq .","title":"2) chat API Test"},{"location":"lab04/#3-to-see-whats-being-sent-to-the-llm","text":"curl -s -X POST http://127.0.0.1:30300/chat \\\\n -H 'content-type: application/json' \\\\n -d '{\"question\":\"How long does scaling take and what aftercare is needed?\",\"k\":4,\"debug\":true}' \\\\n | jq -r '.debug.messages[0].content' curl -s -X POST http://127.0.0.1:30300/chat \\\\n -H 'content-type: application/json' \\\\n -d '{\"question\":\"Whats the price range for Root Canal Therapy?\",\"k\":4,\"debug\":true}' \\\\n | jq -r '.debug.messages[0].content' Expect answers in Atharva\u2019s concise, safety-minded style with a trailing Source: line and citations like treatments.json#TX-RCT-01 .","title":"3) To see whats being sent to the LLM"},{"location":"lab04/#lab-summary","text":"This is what we accomplished in this lab Served your fine-tuned SmolLM2-135M using vLLM with model weights mounted from an ImageVolume . Exposed vLLM via a Service and wired a Chat API that performs RAG \u2192 prompt \u2192 LLM . Verified end-to-end responses and citations.","title":"Lab Summary"},{"location":"lab040/","text":"Lab 4.0 : Testing vLLM on MacOS Pull the vLLM Image created with CPU Only Inference with docker image pull schoolofdevops/vllm-cpu-nonuma:0.9.1 Export the merged model built earlier as a model directory on your Mac e.g. export MODEL_DIR=/Users/gshah/work/llmops/code/project/atharva-dental-assistant/artifacts/train/20251001-084805/merged-model replace the path with actual path to merged-model Launch the vLLM using CPU only mode with options which should work on MacOS Silicon docker run --rm -p 8123:8000 --name vllmtest \\ --security-opt seccomp=unconfined \\ -e VLLM_TARGET_DEVICE=cpu \\ -e OMP_NUM_THREADS=2 -e OPENBLAS_NUM_THREADS=1 -e MKL_NUM_THREADS=1 \\ -e VLLM_CPU_KVCACHE_SPACE=1 \\ -v \"$MODEL_DIR:/models/model:ro\" \\ schoolofdevops/vllm-cpu-nonuma:0.9.1 \\ --model /models/model \\ --host 0.0.0.0 \\ --port 8000 \\ --max-model-len 2048 \\ --served-model-name smollm2-135m-atharva \\ --dtype float16 \\ --disable-frontend-multiprocessing Quick local validation 1) Model list curl -s http://127.0.0.1:8123/v1/models | jq . 2) Simple chat completion curl -s -X POST http://127.0.0.1:8123/v1/chat/completions \\ -H 'Content-Type: application/json' \\ -d '{ \"model\":\"smollm2-135m-atharva\", \"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}], \"max_tokens\": 32, \"temperature\": 0.2 }' | jq . curl -s -X POST http://127.0.0.1:30200/v1/chat/completions \\ -H 'Content-Type: application/json' \\ -d '{ \"model\":\"smollm2-135m-atharva\", \"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}], \"max_tokens\": 32, \"temperature\": 0.2 }' | jq . 3) Legacy completions (optional) curl -s -X POST http://127.0.0.1:8123/v1/completions \\ -H 'Content-Type: application/json' \\ -d '{ \"model\":\"smollm2-135m-atharva\", \"prompt\":\"Write a haiku about dentists:\", \"max_tokens\": 40 }' | jq . 4) Embeddings (optional / Not Available) curl -s -X POST http://127.0.0.1:8123/v1/embeddings \\ -H 'Content-Type: application/json' \\ -d '{ \"model\":\"smollm2-135m-atharva\", \"input\":\"dental care tips\" }' | jq . tip: on CPU, --dtype float32 or bfloat16 is usually safer than float16 . You can leave it as-is since it works; if you hit oddities, try --dtype float32 . Delete the container with docker rm -f vllmtest Load the Model to KIND Cluster Load it to a specific node llmops-kind-worker2 kind load docker-image schoolofdevops/vllm-cpu-nonuma:0.9.1 --name llmops-kind --nodes llmops-kind-worker courses/llmops/labs/v1","title":"Lab 4.0 : Testing vLLM on MacOS"},{"location":"lab040/#lab-40-testing-vllm-on-macos","text":"Pull the vLLM Image created with CPU Only Inference with docker image pull schoolofdevops/vllm-cpu-nonuma:0.9.1 Export the merged model built earlier as a model directory on your Mac e.g. export MODEL_DIR=/Users/gshah/work/llmops/code/project/atharva-dental-assistant/artifacts/train/20251001-084805/merged-model replace the path with actual path to merged-model Launch the vLLM using CPU only mode with options which should work on MacOS Silicon docker run --rm -p 8123:8000 --name vllmtest \\ --security-opt seccomp=unconfined \\ -e VLLM_TARGET_DEVICE=cpu \\ -e OMP_NUM_THREADS=2 -e OPENBLAS_NUM_THREADS=1 -e MKL_NUM_THREADS=1 \\ -e VLLM_CPU_KVCACHE_SPACE=1 \\ -v \"$MODEL_DIR:/models/model:ro\" \\ schoolofdevops/vllm-cpu-nonuma:0.9.1 \\ --model /models/model \\ --host 0.0.0.0 \\ --port 8000 \\ --max-model-len 2048 \\ --served-model-name smollm2-135m-atharva \\ --dtype float16 \\ --disable-frontend-multiprocessing","title":"Lab 4.0 : Testing vLLM on MacOS"},{"location":"lab040/#quick-local-validation","text":"","title":"Quick local validation"},{"location":"lab040/#1-model-list","text":"curl -s http://127.0.0.1:8123/v1/models | jq .","title":"1) Model list"},{"location":"lab040/#2-simple-chat-completion","text":"curl -s -X POST http://127.0.0.1:8123/v1/chat/completions \\ -H 'Content-Type: application/json' \\ -d '{ \"model\":\"smollm2-135m-atharva\", \"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}], \"max_tokens\": 32, \"temperature\": 0.2 }' | jq . curl -s -X POST http://127.0.0.1:30200/v1/chat/completions \\ -H 'Content-Type: application/json' \\ -d '{ \"model\":\"smollm2-135m-atharva\", \"messages\":[{\"role\":\"user\",\"content\":\"Say hello in 5 words.\"}], \"max_tokens\": 32, \"temperature\": 0.2 }' | jq .","title":"2) Simple chat completion"},{"location":"lab040/#3-legacy-completions-optional","text":"curl -s -X POST http://127.0.0.1:8123/v1/completions \\ -H 'Content-Type: application/json' \\ -d '{ \"model\":\"smollm2-135m-atharva\", \"prompt\":\"Write a haiku about dentists:\", \"max_tokens\": 40 }' | jq .","title":"3) Legacy completions (optional)"},{"location":"lab040/#4-embeddings-optional-not-available","text":"curl -s -X POST http://127.0.0.1:8123/v1/embeddings \\ -H 'Content-Type: application/json' \\ -d '{ \"model\":\"smollm2-135m-atharva\", \"input\":\"dental care tips\" }' | jq . tip: on CPU, --dtype float32 or bfloat16 is usually safer than float16 . You can leave it as-is since it works; if you hit oddities, try --dtype float32 . Delete the container with docker rm -f vllmtest","title":"4) Embeddings (optional / Not Available)"},{"location":"lab040/#load-the-model-to-kind-cluster","text":"Load it to a specific node llmops-kind-worker2 kind load docker-image schoolofdevops/vllm-cpu-nonuma:0.9.1 --name llmops-kind --nodes llmops-kind-worker","title":"Load the Model to KIND Cluster"},{"location":"lab040/#coursesllmopslabsv1","text":"","title":"courses/llmops/labs/v1"},{"location":"lab05/","text":"Lab 5 \u2014 Observe vLLM + RAG + Chat API Goal Export system + model metrics : vLLM: TTFT, latency histograms, tokens/sec, request counts (already exposed when --enable-metrics is on). Chat API: end-to-end latency, prompt/response token counts, request/5xx counts. Retriever: retrieval latency and top-k stats. Scrape with Prometheus (kube-prometheus-stack) . Visualize with Grafana . From Lab 0 we installed kube-prometheus-stack with prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \u2014 so any ServiceMonitor is scraped (no special labels needed). What\u2019s new in this lab atharva-dental-assistant/ \u251c\u2500 serving/ \u2502 \u251c\u2500 chat_api.py # UPDATED: Prometheus metrics (/metrics) \u2502 \u2514\u2500 prompt_templates.py \u251c\u2500 rag/ \u2502 \u2514\u2500 retriever.py # UPDATED: Prometheus metrics (/metrics) \u251c\u2500 k8s/ \u2502 \u2514\u2500 50-observability/ \u2502 \u251c\u2500 sm-vllm.yaml # ServiceMonitor for vLLM \u2502 \u251c\u2500 sm-chat-api.yaml # ServiceMonitor for Chat API \u2502 \u251c\u2500 sm-retriever.yaml # ServiceMonitor for Retriever \u2502 \u251c\u2500 pr-alerts.yaml # (optional) basic alert rules \u2502 \u2514\u2500 cm-grafana-dashboard.yaml \u2514\u2500 scripts/ \u2514\u2500 deploy_observability.sh 0) Setup Prometheus Add prometheus repo for helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Setup prometheus stack along with grafana with helm upgrade --install kps -n monitoring \\ prometheus-community/kube-prometheus-stack \\ --create-namespace \\ --set grafana.service.type=NodePort \\ --set grafana.service.nodePort=30400 \\ --set prometheus.service.type=NodePort \\ --set prometheus.service.nodePort=30500 \\ --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false validate kubectl rollout status -n monitoring deploy/kps-grafana --timeout=300s kubectl get pods -n monitoring Grafana : http://localhost:30400/ user: admin pass: prom-operator Prometheus : http://localhost:30500/ 1) Instrument the services A) Chat API \u2014 add Prometheus metrics ( serving/chat_api.py ) Replace your file with this version (differences: imports, metrics, /metrics endpoint, timings): import os import time import httpx from fastapi import FastAPI, Query, Response from pydantic import BaseModel from typing import List, Dict, Any from prometheus_client import ( Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST, ) from prompt_templates import build_messages RETRIEVER_URL = os.getenv(\"RETRIEVER_URL\", \"http://atharva-retriever.atharva-ml.svc.cluster.local:8001\") VLLM_URL = os.getenv(\"VLLM_URL\", \"http://atharva-vllm.atharva-ml.svc.cluster.local:8000\") MODEL_NAME = os.getenv(\"MODEL_NAME\", \"smollm2-135m-atharva\") MAX_CTX_SNIPPETS = int(os.getenv(\"MAX_CTX_SNIPPETS\", \"3\")) MAX_CTX_CHARS = int(os.getenv(\"MAX_CTX_CHARS\", \"2400\")) app = FastAPI() # ----------------------------- # Prometheus metrics # ----------------------------- REQS = Counter(\"chat_requests_total\", \"Total Chat API requests\", [\"route\"]) ERRS = Counter(\"chat_errors_total\", \"Total Chat API errors\", [\"stage\"]) E2E_LAT = Histogram( \"chat_end_to_end_latency_seconds\", \"End-to-end /chat latency in seconds\", buckets=(0.05, 0.1, 0.25, 0.5, 1, 2, 3, 5, 8, 13), ) RAG_LAT = Histogram( \"rag_retrieval_latency_seconds\", \"Retriever call latency in seconds\", buckets=(0.01, 0.02, 0.05, 0.1, 0.2, 0.4, 0.8, 1.5, 3), ) VLLM_LAT = Histogram( \"vllm_request_latency_seconds\", \"vLLM chat/completions call latency in seconds\", buckets=(0.05, 0.1, 0.25, 0.5, 1, 2, 3, 5, 8, 13), ) TOK_PROMPT = Gauge(\"chat_prompt_tokens\", \"Prompt tokens for the last completed /chat request\") TOK_COMPLETION = Gauge(\"chat_completion_tokens\", \"Completion tokens for the last completed /chat request\") TOK_TOTAL = Gauge(\"chat_total_tokens\", \"Total tokens for the last completed /chat request\") class ChatRequest(BaseModel): question: str k: int = 4 max_tokens: int = 200 temperature: float = 0.1 debug: bool = False # when true, include prompt/messages in response def _label(meta: Dict[str, Any]) -> str: did = (meta or {}).get(\"doc_id\") sec = (meta or {}).get(\"section\") if not did: return \"unknown\" return f\"{did}#{sec}\" if sec and sec != \"full\" else did def _collect_citations(hits: List[Dict[str, Any]]) -> List[str]: seen, out = set(), [] for h in hits: lab = _label(h.get(\"meta\")) if lab not in seen: seen.add(lab) out.append(lab) return out def _normalize_hits(hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]: # Drop recent_queries from grounding context (they\u2019re noisy) filt = [] for h in hits: did = ((h.get(\"meta\") or {}).get(\"doc_id\") or \"\").lower() if did.startswith(\"recent_queries.jsonl\"): continue filt.append(h) # Prefer those with text first filt.sort(key=lambda h: (h.get(\"text\") is None), reverse=False) # Dedup by label seen, dedup = set(), [] for h in filt: lab = _label(h.get(\"meta\")) if lab in seen: continue seen.add(lab) dedup.append(h) # Trim by count and char budget total = 0 trimmed = [] for h in dedup: txt = h.get(\"text\") or (h.get(\"meta\") or {}).get(\"text\") or \"\" if len(trimmed) < MAX_CTX_SNIPPETS and total + len(txt) <= MAX_CTX_CHARS: trimmed.append(h) total += len(txt) if len(trimmed) >= MAX_CTX_SNIPPETS: break return trimmed def _strip_existing_source(txt: str) -> str: lines = txt.rstrip().splitlines() kept = [ln for ln in lines if not ln.strip().lower().startswith(\"source:\")] return \"\\n\".join(kept).rstrip() @app.get(\"/health\") def health(): return {\"ok\": True, \"retriever\": RETRIEVER_URL, \"vllm\": VLLM_URL, \"model\": MODEL_NAME} @app.get(\"/metrics\") def metrics(): # Expose Prometheus metrics return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST) @app.get(\"/dryrun\") def dryrun(q: str = Query(..., alias=\"question\"), k: int = 4): \"\"\"Build exactly what /chat would send to vLLM, but don\u2019t call vLLM.\"\"\" REQS.labels(route=\"/dryrun\").inc() with httpx.Client(timeout=30) as cx: t_r0 = time.time() try: r = cx.post(f\"{RETRIEVER_URL}/search\", json={\"query\": q, \"k\": k}) r.raise_for_status() raw_hits = r.json().get(\"hits\", []) except Exception: ERRS.labels(stage=\"retriever\").inc() raise finally: RAG_LAT.observe(time.time() - t_r0) ctx_hits = _normalize_hits(raw_hits) citations = _collect_citations(ctx_hits) messages = build_messages(q, ctx_hits) # Also surface the precise snippets we used (label + text) used_snippets = [] for h in ctx_hits: meta = h.get(\"meta\") or {} used_snippets.append({ \"label\": _label(meta), \"text\": h.get(\"text\") or meta.get(\"text\") or \"\" }) return { \"question\": q, \"citations\": citations, \"used_snippets\": used_snippets, # what the model will actually see \"messages\": messages, # the exact OpenAI Chat payload \"note\": \"This is a dry run; no LLM call was made.\" } @app.post(\"/chat\") def chat(req: ChatRequest): REQS.labels(route=\"/chat\").inc() t0 = time.time() # 1) retrieve with httpx.Client(timeout=30) as cx: t_r0 = time.time() try: r = cx.post(f\"{RETRIEVER_URL}/search\", json={\"query\": req.question, \"k\": req.k}) r.raise_for_status() raw_hits = r.json().get(\"hits\", []) except Exception: ERRS.labels(stage=\"retriever\").inc() raise finally: RAG_LAT.observe(time.time() - t_r0) # 2) normalize + citations ctx_hits = _normalize_hits(raw_hits) citations = _collect_citations(ctx_hits) # 3) build messages with actual snippet text messages = build_messages(req.question, ctx_hits) # 4) call vLLM (OpenAI-compatible) temperature = max(0.0, min(req.temperature, 0.5)) max_tokens = min(req.max_tokens, 256) payload = { \"model\": MODEL_NAME, \"messages\": messages, \"temperature\": temperature, \"top_p\": 0.9, \"max_tokens\": max_tokens, \"stream\": False, } with httpx.Client(timeout=120) as cx: t_llm0 = time.time() try: rr = cx.post(f\"{VLLM_URL}/v1/chat/completions\", json=payload) rr.raise_for_status() data = rr.json() except Exception: ERRS.labels(stage=\"vllm\").inc() raise finally: VLLM_LAT.observe(time.time() - t_llm0) content = data[\"choices\"][0][\"message\"][\"content\"] usage = data.get(\"usage\", {}) dt = time.time() - t0 content = _strip_existing_source(content) content = content + (\"\\nSource: \" + \"; \".join(citations) if citations else \"\\nSource: (none)\") # Update token gauges (best-effort) try: TOK_PROMPT.set(float(usage.get(\"prompt_tokens\", 0) or 0)) TOK_COMPLETION.set(float(usage.get(\"completion_tokens\", 0) or 0)) TOK_TOTAL.set(float(usage.get(\"total_tokens\", 0) or 0)) except Exception: # Avoid failing the request if usage is missing or malformed pass # Observe end-to-end latency E2E_LAT.observe(dt) resp = { \"answer\": content, \"citations\": citations, \"latency_seconds\": round(dt, 3), \"usage\": usage, } # 5) optional debug payload so you can inspect exactly what was sent if req.debug: used_snippets = [] for h in ctx_hits: meta = h.get(\"meta\") or {} used_snippets.append({ \"label\": _label(meta), \"text\": h.get(\"text\") or meta.get(\"text\") or \"\" }) resp[\"debug\"] = { \"messages\": messages, # exact system+user messages sent \"used_snippets\": used_snippets, \"raw_hits\": raw_hits[:10], # original retriever output (trimmed) \"payload_model\": MODEL_NAME, \"payload_temperature\": temperature, \"payload_max_tokens\": max_tokens, } return resp Update deployment spec to install prometheus-client>=0.20.0 . File: k8s/40-serve/deploy-chat-api.yaml command: [\"bash\",\"-lc\"] args: - | pip install --no-cache-dir fastapi==0.112.2 uvicorn==0.30.6 httpx==0.27.2 prometheus-client>=0.20.0 uvicorn chat_api:app --host 0.0.0.0 --port 8080 --proxy-headers Redeploy chat api as kubectl delete -f k8s/40-serve/deploy-chat-api.yaml kubectl apply -f k8s/40-serve/deploy-chat-api.yaml B) Retriever \u2014 add Prometheus metrics ( rag/retriever.py ) Replace your retriever with this version (adds /metrics , latency histogram, request counter): import os import json from pathlib import Path from typing import List, Optional, Tuple, Any from fastapi import FastAPI, HTTPException, Response from pydantic import BaseModel # --- Prometheus (only new dependency) --- from prometheus_client import ( Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST, ) BACKEND = os.getenv(\"BACKEND\", \"dense\") # \"sparse\" or \"dense\" INDEX_PATH = Path(os.getenv(\"INDEX_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/index.faiss\")) META_PATH = Path(os.getenv(\"META_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/meta.json\")) MODEL_DIR = os.getenv(\"MODEL_DIR\") # optional for dense MODEL_NAME = os.getenv(\"MODEL_NAME\", \"sentence-transformers/all-MiniLM-L6-v2\") app = FastAPI(title=f\"Atharva Retriever ({BACKEND})\") class SearchRequest(BaseModel): query: str k: int = 4 _ready_reason = \"starting\" _model = None; _index = None; _meta: List[dict] = [] _vec = None; _X = None # sparse objects # ------------------ Prometheus metrics (added) ------------------ REQS_TOTAL = Counter(\"retriever_requests_total\", \"Total /search requests\") ERRS_TOTAL = Counter(\"retriever_errors_total\", \"Total retriever errors\", [\"stage\"]) E2E_LAT = Histogram( \"retriever_search_latency_seconds\", \"End-to-end /search latency (s)\", buckets=(0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2), ) # Dense sub-steps ENC_LAT = Histogram( \"retriever_dense_encode_latency_seconds\", \"SentenceTransformer.encode latency (s)\", buckets=(0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2), ) FAISS_LAT = Histogram( \"retriever_dense_faiss_latency_seconds\", \"FAISS search latency (s)\", buckets=(0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1), ) # Sparse sub-steps VEC_LAT = Histogram( \"retriever_sparse_vectorize_latency_seconds\", \"TF-IDF vectorizer.transform latency (s)\", buckets=(0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1), ) DOT_LAT = Histogram( \"retriever_sparse_dot_latency_seconds\", \"Sparse dot/matmul latency (s)\", buckets=(0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1), ) # Load-time & sizes MODEL_LOAD_SEC = Gauge(\"retriever_model_load_seconds\", \"Dense model load time (s)\") INDEX_LOAD_SEC = Gauge(\"retriever_index_load_seconds\", \"FAISS index load time (s)\") SPARSE_VEC_LOAD_SEC = Gauge(\"retriever_sparse_vectorizer_load_seconds\", \"TF-IDF vectorizer load time (s)\") SPARSE_MAT_LOAD_SEC = Gauge(\"retriever_sparse_matrix_load_seconds\", \"TF-IDF matrix load time (s)\") INDEX_ITEMS = Gauge(\"retriever_index_items\", \"Items in index/matrix\") META_ITEMS = Gauge(\"retriever_meta_items\", \"Number of meta records\") # ------------------ Utils ------------------ def _normalize_meta_loaded(data: Any) -> List[dict]: \"\"\" Accepts various shapes of meta.json and returns a list of entries. Supported: - list[dict] - {\"items\": [...]} (common pattern) - {\"hits\": [...]} (fallback) \"\"\" if isinstance(data, list): return data if isinstance(data, dict): # keep original behavior if \"items\" in data and isinstance(data[\"items\"], list): return data[\"items\"] if \"hits\" in data and isinstance(data[\"hits\"], list): return data[\"hits\"] raise ValueError(\"META_PATH must contain a list or a dict with 'items'/'hits'.\") def _parse_doc_and_section(path: Optional[str]) -> Tuple[str, Optional[str]]: \"\"\" Parse labels from meta.path: - 'treatments.json#0' -> ('treatments.json', '0') - 'faq.md' -> ('faq.md', None) - 'policies/emergency.md' -> ('policies/emergency.md', None) \"\"\" if not path: return \"unknown\", None if \"#\" in path: d, s = path.split(\"#\", 1) return d, s return path, None def _extract_text(m: dict) -> Optional[str]: \"\"\"Try common keys for stored chunk text.\"\"\" return m.get(\"text\") or m.get(\"chunk\") or m.get(\"content\") def _enrich_hit(idx: int, score: float) -> dict: \"\"\" Build a single enriched hit from meta[idx]. \"\"\" if idx < 0 or idx >= len(_meta): doc_id, section, path, typ, txt = \"unknown\", None, None, None, None else: m = _meta[idx] or {} path = m.get(\"path\") typ = m.get(\"type\") doc_id, section = _parse_doc_and_section(path) txt = _extract_text(m) hit = { \"score\": float(score), \"meta\": { \"doc_id\": doc_id, \"section\": section, \"path\": path, \"type\": typ, }, } if txt: hit[\"text\"] = txt return hit # ------------------ Loaders (unchanged behavior; just timed gauges) ------------------ def _load_dense(): global _model, _index, _meta try: import time as _t import faiss from sentence_transformers import SentenceTransformer t0 = _t.time() _model = SentenceTransformer(MODEL_DIR) if (MODEL_DIR and Path(MODEL_DIR).exists()) else SentenceTransformer(MODEL_NAME) MODEL_LOAD_SEC.set(_t.time() - t0) t1 = _t.time() _index = faiss.read_index(str(INDEX_PATH)) INDEX_LOAD_SEC.set(_t.time() - t1) _meta = _normalize_meta_loaded(json.loads(META_PATH.read_text(encoding=\"utf-8\"))) META_ITEMS.set(len(_meta) if isinstance(_meta, list) else 0) # best-effort size (for FAISS) try: INDEX_ITEMS.set(int(getattr(_index, \"ntotal\", len(_meta)))) except Exception: INDEX_ITEMS.set(len(_meta)) return None except Exception as e: return f\"dense load error: {e}\" def _load_sparse(): global _vec, _X, _meta try: import time as _t import joblib from scipy import sparse vec_p = Path(os.getenv(\"VEC_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/tfidf_vectorizer.joblib\")) X_p = Path(os.getenv(\"MAT_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/tfidf_matrix.npz\")) t0 = _t.time() _vec = joblib.load(vec_p) SPARSE_VEC_LOAD_SEC.set(_t.time() - t0) t1 = _t.time() _X = sparse.load_npz(X_p) # assume rows L2-normalized; dot == cosine SPARSE_MAT_LOAD_SEC.set(_t.time() - t1) _meta = _normalize_meta_loaded(json.loads(META_PATH.read_text(encoding=\"utf-8\"))) META_ITEMS.set(len(_meta) if isinstance(_meta, list) else 0) try: INDEX_ITEMS.set(int(getattr(_X, \"shape\", (0, 0))[0])) except Exception: INDEX_ITEMS.set(len(_meta)) return None except Exception as e: return f\"sparse load error: {e}\" @app.on_event(\"startup\") def startup(): global _ready_reason _ready_reason = _load_sparse() if BACKEND == \"sparse\" else _load_dense() # ------------------ Endpoints (original behavior preserved) ------------------ @app.get(\"/health\") def health(): return {\"ok\": True} @app.get(\"/ready\") def ready(): return {\"ready\": _ready_reason is None, \"reason\": _ready_reason} @app.post(\"/reload\") def reload_index(): global _ready_reason _ready_reason = _load_sparse() if BACKEND == \"sparse\" else _load_dense() if _ready_reason is not None: raise HTTPException(status_code=503, detail=_ready_reason) return {\"reloaded\": True} # --- /metrics added (Prometheus text format) --- @app.get(\"/metrics\") def metrics(): return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST) @app.post(\"/search\") def search(req: SearchRequest): if _ready_reason is not None: raise HTTPException(status_code=503, detail=_ready_reason) import time as _t t0 = _t.time() REQS_TOTAL.inc() k = max(1, min(int(req.k), 20)) if BACKEND == \"sparse\": try: import numpy as np t_vec0 = _t.time() q = _vec.transform([req.query]) except Exception: ERRS_TOTAL.labels(stage=\"sparse_vectorize\").inc() raise finally: VEC_LAT.observe(_t.time() - t_vec0) try: t_dot0 = _t.time() scores = (_X @ q.T).toarray().ravel() # cosine since rows normalized except Exception: ERRS_TOTAL.labels(stage=\"sparse_dot\").inc() raise finally: DOT_LAT.observe(_t.time() - t_dot0) if scores.size == 0: E2E_LAT.observe(_t.time() - t0) return {\"hits\": []} # get top-k indices by score desc k_eff = min(k, scores.size) top = np.argpartition(-scores, range(k_eff))[:k_eff] top = top[np.argsort(-scores[top])] hits = [ _enrich_hit(int(i), float(scores[int(i)])) for i in top if scores[int(i)] > 0 ] E2E_LAT.observe(_t.time() - t0) return {\"hits\": hits} # dense (unchanged behavior; with timing) try: import faiss import numpy as np t_enc0 = _t.time() v = _model.encode([req.query], normalize_embeddings=True) # IP ~ cosine except Exception: ERRS_TOTAL.labels(stage=\"dense_encode\").inc() raise finally: ENC_LAT.observe(_t.time() - t_enc0) try: t_faiss0 = _t.time() D, I = _index.search(v.astype(\"float32\"), k) except Exception: ERRS_TOTAL.labels(stage=\"dense_faiss_search\").inc() raise finally: FAISS_LAT.observe(_t.time() - t_faiss0) hits = [] for score, idx in zip(D[0].tolist(), I[0].tolist()): if idx == -1: continue hits.append(_enrich_hit(int(idx), float(score))) E2E_LAT.observe(_t.time() - t0) return {\"hits\": hits} Update deployment spec to install prometheus-client>=0.20.0 . File: k8s/40-serve/deploy-retriever.yaml python -m pip install --upgrade pip if [ \"${BACKEND:-sparse}\" = \"sparse\" ]; then # Pure sparse stack (forces wheels only; will fail if a wheel is unavailable for your arch) python -m pip install --only-binary=:all: \\ \"numpy==1.26.4\" \"scipy==1.10.1\" \"scikit-learn==1.3.2\" joblib==1.4.2 \\ fastapi==0.112.2 uvicorn==0.30.6 prometheus-client>=0.20.0 Redeploy chat api as kubectl delete -f k8s/40-serve/deploy-retriever.yaml kubectl apply -f k8s/40-serve/deploy-retriever.yaml No rebuild needed\u2014these run from the mounted repo ( /mnt/project ). 2) ServiceMonitors These tell Prometheus which Services to scrape. We\u2019ll scrape: vLLM on atharva-ml/atharva-vllm:8000/metrics Chat API on atharva-app/atharva-chat-api:8080/metrics Retriever on atharva-ml/atharva-retriever:8001/metrics Create these under k8s/50-observability/ . cd project/atharva-dental-assistant mkdir k8s/50-observability/ k8s/50-observability/sm-vllm.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: sm-atharva-vllm namespace: monitoring labels: release: kps # <-- REQUIRED if your Prometheus CR uses selector.matchLabels.release spec: namespaceSelector: matchNames: - atharva-ml selector: matchLabels: app: isvc.atharva-vllm-predictor # <-- must match your Service\u2019s label endpoints: - port: http1 # <-- must match the Service port *name* path: /metrics interval: 15s scrapeTimeout: 10s # optional, good practice k8s/50-observability/sm-chat-api.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: sm-atharva-chat-api namespace: monitoring labels: release: kps # optional, add if your Prometheus selects by this spec: namespaceSelector: matchNames: - atharva-app selector: matchLabels: app: atharva-chat-api # <-- must match your Service\u2019s label endpoints: - port: http # <-- must match the Service port *name* path: /metrics interval: 15s scrapeTimeout: 10s # optional but recommended k8s/50-observability/sm-retriever.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: sm-atharva-retriever namespace: monitoring labels: release: kps # optional; include if your Prometheus uses this selector spec: namespaceSelector: matchNames: - atharva-ml selector: matchLabels: app: retriever # <-- must match your Service\u2019s label endpoints: - port: http # <-- must match the port *name* in Service path: /metrics interval: 15s scrapeTimeout: 10s # optional, safe default (Your Services from previous labs used port names http . If you changed them, reflect the right name here.) 3) (Optional) Alerts (PrometheusRule) Basic examples to show how you\u2019d alert on high error rate or slow E2E latency. k8s/50-observability/pr-alerts.yaml apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: atharva-alerts namespace: monitoring spec: groups: - name: atharva-chat rules: - alert: ChatApiHighErrorRate expr: sum(rate(chat_errors_total[5m])) by (stage) / sum(rate(chat_requests_total[5m])) by () > 0.05 for: 5m labels: { severity: warning } annotations: summary: \"Chat API error rate > 5% (stage={{ $labels.stage }})\" - alert: ChatApiLatencyHighP95 expr: histogram_quantile(0.95, sum(rate(chat_end_to_end_latency_seconds_bucket[5m])) by (le)) > 3 for: 10m labels: { severity: warning } annotations: summary: \"Chat API p95 latency > 3s\" 5) Add Labels to the Service Spec Also update the existing service spec to add relevant labels which are then used by prometheus service monitor to scrape the metrics from File : k8s/40-serve/svc-retriever.yaml apiVersion: v1 kind: Service metadata: name: atharva-retriever namespace: atharva-ml labels: app: retriever spec: type: NodePort selector: { app: retriever } ports: - name: http port: 8001 targetPort: 8001 nodePort: 30100 File : k8s/40-serve/svc-chat-api.yaml apiVersion: v1 kind: Service metadata: name: atharva-chat-api namespace: atharva-app labels: app: atharva-chat-api spec: type: NodePort selector: { app: atharva-chat-api } ports: - name: http port: 8080 targetPort: 8080 nodePort: 30300 Apply service changes kubectl apply -f k8s/40-serve/svc-retriever.yaml kubectl apply -f k8s/40-serve/svc-chat-api.yaml 5) Deploy all observability bits scripts/deploy_observability.sh #!/usr/bin/env bash set -euo pipefail # Apply ServiceMonitors + Alerts + Dashboard kubectl apply -f k8s/50-observability/sm-vllm.yaml kubectl apply -f k8s/50-observability/sm-chat-api.yaml kubectl apply -f k8s/50-observability/sm-retriever.yaml kubectl apply -f k8s/50-observability/pr-alerts.yaml || true echo \"Waiting a bit for Prometheus to pick up targets...\" sleep 10 echo \"List ServiceMonitors:\" kubectl -n monitoring get servicemonitors echo \"Prometheus targets (check Up status via UI).\" echo -e \"Access Prometheus and Grafana using\\n\\ * Prometheus : http://localhost:30500/\\n\\ * Grafana : http://localhost:30400/\\n\\ * user: admin\\n\\ * pass: prom-operator\" 6) Run the lab # Make sure Lab 4 services are running: kubectl -n atharva-ml get svc atharva-vllm atharva-retriever kubectl -n atharva-app get svc atharva-chat-api # Deploy observability bash scripts/deploy_observability.sh Open Grafana at http://127.0.0.1:3000 (Default admin/admin unless you changed it via Helm values.) From Dashboards -> New -> Import Paste the Json Dashboard copied from this link Find dashboard: \u201cAtharva LLMOps - Overview\u201d . Generate some traffic (reuse Lab 4\u2019s smoke_e2e.sh a few times) and watch: Chat RPS , errors by stage , E2E p50/p95 , Retriever p95 , Tokens gauges , (If present) vLLM tokens/sec . Lab Summary This is what we accomplished in this lab Exported custom metrics from Chat API & Retriever. Scraped vLLM metrics alongside app metrics via ServiceMonitors . Visualized a consolidated LLMOps dashboard in Grafana. (Optional) Added alerts on error rate and latency. courses/llmops/labs/v1","title":"Lab 05 - LLM Observability with Prometheus"},{"location":"lab05/#lab-5-observe-vllm-rag-chat-api","text":"","title":"Lab 5 \u2014 Observe vLLM + RAG + Chat API"},{"location":"lab05/#goal","text":"Export system + model metrics : vLLM: TTFT, latency histograms, tokens/sec, request counts (already exposed when --enable-metrics is on). Chat API: end-to-end latency, prompt/response token counts, request/5xx counts. Retriever: retrieval latency and top-k stats. Scrape with Prometheus (kube-prometheus-stack) . Visualize with Grafana . From Lab 0 we installed kube-prometheus-stack with prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \u2014 so any ServiceMonitor is scraped (no special labels needed).","title":"Goal"},{"location":"lab05/#whats-new-in-this-lab","text":"atharva-dental-assistant/ \u251c\u2500 serving/ \u2502 \u251c\u2500 chat_api.py # UPDATED: Prometheus metrics (/metrics) \u2502 \u2514\u2500 prompt_templates.py \u251c\u2500 rag/ \u2502 \u2514\u2500 retriever.py # UPDATED: Prometheus metrics (/metrics) \u251c\u2500 k8s/ \u2502 \u2514\u2500 50-observability/ \u2502 \u251c\u2500 sm-vllm.yaml # ServiceMonitor for vLLM \u2502 \u251c\u2500 sm-chat-api.yaml # ServiceMonitor for Chat API \u2502 \u251c\u2500 sm-retriever.yaml # ServiceMonitor for Retriever \u2502 \u251c\u2500 pr-alerts.yaml # (optional) basic alert rules \u2502 \u2514\u2500 cm-grafana-dashboard.yaml \u2514\u2500 scripts/ \u2514\u2500 deploy_observability.sh","title":"What\u2019s new in this lab"},{"location":"lab05/#0-setup-prometheus","text":"Add prometheus repo for helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Setup prometheus stack along with grafana with helm upgrade --install kps -n monitoring \\ prometheus-community/kube-prometheus-stack \\ --create-namespace \\ --set grafana.service.type=NodePort \\ --set grafana.service.nodePort=30400 \\ --set prometheus.service.type=NodePort \\ --set prometheus.service.nodePort=30500 \\ --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false validate kubectl rollout status -n monitoring deploy/kps-grafana --timeout=300s kubectl get pods -n monitoring Grafana : http://localhost:30400/ user: admin pass: prom-operator Prometheus : http://localhost:30500/","title":"0) Setup Prometheus"},{"location":"lab05/#1-instrument-the-services","text":"","title":"1) Instrument the services"},{"location":"lab05/#a-chat-api-add-prometheus-metrics-servingchat_apipy","text":"Replace your file with this version (differences: imports, metrics, /metrics endpoint, timings): import os import time import httpx from fastapi import FastAPI, Query, Response from pydantic import BaseModel from typing import List, Dict, Any from prometheus_client import ( Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST, ) from prompt_templates import build_messages RETRIEVER_URL = os.getenv(\"RETRIEVER_URL\", \"http://atharva-retriever.atharva-ml.svc.cluster.local:8001\") VLLM_URL = os.getenv(\"VLLM_URL\", \"http://atharva-vllm.atharva-ml.svc.cluster.local:8000\") MODEL_NAME = os.getenv(\"MODEL_NAME\", \"smollm2-135m-atharva\") MAX_CTX_SNIPPETS = int(os.getenv(\"MAX_CTX_SNIPPETS\", \"3\")) MAX_CTX_CHARS = int(os.getenv(\"MAX_CTX_CHARS\", \"2400\")) app = FastAPI() # ----------------------------- # Prometheus metrics # ----------------------------- REQS = Counter(\"chat_requests_total\", \"Total Chat API requests\", [\"route\"]) ERRS = Counter(\"chat_errors_total\", \"Total Chat API errors\", [\"stage\"]) E2E_LAT = Histogram( \"chat_end_to_end_latency_seconds\", \"End-to-end /chat latency in seconds\", buckets=(0.05, 0.1, 0.25, 0.5, 1, 2, 3, 5, 8, 13), ) RAG_LAT = Histogram( \"rag_retrieval_latency_seconds\", \"Retriever call latency in seconds\", buckets=(0.01, 0.02, 0.05, 0.1, 0.2, 0.4, 0.8, 1.5, 3), ) VLLM_LAT = Histogram( \"vllm_request_latency_seconds\", \"vLLM chat/completions call latency in seconds\", buckets=(0.05, 0.1, 0.25, 0.5, 1, 2, 3, 5, 8, 13), ) TOK_PROMPT = Gauge(\"chat_prompt_tokens\", \"Prompt tokens for the last completed /chat request\") TOK_COMPLETION = Gauge(\"chat_completion_tokens\", \"Completion tokens for the last completed /chat request\") TOK_TOTAL = Gauge(\"chat_total_tokens\", \"Total tokens for the last completed /chat request\") class ChatRequest(BaseModel): question: str k: int = 4 max_tokens: int = 200 temperature: float = 0.1 debug: bool = False # when true, include prompt/messages in response def _label(meta: Dict[str, Any]) -> str: did = (meta or {}).get(\"doc_id\") sec = (meta or {}).get(\"section\") if not did: return \"unknown\" return f\"{did}#{sec}\" if sec and sec != \"full\" else did def _collect_citations(hits: List[Dict[str, Any]]) -> List[str]: seen, out = set(), [] for h in hits: lab = _label(h.get(\"meta\")) if lab not in seen: seen.add(lab) out.append(lab) return out def _normalize_hits(hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]: # Drop recent_queries from grounding context (they\u2019re noisy) filt = [] for h in hits: did = ((h.get(\"meta\") or {}).get(\"doc_id\") or \"\").lower() if did.startswith(\"recent_queries.jsonl\"): continue filt.append(h) # Prefer those with text first filt.sort(key=lambda h: (h.get(\"text\") is None), reverse=False) # Dedup by label seen, dedup = set(), [] for h in filt: lab = _label(h.get(\"meta\")) if lab in seen: continue seen.add(lab) dedup.append(h) # Trim by count and char budget total = 0 trimmed = [] for h in dedup: txt = h.get(\"text\") or (h.get(\"meta\") or {}).get(\"text\") or \"\" if len(trimmed) < MAX_CTX_SNIPPETS and total + len(txt) <= MAX_CTX_CHARS: trimmed.append(h) total += len(txt) if len(trimmed) >= MAX_CTX_SNIPPETS: break return trimmed def _strip_existing_source(txt: str) -> str: lines = txt.rstrip().splitlines() kept = [ln for ln in lines if not ln.strip().lower().startswith(\"source:\")] return \"\\n\".join(kept).rstrip() @app.get(\"/health\") def health(): return {\"ok\": True, \"retriever\": RETRIEVER_URL, \"vllm\": VLLM_URL, \"model\": MODEL_NAME} @app.get(\"/metrics\") def metrics(): # Expose Prometheus metrics return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST) @app.get(\"/dryrun\") def dryrun(q: str = Query(..., alias=\"question\"), k: int = 4): \"\"\"Build exactly what /chat would send to vLLM, but don\u2019t call vLLM.\"\"\" REQS.labels(route=\"/dryrun\").inc() with httpx.Client(timeout=30) as cx: t_r0 = time.time() try: r = cx.post(f\"{RETRIEVER_URL}/search\", json={\"query\": q, \"k\": k}) r.raise_for_status() raw_hits = r.json().get(\"hits\", []) except Exception: ERRS.labels(stage=\"retriever\").inc() raise finally: RAG_LAT.observe(time.time() - t_r0) ctx_hits = _normalize_hits(raw_hits) citations = _collect_citations(ctx_hits) messages = build_messages(q, ctx_hits) # Also surface the precise snippets we used (label + text) used_snippets = [] for h in ctx_hits: meta = h.get(\"meta\") or {} used_snippets.append({ \"label\": _label(meta), \"text\": h.get(\"text\") or meta.get(\"text\") or \"\" }) return { \"question\": q, \"citations\": citations, \"used_snippets\": used_snippets, # what the model will actually see \"messages\": messages, # the exact OpenAI Chat payload \"note\": \"This is a dry run; no LLM call was made.\" } @app.post(\"/chat\") def chat(req: ChatRequest): REQS.labels(route=\"/chat\").inc() t0 = time.time() # 1) retrieve with httpx.Client(timeout=30) as cx: t_r0 = time.time() try: r = cx.post(f\"{RETRIEVER_URL}/search\", json={\"query\": req.question, \"k\": req.k}) r.raise_for_status() raw_hits = r.json().get(\"hits\", []) except Exception: ERRS.labels(stage=\"retriever\").inc() raise finally: RAG_LAT.observe(time.time() - t_r0) # 2) normalize + citations ctx_hits = _normalize_hits(raw_hits) citations = _collect_citations(ctx_hits) # 3) build messages with actual snippet text messages = build_messages(req.question, ctx_hits) # 4) call vLLM (OpenAI-compatible) temperature = max(0.0, min(req.temperature, 0.5)) max_tokens = min(req.max_tokens, 256) payload = { \"model\": MODEL_NAME, \"messages\": messages, \"temperature\": temperature, \"top_p\": 0.9, \"max_tokens\": max_tokens, \"stream\": False, } with httpx.Client(timeout=120) as cx: t_llm0 = time.time() try: rr = cx.post(f\"{VLLM_URL}/v1/chat/completions\", json=payload) rr.raise_for_status() data = rr.json() except Exception: ERRS.labels(stage=\"vllm\").inc() raise finally: VLLM_LAT.observe(time.time() - t_llm0) content = data[\"choices\"][0][\"message\"][\"content\"] usage = data.get(\"usage\", {}) dt = time.time() - t0 content = _strip_existing_source(content) content = content + (\"\\nSource: \" + \"; \".join(citations) if citations else \"\\nSource: (none)\") # Update token gauges (best-effort) try: TOK_PROMPT.set(float(usage.get(\"prompt_tokens\", 0) or 0)) TOK_COMPLETION.set(float(usage.get(\"completion_tokens\", 0) or 0)) TOK_TOTAL.set(float(usage.get(\"total_tokens\", 0) or 0)) except Exception: # Avoid failing the request if usage is missing or malformed pass # Observe end-to-end latency E2E_LAT.observe(dt) resp = { \"answer\": content, \"citations\": citations, \"latency_seconds\": round(dt, 3), \"usage\": usage, } # 5) optional debug payload so you can inspect exactly what was sent if req.debug: used_snippets = [] for h in ctx_hits: meta = h.get(\"meta\") or {} used_snippets.append({ \"label\": _label(meta), \"text\": h.get(\"text\") or meta.get(\"text\") or \"\" }) resp[\"debug\"] = { \"messages\": messages, # exact system+user messages sent \"used_snippets\": used_snippets, \"raw_hits\": raw_hits[:10], # original retriever output (trimmed) \"payload_model\": MODEL_NAME, \"payload_temperature\": temperature, \"payload_max_tokens\": max_tokens, } return resp Update deployment spec to install prometheus-client>=0.20.0 . File: k8s/40-serve/deploy-chat-api.yaml command: [\"bash\",\"-lc\"] args: - | pip install --no-cache-dir fastapi==0.112.2 uvicorn==0.30.6 httpx==0.27.2 prometheus-client>=0.20.0 uvicorn chat_api:app --host 0.0.0.0 --port 8080 --proxy-headers Redeploy chat api as kubectl delete -f k8s/40-serve/deploy-chat-api.yaml kubectl apply -f k8s/40-serve/deploy-chat-api.yaml","title":"A) Chat API \u2014 add Prometheus metrics (serving/chat_api.py)"},{"location":"lab05/#b-retriever-add-prometheus-metrics-ragretrieverpy","text":"Replace your retriever with this version (adds /metrics , latency histogram, request counter): import os import json from pathlib import Path from typing import List, Optional, Tuple, Any from fastapi import FastAPI, HTTPException, Response from pydantic import BaseModel # --- Prometheus (only new dependency) --- from prometheus_client import ( Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST, ) BACKEND = os.getenv(\"BACKEND\", \"dense\") # \"sparse\" or \"dense\" INDEX_PATH = Path(os.getenv(\"INDEX_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/index.faiss\")) META_PATH = Path(os.getenv(\"META_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/meta.json\")) MODEL_DIR = os.getenv(\"MODEL_DIR\") # optional for dense MODEL_NAME = os.getenv(\"MODEL_NAME\", \"sentence-transformers/all-MiniLM-L6-v2\") app = FastAPI(title=f\"Atharva Retriever ({BACKEND})\") class SearchRequest(BaseModel): query: str k: int = 4 _ready_reason = \"starting\" _model = None; _index = None; _meta: List[dict] = [] _vec = None; _X = None # sparse objects # ------------------ Prometheus metrics (added) ------------------ REQS_TOTAL = Counter(\"retriever_requests_total\", \"Total /search requests\") ERRS_TOTAL = Counter(\"retriever_errors_total\", \"Total retriever errors\", [\"stage\"]) E2E_LAT = Histogram( \"retriever_search_latency_seconds\", \"End-to-end /search latency (s)\", buckets=(0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2), ) # Dense sub-steps ENC_LAT = Histogram( \"retriever_dense_encode_latency_seconds\", \"SentenceTransformer.encode latency (s)\", buckets=(0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2), ) FAISS_LAT = Histogram( \"retriever_dense_faiss_latency_seconds\", \"FAISS search latency (s)\", buckets=(0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1), ) # Sparse sub-steps VEC_LAT = Histogram( \"retriever_sparse_vectorize_latency_seconds\", \"TF-IDF vectorizer.transform latency (s)\", buckets=(0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1), ) DOT_LAT = Histogram( \"retriever_sparse_dot_latency_seconds\", \"Sparse dot/matmul latency (s)\", buckets=(0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1), ) # Load-time & sizes MODEL_LOAD_SEC = Gauge(\"retriever_model_load_seconds\", \"Dense model load time (s)\") INDEX_LOAD_SEC = Gauge(\"retriever_index_load_seconds\", \"FAISS index load time (s)\") SPARSE_VEC_LOAD_SEC = Gauge(\"retriever_sparse_vectorizer_load_seconds\", \"TF-IDF vectorizer load time (s)\") SPARSE_MAT_LOAD_SEC = Gauge(\"retriever_sparse_matrix_load_seconds\", \"TF-IDF matrix load time (s)\") INDEX_ITEMS = Gauge(\"retriever_index_items\", \"Items in index/matrix\") META_ITEMS = Gauge(\"retriever_meta_items\", \"Number of meta records\") # ------------------ Utils ------------------ def _normalize_meta_loaded(data: Any) -> List[dict]: \"\"\" Accepts various shapes of meta.json and returns a list of entries. Supported: - list[dict] - {\"items\": [...]} (common pattern) - {\"hits\": [...]} (fallback) \"\"\" if isinstance(data, list): return data if isinstance(data, dict): # keep original behavior if \"items\" in data and isinstance(data[\"items\"], list): return data[\"items\"] if \"hits\" in data and isinstance(data[\"hits\"], list): return data[\"hits\"] raise ValueError(\"META_PATH must contain a list or a dict with 'items'/'hits'.\") def _parse_doc_and_section(path: Optional[str]) -> Tuple[str, Optional[str]]: \"\"\" Parse labels from meta.path: - 'treatments.json#0' -> ('treatments.json', '0') - 'faq.md' -> ('faq.md', None) - 'policies/emergency.md' -> ('policies/emergency.md', None) \"\"\" if not path: return \"unknown\", None if \"#\" in path: d, s = path.split(\"#\", 1) return d, s return path, None def _extract_text(m: dict) -> Optional[str]: \"\"\"Try common keys for stored chunk text.\"\"\" return m.get(\"text\") or m.get(\"chunk\") or m.get(\"content\") def _enrich_hit(idx: int, score: float) -> dict: \"\"\" Build a single enriched hit from meta[idx]. \"\"\" if idx < 0 or idx >= len(_meta): doc_id, section, path, typ, txt = \"unknown\", None, None, None, None else: m = _meta[idx] or {} path = m.get(\"path\") typ = m.get(\"type\") doc_id, section = _parse_doc_and_section(path) txt = _extract_text(m) hit = { \"score\": float(score), \"meta\": { \"doc_id\": doc_id, \"section\": section, \"path\": path, \"type\": typ, }, } if txt: hit[\"text\"] = txt return hit # ------------------ Loaders (unchanged behavior; just timed gauges) ------------------ def _load_dense(): global _model, _index, _meta try: import time as _t import faiss from sentence_transformers import SentenceTransformer t0 = _t.time() _model = SentenceTransformer(MODEL_DIR) if (MODEL_DIR and Path(MODEL_DIR).exists()) else SentenceTransformer(MODEL_NAME) MODEL_LOAD_SEC.set(_t.time() - t0) t1 = _t.time() _index = faiss.read_index(str(INDEX_PATH)) INDEX_LOAD_SEC.set(_t.time() - t1) _meta = _normalize_meta_loaded(json.loads(META_PATH.read_text(encoding=\"utf-8\"))) META_ITEMS.set(len(_meta) if isinstance(_meta, list) else 0) # best-effort size (for FAISS) try: INDEX_ITEMS.set(int(getattr(_index, \"ntotal\", len(_meta)))) except Exception: INDEX_ITEMS.set(len(_meta)) return None except Exception as e: return f\"dense load error: {e}\" def _load_sparse(): global _vec, _X, _meta try: import time as _t import joblib from scipy import sparse vec_p = Path(os.getenv(\"VEC_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/tfidf_vectorizer.joblib\")) X_p = Path(os.getenv(\"MAT_PATH\", \"/mnt/project/atharva-dental-assistant/artifacts/rag/tfidf_matrix.npz\")) t0 = _t.time() _vec = joblib.load(vec_p) SPARSE_VEC_LOAD_SEC.set(_t.time() - t0) t1 = _t.time() _X = sparse.load_npz(X_p) # assume rows L2-normalized; dot == cosine SPARSE_MAT_LOAD_SEC.set(_t.time() - t1) _meta = _normalize_meta_loaded(json.loads(META_PATH.read_text(encoding=\"utf-8\"))) META_ITEMS.set(len(_meta) if isinstance(_meta, list) else 0) try: INDEX_ITEMS.set(int(getattr(_X, \"shape\", (0, 0))[0])) except Exception: INDEX_ITEMS.set(len(_meta)) return None except Exception as e: return f\"sparse load error: {e}\" @app.on_event(\"startup\") def startup(): global _ready_reason _ready_reason = _load_sparse() if BACKEND == \"sparse\" else _load_dense() # ------------------ Endpoints (original behavior preserved) ------------------ @app.get(\"/health\") def health(): return {\"ok\": True} @app.get(\"/ready\") def ready(): return {\"ready\": _ready_reason is None, \"reason\": _ready_reason} @app.post(\"/reload\") def reload_index(): global _ready_reason _ready_reason = _load_sparse() if BACKEND == \"sparse\" else _load_dense() if _ready_reason is not None: raise HTTPException(status_code=503, detail=_ready_reason) return {\"reloaded\": True} # --- /metrics added (Prometheus text format) --- @app.get(\"/metrics\") def metrics(): return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST) @app.post(\"/search\") def search(req: SearchRequest): if _ready_reason is not None: raise HTTPException(status_code=503, detail=_ready_reason) import time as _t t0 = _t.time() REQS_TOTAL.inc() k = max(1, min(int(req.k), 20)) if BACKEND == \"sparse\": try: import numpy as np t_vec0 = _t.time() q = _vec.transform([req.query]) except Exception: ERRS_TOTAL.labels(stage=\"sparse_vectorize\").inc() raise finally: VEC_LAT.observe(_t.time() - t_vec0) try: t_dot0 = _t.time() scores = (_X @ q.T).toarray().ravel() # cosine since rows normalized except Exception: ERRS_TOTAL.labels(stage=\"sparse_dot\").inc() raise finally: DOT_LAT.observe(_t.time() - t_dot0) if scores.size == 0: E2E_LAT.observe(_t.time() - t0) return {\"hits\": []} # get top-k indices by score desc k_eff = min(k, scores.size) top = np.argpartition(-scores, range(k_eff))[:k_eff] top = top[np.argsort(-scores[top])] hits = [ _enrich_hit(int(i), float(scores[int(i)])) for i in top if scores[int(i)] > 0 ] E2E_LAT.observe(_t.time() - t0) return {\"hits\": hits} # dense (unchanged behavior; with timing) try: import faiss import numpy as np t_enc0 = _t.time() v = _model.encode([req.query], normalize_embeddings=True) # IP ~ cosine except Exception: ERRS_TOTAL.labels(stage=\"dense_encode\").inc() raise finally: ENC_LAT.observe(_t.time() - t_enc0) try: t_faiss0 = _t.time() D, I = _index.search(v.astype(\"float32\"), k) except Exception: ERRS_TOTAL.labels(stage=\"dense_faiss_search\").inc() raise finally: FAISS_LAT.observe(_t.time() - t_faiss0) hits = [] for score, idx in zip(D[0].tolist(), I[0].tolist()): if idx == -1: continue hits.append(_enrich_hit(int(idx), float(score))) E2E_LAT.observe(_t.time() - t0) return {\"hits\": hits} Update deployment spec to install prometheus-client>=0.20.0 . File: k8s/40-serve/deploy-retriever.yaml python -m pip install --upgrade pip if [ \"${BACKEND:-sparse}\" = \"sparse\" ]; then # Pure sparse stack (forces wheels only; will fail if a wheel is unavailable for your arch) python -m pip install --only-binary=:all: \\ \"numpy==1.26.4\" \"scipy==1.10.1\" \"scikit-learn==1.3.2\" joblib==1.4.2 \\ fastapi==0.112.2 uvicorn==0.30.6 prometheus-client>=0.20.0 Redeploy chat api as kubectl delete -f k8s/40-serve/deploy-retriever.yaml kubectl apply -f k8s/40-serve/deploy-retriever.yaml No rebuild needed\u2014these run from the mounted repo ( /mnt/project ).","title":"B) Retriever \u2014 add Prometheus metrics (rag/retriever.py)"},{"location":"lab05/#2-servicemonitors","text":"These tell Prometheus which Services to scrape. We\u2019ll scrape: vLLM on atharva-ml/atharva-vllm:8000/metrics Chat API on atharva-app/atharva-chat-api:8080/metrics Retriever on atharva-ml/atharva-retriever:8001/metrics Create these under k8s/50-observability/ . cd project/atharva-dental-assistant mkdir k8s/50-observability/","title":"2) ServiceMonitors"},{"location":"lab05/#k8s50-observabilitysm-vllmyaml","text":"apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: sm-atharva-vllm namespace: monitoring labels: release: kps # <-- REQUIRED if your Prometheus CR uses selector.matchLabels.release spec: namespaceSelector: matchNames: - atharva-ml selector: matchLabels: app: isvc.atharva-vllm-predictor # <-- must match your Service\u2019s label endpoints: - port: http1 # <-- must match the Service port *name* path: /metrics interval: 15s scrapeTimeout: 10s # optional, good practice","title":"k8s/50-observability/sm-vllm.yaml"},{"location":"lab05/#k8s50-observabilitysm-chat-apiyaml","text":"apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: sm-atharva-chat-api namespace: monitoring labels: release: kps # optional, add if your Prometheus selects by this spec: namespaceSelector: matchNames: - atharva-app selector: matchLabels: app: atharva-chat-api # <-- must match your Service\u2019s label endpoints: - port: http # <-- must match the Service port *name* path: /metrics interval: 15s scrapeTimeout: 10s # optional but recommended","title":"k8s/50-observability/sm-chat-api.yaml"},{"location":"lab05/#k8s50-observabilitysm-retrieveryaml","text":"apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: sm-atharva-retriever namespace: monitoring labels: release: kps # optional; include if your Prometheus uses this selector spec: namespaceSelector: matchNames: - atharva-ml selector: matchLabels: app: retriever # <-- must match your Service\u2019s label endpoints: - port: http # <-- must match the port *name* in Service path: /metrics interval: 15s scrapeTimeout: 10s # optional, safe default (Your Services from previous labs used port names http . If you changed them, reflect the right name here.)","title":"k8s/50-observability/sm-retriever.yaml"},{"location":"lab05/#3-optional-alerts-prometheusrule","text":"Basic examples to show how you\u2019d alert on high error rate or slow E2E latency.","title":"3) (Optional) Alerts (PrometheusRule)"},{"location":"lab05/#k8s50-observabilitypr-alertsyaml","text":"apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: atharva-alerts namespace: monitoring spec: groups: - name: atharva-chat rules: - alert: ChatApiHighErrorRate expr: sum(rate(chat_errors_total[5m])) by (stage) / sum(rate(chat_requests_total[5m])) by () > 0.05 for: 5m labels: { severity: warning } annotations: summary: \"Chat API error rate > 5% (stage={{ $labels.stage }})\" - alert: ChatApiLatencyHighP95 expr: histogram_quantile(0.95, sum(rate(chat_end_to_end_latency_seconds_bucket[5m])) by (le)) > 3 for: 10m labels: { severity: warning } annotations: summary: \"Chat API p95 latency > 3s\"","title":"k8s/50-observability/pr-alerts.yaml"},{"location":"lab05/#5-add-labels-to-the-service-spec","text":"Also update the existing service spec to add relevant labels which are then used by prometheus service monitor to scrape the metrics from File : k8s/40-serve/svc-retriever.yaml apiVersion: v1 kind: Service metadata: name: atharva-retriever namespace: atharva-ml labels: app: retriever spec: type: NodePort selector: { app: retriever } ports: - name: http port: 8001 targetPort: 8001 nodePort: 30100 File : k8s/40-serve/svc-chat-api.yaml apiVersion: v1 kind: Service metadata: name: atharva-chat-api namespace: atharva-app labels: app: atharva-chat-api spec: type: NodePort selector: { app: atharva-chat-api } ports: - name: http port: 8080 targetPort: 8080 nodePort: 30300 Apply service changes kubectl apply -f k8s/40-serve/svc-retriever.yaml kubectl apply -f k8s/40-serve/svc-chat-api.yaml","title":"5) Add Labels to the Service Spec"},{"location":"lab05/#5-deploy-all-observability-bits","text":"","title":"5) Deploy all observability bits"},{"location":"lab05/#scriptsdeploy_observabilitysh","text":"#!/usr/bin/env bash set -euo pipefail # Apply ServiceMonitors + Alerts + Dashboard kubectl apply -f k8s/50-observability/sm-vllm.yaml kubectl apply -f k8s/50-observability/sm-chat-api.yaml kubectl apply -f k8s/50-observability/sm-retriever.yaml kubectl apply -f k8s/50-observability/pr-alerts.yaml || true echo \"Waiting a bit for Prometheus to pick up targets...\" sleep 10 echo \"List ServiceMonitors:\" kubectl -n monitoring get servicemonitors echo \"Prometheus targets (check Up status via UI).\" echo -e \"Access Prometheus and Grafana using\\n\\ * Prometheus : http://localhost:30500/\\n\\ * Grafana : http://localhost:30400/\\n\\ * user: admin\\n\\ * pass: prom-operator\"","title":"scripts/deploy_observability.sh"},{"location":"lab05/#6-run-the-lab","text":"# Make sure Lab 4 services are running: kubectl -n atharva-ml get svc atharva-vllm atharva-retriever kubectl -n atharva-app get svc atharva-chat-api # Deploy observability bash scripts/deploy_observability.sh Open Grafana at http://127.0.0.1:3000 (Default admin/admin unless you changed it via Helm values.) From Dashboards -> New -> Import Paste the Json Dashboard copied from this link Find dashboard: \u201cAtharva LLMOps - Overview\u201d . Generate some traffic (reuse Lab 4\u2019s smoke_e2e.sh a few times) and watch: Chat RPS , errors by stage , E2E p50/p95 , Retriever p95 , Tokens gauges , (If present) vLLM tokens/sec .","title":"6) Run the lab"},{"location":"lab05/#lab-summary","text":"This is what we accomplished in this lab Exported custom metrics from Chat API & Retriever. Scraped vLLM metrics alongside app metrics via ServiceMonitors . Visualized a consolidated LLMOps dashboard in Grafana. (Optional) Added alerts on error rate and latency.","title":"Lab Summary"},{"location":"lab05/#coursesllmopslabsv1","text":"","title":"courses/llmops/labs/v1"},{"location":"lab06/","text":"Lab 6 \u2014 Autoscaling vLLM + RAG + Chat API Goal HPA : scale Chat API on CPU (simple), and (optionally) on Prometheus RPS via Prometheus Adapter. VPA : get resource recommendations for vLLM & Retriever, then (optionally) apply. KEDA : event-driven Prometheus trigger (e.g., RPS per pod). Works with Labs 0\u20135 (Prometheus/Grafana already installed). What you\u2019ll add atharva-dental-assistant/ \u251c\u2500 k8s/ \u2502 \u2514\u2500 60-autoscale/ \u2502 \u251c\u2500 metrics-server-values.yaml \u2502 \u251c\u2500 hpa-chat-api-cpu.yaml \u2502 \u251c\u2500 adapter-values.yaml # Prometheus Adapter (external/custom metrics) \u2502 \u251c\u2500 hpa-chat-api-rps.yaml # HPA using external metrics (RPS) \u2502 \u251c\u2500 vpa-vllm.yaml \u2502 \u251c\u2500 vpa-retriever.yaml \u2502 \u251c\u2500 keda-values.yaml \u2502 \u251c\u2500 keda-scaledobject-chat.yaml # Prometheus trigger -> scale Chat API \u2502 \u2514\u2500 loadgen-job.yaml # simple load generator \u2514\u2500 scripts/ \u2514\u2500 deploy_autoscaling.sh Begin the la by creating the directory cd project/atharva-dental-assistant mkdir k8s/60-autoscale/ 0) Pre-reqs (install once) A) metrics-server (HPA on CPU needs this) Kind often lacks it by default. you can check it by running the following kubectl top pod kubectl top node If you do not see the metrics but error: Metrics API not available , you need to install this component. # switch to home dir. alternately you pick a path cd ~ git clone https://github.com/schoolofdevops/metrics-server.git kubectl apply -k metrics-server/manifests/overlays/release validate: kubectl top pod kubectl top node [sample output] \u279c kubernetes kubectl top node NAME CPU(cores) CPU(%) MEMORY(bytes) MEMORY(%) llmops-kind-control-plane 285m 4% 1344Mi 13% llmops-kind-worker 82m 1% 2721Mi 27% llmops-kind-worker2 110m 1% 1377Mi 13% \u279c kubernetes kubectl top pods NAME CPU(cores) MEMORY(bytes) atharva-retriever-75b5996b54-w5vgc 4m 98Mi atharva-vllm-predictor-64845bf646-dcdmf 14m 2117Mi B) Add Resource Spec to Chat API Step 2: Add Resource Spec to the Pod File: k8s/40-serve/deploy-chat-api.yaml spec: replicas: 1 selector: matchLabels: { app: atharva-chat-api } template: metadata: labels: { app: atharva-chat-api } spec: containers: - name: api image: python:3.11-slim resources: requests: cpu: \"50m\" memory: \"64Mi\" limits: cpu: \"250m\" memory: \"256Mi\" kubectl apply -f k8s/40-serve/deploy-chat-api.yaml validate kubectl describe pod -n atharva-app -l \"app=atharva-chat-api\" Look for the container section where you should see the resource spec added. 1) HPA on CPU (Chat API) k8s/60-autoscale/hpa-chat-api.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: hpa-chat-api namespace: atharva-app spec: minReplicas: 2 maxReplicas: 6 metrics: - type: ContainerResource containerResource: name: cpu container: api # change this as per actual container name target: type: Utilization averageUtilization: 50 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: atharva-chat-api behavior: scaleDown: policies: - type: Pods value: 2 periodSeconds: 120 - type: Percent value: 25 periodSeconds: 120 stabilizationWindowSeconds: 60 scaleUp: stabilizationWindowSeconds: 45 policies: - type: Percent value: 100 periodSeconds: 15 - type: Pods value: 2 periodSeconds: 15 selectPolicy: Max Apply: kubectl apply -f k8s/60-autoscale/hpa-chat-api.yaml kubectl -n atharva-app get hpa hpa-chat-api -w [expected output] NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-chat-api Deployment/atharva-chat-api cpu: <unknown>/50% 2 6 2 2m54s hpa-chat-api Deployment/atharva-chat-api cpu: 10%/50% 2 6 2 3m Wait till you see the metrics appear and the minimum pods set to 2 as per HPA spec to proceed with load test next. 3) VPA (recommendations for vLLM & Retriever) Install the VPA components (once): git clone https://github.com/kubernetes/autoscaler.git cd autoscaler/vertical-pod-autoscaler/ # deploy vpa ./hack/vpa-up.sh Recommend-only mode first: k8s/60-autoscale/vpa-vllm.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: vpa-vllm namespace: atharva-ml spec: targetRef: apiVersion: apps/v1 kind: Deployment name: atharva-vllm-predictor updatePolicy: updateMode: \"Off\" # \"Auto\" to apply automatically k8s/60-autoscale/vpa-retriever.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: vpa-retriever namespace: atharva-ml spec: targetRef: apiVersion: apps/v1 kind: Deployment name: atharva-retriever updatePolicy: updateMode: \"Off\" Apply: kubectl apply -f k8s/60-autoscale/vpa-vllm.yaml kubectl apply -f k8s/60-autoscale/vpa-retriever.yaml # After some traffic: kubectl -n atharva-ml describe vpa vpa-vllm | sed -n '1,200p' kubectl -n atharva-ml describe vpa vpa-retriever | sed -n '1,200p' 5) Load generator (to trigger scaling) A tiny Job that hammers the Chat API. Adjust CONCURRENCY & REQUESTS . k8s/60-autoscale/loadgen-job.yaml apiVersion: batch/v1 kind: Job metadata: generateName: loadgen-job- namespace: atharva-app spec: backoffLimit: 0 template: spec: restartPolicy: Never containers: - name: hey image: schoolofdevops/hey:latest command: [\"hey\"] args: - \"-z\" - \"4m\" # duration - \"-q\" - \"50\" # target QPS - \"-c\" - \"25\" # concurrency - \"-m\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"-d\" - '{\"question\":\"Do you accept UPI Payments?\",\"k\":4,\"max_tokens\":160}' - \"http://atharva-chat-api.atharva-app.svc.cluster.local:8080/chat\" Run it: kubectl create -f k8s/60-autoscale/loadgen-job.yaml kubectl -n atharva-app get jobs kubectl -n atharva-app logs -f job/loadgen-job-xxxx Watch scaling [in a new terminal]: # Watch Horizontal Scling with HPA kubectl -n atharva-app get hpa hpa-chat-api -w # Check Resource Scaling Recommendations with VPA kubectl get vpa -w # If using KEDA: kubectl -n atharva-app get deploy/atharva-chat-api -w Once the load test is complete you could delete HPA with kubectl delete -f k8s/60-autoscale/hpa-chat-api.yaml C) KEDA (for event-driven scale on Prometheus) Install KEDA as helm repo add kedacore https://kedacore.github.io/charts helm repo update helm install keda kedacore/keda \\ --namespace keda \\ --create-namespace validate kubectl get all -n keda 4) KEDA ScaledObject (Prometheus trigger) Scale Chat API when RPS > 1 per replica (same signal as HPA external metric). Point KEDA to Prometheus from kube-prometheus-stack. k8s/60-autoscale/keda-scaledobject-chat.yaml apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: chat-api-qps namespace: atharva-app spec: scaleTargetRef: name: atharva-chat-api minReplicaCount: 1 maxReplicaCount: 8 cooldownPeriod: 60 triggers: - type: prometheus metadata: serverAddress: http://kps-kube-prometheus-stack-prometheus.monitoring.svc:9090 metricName: vllm_queue_length threshold: \"5\" query: sum(vllm:num_requests_waiting) Apply: kubectl apply -f k8s/60-autoscale/keda-scaledobject-chat.yaml kubectl -n atharva-app get scaledobject launch the load test and watch ScaledObject + HPA kubectl create -f k8s/60-autoscale/loadgen-job.yaml watch kubectl -n atharva-app get scaledobject,hpa You don\u2019t need Prometheus Adapter for KEDA if you use the prometheus trigger (it queries Prom directly). We keep Adapter anyway for the HPA external-metric demo. 5) Load generator (to trigger scaling) A tiny Job that hammers the Chat API. Adjust CONCURRENCY & REQUESTS . k8s/60-autoscale/loadgen-job.yaml apiVersion: batch/v1 kind: Job metadata: generateName: loadgen-job- namespace: atharva-app spec: backoffLimit: 0 template: spec: restartPolicy: Never containers: - name: hey image: schoolofdevops/hey:latest command: [\"hey\"] args: - \"-z\" - \"3m\" # duration - \"-q\" - \"5\" # target QPS - \"-c\" - \"5\" # concurrency - \"-m\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"-d\" - '{\"question\":\"Do you accept UPI Payments?\",\"k\":4,\"max_tokens\":160}' - \"http://atharva-chat-api.atharva-app.svc.cluster.local:8080/chat\" Run it: kubectl apply -f k8s/60-autoscale/loadgen-job.yaml kubectl -n atharva-app logs -f job/chat-loadgen Watch scaling: # If using HPA CPU: kubectl -n atharva-app get hpa hpa-chat-api -w # If using KEDA: kubectl -n atharva-app get deploy/atharva-chat-api -w Tips / Gotchas Pick one scaler per target at a time to avoid fights. For demos: Start with CPU HPA , Switch to RPS HPA (delete CPU HPA), Try KEDA . HPA external metrics require Prometheus Adapter (we installed & mapped chat_requests_per_second ). KEDA Prometheus trigger queries Prom directly; Adapter not required for that path. VPA in \u201cOff\u201d mode shows recommendations under describe vpa . Flip to \"Auto\" if you want automatic updates (be mindful of restarts). Lab Summary This is what we accomplished in this lab Horizontal scaling of Chat API on CPU or RPS (Prometheus-backed). Vertical recommendations for vLLM and Retriever to right-size resources. Event-driven scaling with KEDA using a Prometheus trigger. A simple load generator to visualize scaling behavior. courses/llmops/labs/v1","title":"Lab 06 - Autoscaling vLLM + RAG + Chat API"},{"location":"lab06/#lab-6-autoscaling-vllm-rag-chat-api","text":"","title":"Lab 6 \u2014 Autoscaling vLLM + RAG + Chat API"},{"location":"lab06/#goal","text":"HPA : scale Chat API on CPU (simple), and (optionally) on Prometheus RPS via Prometheus Adapter. VPA : get resource recommendations for vLLM & Retriever, then (optionally) apply. KEDA : event-driven Prometheus trigger (e.g., RPS per pod). Works with Labs 0\u20135 (Prometheus/Grafana already installed).","title":"Goal"},{"location":"lab06/#what-youll-add","text":"atharva-dental-assistant/ \u251c\u2500 k8s/ \u2502 \u2514\u2500 60-autoscale/ \u2502 \u251c\u2500 metrics-server-values.yaml \u2502 \u251c\u2500 hpa-chat-api-cpu.yaml \u2502 \u251c\u2500 adapter-values.yaml # Prometheus Adapter (external/custom metrics) \u2502 \u251c\u2500 hpa-chat-api-rps.yaml # HPA using external metrics (RPS) \u2502 \u251c\u2500 vpa-vllm.yaml \u2502 \u251c\u2500 vpa-retriever.yaml \u2502 \u251c\u2500 keda-values.yaml \u2502 \u251c\u2500 keda-scaledobject-chat.yaml # Prometheus trigger -> scale Chat API \u2502 \u2514\u2500 loadgen-job.yaml # simple load generator \u2514\u2500 scripts/ \u2514\u2500 deploy_autoscaling.sh Begin the la by creating the directory cd project/atharva-dental-assistant mkdir k8s/60-autoscale/","title":"What you\u2019ll add"},{"location":"lab06/#0-pre-reqs-install-once","text":"","title":"0) Pre-reqs (install once)"},{"location":"lab06/#a-metrics-server-hpa-on-cpu-needs-this","text":"Kind often lacks it by default. you can check it by running the following kubectl top pod kubectl top node If you do not see the metrics but error: Metrics API not available , you need to install this component. # switch to home dir. alternately you pick a path cd ~ git clone https://github.com/schoolofdevops/metrics-server.git kubectl apply -k metrics-server/manifests/overlays/release validate: kubectl top pod kubectl top node [sample output] \u279c kubernetes kubectl top node NAME CPU(cores) CPU(%) MEMORY(bytes) MEMORY(%) llmops-kind-control-plane 285m 4% 1344Mi 13% llmops-kind-worker 82m 1% 2721Mi 27% llmops-kind-worker2 110m 1% 1377Mi 13% \u279c kubernetes kubectl top pods NAME CPU(cores) MEMORY(bytes) atharva-retriever-75b5996b54-w5vgc 4m 98Mi atharva-vllm-predictor-64845bf646-dcdmf 14m 2117Mi","title":"A) metrics-server (HPA on CPU needs this)"},{"location":"lab06/#b-add-resource-spec-to-chat-api","text":"Step 2: Add Resource Spec to the Pod File: k8s/40-serve/deploy-chat-api.yaml spec: replicas: 1 selector: matchLabels: { app: atharva-chat-api } template: metadata: labels: { app: atharva-chat-api } spec: containers: - name: api image: python:3.11-slim resources: requests: cpu: \"50m\" memory: \"64Mi\" limits: cpu: \"250m\" memory: \"256Mi\" kubectl apply -f k8s/40-serve/deploy-chat-api.yaml validate kubectl describe pod -n atharva-app -l \"app=atharva-chat-api\" Look for the container section where you should see the resource spec added.","title":"B) Add Resource Spec to Chat API"},{"location":"lab06/#1-hpa-on-cpu-chat-api","text":"k8s/60-autoscale/hpa-chat-api.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: hpa-chat-api namespace: atharva-app spec: minReplicas: 2 maxReplicas: 6 metrics: - type: ContainerResource containerResource: name: cpu container: api # change this as per actual container name target: type: Utilization averageUtilization: 50 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: atharva-chat-api behavior: scaleDown: policies: - type: Pods value: 2 periodSeconds: 120 - type: Percent value: 25 periodSeconds: 120 stabilizationWindowSeconds: 60 scaleUp: stabilizationWindowSeconds: 45 policies: - type: Percent value: 100 periodSeconds: 15 - type: Pods value: 2 periodSeconds: 15 selectPolicy: Max Apply: kubectl apply -f k8s/60-autoscale/hpa-chat-api.yaml kubectl -n atharva-app get hpa hpa-chat-api -w [expected output] NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE hpa-chat-api Deployment/atharva-chat-api cpu: <unknown>/50% 2 6 2 2m54s hpa-chat-api Deployment/atharva-chat-api cpu: 10%/50% 2 6 2 3m","title":"1) HPA on CPU (Chat API)"},{"location":"lab06/#wait-till-you-see-the-metrics-appear-and-the-minimum-pods-set-to-2-as-per-hpa-spec-to-proceed-with-load-test-next","text":"","title":"Wait till you see the metrics appear and the minimum pods set to 2 as per HPA spec to proceed with load test next."},{"location":"lab06/#3-vpa-recommendations-for-vllm-retriever","text":"Install the VPA components (once): git clone https://github.com/kubernetes/autoscaler.git cd autoscaler/vertical-pod-autoscaler/ # deploy vpa ./hack/vpa-up.sh Recommend-only mode first: k8s/60-autoscale/vpa-vllm.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: vpa-vllm namespace: atharva-ml spec: targetRef: apiVersion: apps/v1 kind: Deployment name: atharva-vllm-predictor updatePolicy: updateMode: \"Off\" # \"Auto\" to apply automatically k8s/60-autoscale/vpa-retriever.yaml apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: vpa-retriever namespace: atharva-ml spec: targetRef: apiVersion: apps/v1 kind: Deployment name: atharva-retriever updatePolicy: updateMode: \"Off\" Apply: kubectl apply -f k8s/60-autoscale/vpa-vllm.yaml kubectl apply -f k8s/60-autoscale/vpa-retriever.yaml # After some traffic: kubectl -n atharva-ml describe vpa vpa-vllm | sed -n '1,200p' kubectl -n atharva-ml describe vpa vpa-retriever | sed -n '1,200p'","title":"3) VPA (recommendations for vLLM &amp; Retriever)"},{"location":"lab06/#5-load-generator-to-trigger-scaling","text":"A tiny Job that hammers the Chat API. Adjust CONCURRENCY & REQUESTS . k8s/60-autoscale/loadgen-job.yaml apiVersion: batch/v1 kind: Job metadata: generateName: loadgen-job- namespace: atharva-app spec: backoffLimit: 0 template: spec: restartPolicy: Never containers: - name: hey image: schoolofdevops/hey:latest command: [\"hey\"] args: - \"-z\" - \"4m\" # duration - \"-q\" - \"50\" # target QPS - \"-c\" - \"25\" # concurrency - \"-m\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"-d\" - '{\"question\":\"Do you accept UPI Payments?\",\"k\":4,\"max_tokens\":160}' - \"http://atharva-chat-api.atharva-app.svc.cluster.local:8080/chat\" Run it: kubectl create -f k8s/60-autoscale/loadgen-job.yaml kubectl -n atharva-app get jobs kubectl -n atharva-app logs -f job/loadgen-job-xxxx Watch scaling [in a new terminal]: # Watch Horizontal Scling with HPA kubectl -n atharva-app get hpa hpa-chat-api -w # Check Resource Scaling Recommendations with VPA kubectl get vpa -w # If using KEDA: kubectl -n atharva-app get deploy/atharva-chat-api -w Once the load test is complete you could delete HPA with kubectl delete -f k8s/60-autoscale/hpa-chat-api.yaml","title":"5) Load generator (to trigger scaling)"},{"location":"lab06/#c-keda-for-event-driven-scale-on-prometheus","text":"Install KEDA as helm repo add kedacore https://kedacore.github.io/charts helm repo update helm install keda kedacore/keda \\ --namespace keda \\ --create-namespace validate kubectl get all -n keda","title":"C) KEDA (for event-driven scale on Prometheus)"},{"location":"lab06/#4-keda-scaledobject-prometheus-trigger","text":"Scale Chat API when RPS > 1 per replica (same signal as HPA external metric). Point KEDA to Prometheus from kube-prometheus-stack. k8s/60-autoscale/keda-scaledobject-chat.yaml apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: chat-api-qps namespace: atharva-app spec: scaleTargetRef: name: atharva-chat-api minReplicaCount: 1 maxReplicaCount: 8 cooldownPeriod: 60 triggers: - type: prometheus metadata: serverAddress: http://kps-kube-prometheus-stack-prometheus.monitoring.svc:9090 metricName: vllm_queue_length threshold: \"5\" query: sum(vllm:num_requests_waiting) Apply: kubectl apply -f k8s/60-autoscale/keda-scaledobject-chat.yaml kubectl -n atharva-app get scaledobject launch the load test and watch ScaledObject + HPA kubectl create -f k8s/60-autoscale/loadgen-job.yaml watch kubectl -n atharva-app get scaledobject,hpa You don\u2019t need Prometheus Adapter for KEDA if you use the prometheus trigger (it queries Prom directly). We keep Adapter anyway for the HPA external-metric demo.","title":"4) KEDA ScaledObject (Prometheus trigger)"},{"location":"lab06/#5-load-generator-to-trigger-scaling_1","text":"A tiny Job that hammers the Chat API. Adjust CONCURRENCY & REQUESTS . k8s/60-autoscale/loadgen-job.yaml apiVersion: batch/v1 kind: Job metadata: generateName: loadgen-job- namespace: atharva-app spec: backoffLimit: 0 template: spec: restartPolicy: Never containers: - name: hey image: schoolofdevops/hey:latest command: [\"hey\"] args: - \"-z\" - \"3m\" # duration - \"-q\" - \"5\" # target QPS - \"-c\" - \"5\" # concurrency - \"-m\" - \"POST\" - \"-H\" - \"Content-Type: application/json\" - \"-d\" - '{\"question\":\"Do you accept UPI Payments?\",\"k\":4,\"max_tokens\":160}' - \"http://atharva-chat-api.atharva-app.svc.cluster.local:8080/chat\" Run it: kubectl apply -f k8s/60-autoscale/loadgen-job.yaml kubectl -n atharva-app logs -f job/chat-loadgen Watch scaling: # If using HPA CPU: kubectl -n atharva-app get hpa hpa-chat-api -w # If using KEDA: kubectl -n atharva-app get deploy/atharva-chat-api -w","title":"5) Load generator (to trigger scaling)"},{"location":"lab06/#tips-gotchas","text":"Pick one scaler per target at a time to avoid fights. For demos: Start with CPU HPA , Switch to RPS HPA (delete CPU HPA), Try KEDA . HPA external metrics require Prometheus Adapter (we installed & mapped chat_requests_per_second ). KEDA Prometheus trigger queries Prom directly; Adapter not required for that path. VPA in \u201cOff\u201d mode shows recommendations under describe vpa . Flip to \"Auto\" if you want automatic updates (be mindful of restarts).","title":"Tips / Gotchas"},{"location":"lab06/#lab-summary","text":"This is what we accomplished in this lab Horizontal scaling of Chat API on CPU or RPS (Prometheus-backed). Vertical recommendations for vLLM and Retriever to right-size resources. Event-driven scaling with KEDA using a Prometheus trigger. A simple load generator to visualize scaling behavior.","title":"Lab Summary"},{"location":"lab06/#coursesllmopslabsv1","text":"","title":"courses/llmops/labs/v1"},{"location":"lab07/","text":"Lab 7 \u2014 GitOps with ArgoCD Goal Run ArgoCD in-cluster. Manage all Atharva components as declarative Applications driven from Git. Order deploys via sync waves . Promote models by editing one line (ImageVolume reference: tag) and letting Argo roll it out. Roll back with a Git revert. Assumptions Your repo is at https://github.com/<you>/atharva-dental-assistant.git (replace everywhere below). Labs 1\u20136 manifests already live under atharva-dental-assistant/k8s/** . 0) Repo layout (GitOps view) Add this under k8s/70-gitops/ : k8s/ 70-gitops/ app-of-apps.yaml # the \u201croot\u201d Application projects/ atharva-project.yaml # (optional) ArgoCD project apps/ app-data.yaml # data synth + FAISS index jobs (Lab 1) app-train.yaml # LoRA train + merge (Lab 2) app-model.yaml # model mount check (Lab 3) or future registry checks app-serve.yaml # retriever + vLLM + chat API (Labs 1 & 4) app-observability.yaml # SMs, alerts, grafana dashboard (Lab 5) app-autoscale.yaml # HPA/VPA/KEDA (Lab 6) Each Application points at an existing path in your repo. No refactor required. cd project/atharva-dental-assistant mkdir k8s/70-gitops mkdir k8s/70-gitops/projects mkdir k8s/70-gitops/apps touch .gitignore Also prepare your repo to be revision controlled so that you could deploy using GitOps. Add .gitignore file to atharva-dental-assistant # ============================ # Python # ============================ __pycache__/ *.py[cod] *.pyo *.pyd *.pdb *.so *.orig *.log *.tmp *.swp .env .venv venv/ env/ pip-wheel-metadata/ *.egg-info/ .eggs/ dist/ build/ # ============================ # Datasets & Artifacts # ============================ artifacts/ datasets/training/* !datasets/clinic/** # Keep clinic dataset (source knowledge) # Ignore training data (sensitive or large) # ============================ # Model Checkpoints / Outputs # ============================ *.ckpt *.pt *.bin *.onnx *.safetensors *.h5 *.tar.gz *.zip *.npz # ============================ # Jupyter & Dev Artifacts # ============================ .ipynb_checkpoints/ *.ipynb~ *.DS_Store *.idea/ .vscode/ *.code-workspace # ============================ # Kubernetes & Deployment Junk # ============================ # Ignore backups or temp yaml files k8s/**/*.orig k8s/**/*.bak k8s/**/*.tmp # Keep main manifests !k8s/**/*.yaml # ============================ # Logs & Results # ============================ logs/ *.log *.out *.err nohup.out results/ outputs/ # ============================ # Temporary Scripts & Cache # ============================ tmp/ .cache/ .tox/ pytest_cache/ __snapshots__/ .coverage coverage.xml htmlcov/ # ============================ # Local Configs & Misc # ============================ *.local *.env .envrc .env.* *.cfg *.ini *.toml *.jsonl.backup # ============================ # OS / Editor # ============================ .DS_Store Thumbs.db .swp *.bak *.tmp Create a new repo https://github.com/new e.g. xxxxxx/atharva-dental-assistant Initialise the local repo and prepate to push the changes # make sure you are in the right path cd project/atharva-dental-assistant git init git add * git commit -am \"importing llmops code\" # Replace xxxxxx with your github id git remote add origin https://github.com/xxxxxx/atharva-dental-assistant.git git push -u origin main 1) Install ArgoCD Create namespace & install: kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml # scale down optional components kubectl -n argocd scale deploy argocd-dex-server argocd-notifications-controller argocd-redis argocd-applicationset-controller --replicas=0 # Wait & get admin password kubectl -n argocd rollout status deploy/argocd-server --timeout=300s || true kubectl -n argocd get pods kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d; echo # Patch and expose ArgoCD sevice on port 32100 kubectl patch svc argocd-server -n argocd --patch \\ '{\"spec\": { \"type\": \"NodePort\", \"ports\": [ { \"nodePort\": 32100, \"port\": 443, \"protocol\": \"TCP\", \"targetPort\": 8080 } ] } }' kubectl get svc -n argocd Login by visiting http://localhost:32100 Login in another shell: argocd login 127.0.0.1:32100 --username admin --password <printed-password> --insecure If you don\u2019t have the argocd CLI, you can do everything from the UI. 2) ArgoCD Project (optional but tidy) k8s/70-gitops/projects/atharva-project.yaml apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: atharva namespace: argocd spec: description: Atharva Dental Clinic - LLMOps Bootcamp sourceRepos: - https://github.com/xxxxxx/atharva-dental-assistant.git destinations: - namespace: atharva-ml server: https://kubernetes.default.svc - namespace: atharva-app server: https://kubernetes.default.svc - namespace: monitoring server: https://kubernetes.default.svc - namespace: keda server: https://kubernetes.default.svc - namespace: argocd server: https://kubernetes.default.svc clusterResourceWhitelist: - group: '*' kind: '*' Ensure you replace xxxxxx with the actual Git Repository Apply: kubectl apply -f k8s/70-gitops/projects/atharva-project.yaml 3) App-of-Apps (the \u201croot\u201d Application) k8s/70-gitops/app-of-apps.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: atharva-root namespace: argocd spec: project: atharva source: repoURL: https://github.com/xxxxxx/atharva-dental-assistant.git targetRevision: main path: k8s/70-gitops/apps destination: server: https://kubernetes.default.svc namespace: argocd syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true Ensure you replace xxxxxx with the actual Git Repository Apply: kubectl apply -f k8s/70-gitops/app-of-apps.yaml 4) Child Applications (one per stack) We\u2019ll sequence with sync-waves so infra comes first, then serving, then autoscaling: Wave 0: observability (Prometheus already installed in Lab 0; this just adds SMs & dashboards) Wave 1: data (jobs), train (jobs), model (image-volume smoke) Wave 2: serve (retriever, vLLM, chat API) Wave 3: autoscale (HPA/VPA/KEDA) Create files in k8s/70-gitops/apps/ : cd k8s/70-gitops/apps/ app-observability.yaml (wave 0) apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: atharva-observability namespace: argocd annotations: argocd.argoproj.io/sync-wave: \"0\" spec: project: atharva source: repoURL: https://github.com/xxxxxx/atharva-dental-assistant.git targetRevision: main path: k8s/50-observability destination: server: https://kubernetes.default.svc namespace: monitoring syncPolicy: automated: { prune: true, selfHeal: true } Ensure you replace xxxxxx with the actual Git Repository app-serve.yaml (wave 1) Points at Lab 4 (vLLM RawDeployment + svc, chat API, cm). Includes ignoreDifferences for KServe\u2019s generated fields (if needed) and a health timeout. apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: atharva-serve namespace: argocd annotations: argocd.argoproj.io/sync-wave: \"1\" spec: project: atharva source: repoURL: https://github.com/xxxxxx/atharva-dental-assistant.git targetRevision: main path: k8s/40-serve destination: server: https://kubernetes.default.svc namespace: atharva-ml syncPolicy: automated: { prune: true, selfHeal: true } ignoreDifferences: - group: serving.kserve.io kind: RawDeployment jsonPointers: - /spec/deployments/0/replicas # controller may reconcile syncOptions: - Validate=true - PrunePropagationPolicy=background revisionHistoryLimit: 5 Ensure you replace xxxxxx with the actual Git Repository If your Chat API manifests live in atharva-app , keep them in k8s/40-serve/ \u2014Argo will create resources in their native namespaces. (Your YAML already sets the namespace for each object.) app-autoscale.yaml (wave 2) Points at Lab 6 (HPA/VPA/KEDA + loadgen). apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: atharva-autoscale namespace: argocd annotations: argocd.argoproj.io/sync-wave: \"2\" spec: project: atharva source: repoURL: https://github.com/xxxxxx/atharva-dental-assistant.git targetRevision: main path: k8s/60-autoscale destination: server: https://kubernetes.default.svc namespace: atharva-app syncPolicy: automated: { prune: true, selfHeal: true } Ensure you replace xxxxxx with the actual Git Repository In case if you want to bulk replace xxxxxx with your userid do this sed -i '' 's/xxxxxx/yourid/g' *.yaml where yourid is what you are replacing the placeholders with Also create kustomization spec at k8s/60-autoscale/kustomization.yaml to apply selective manifests apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - vpa-retriever.yaml - vpa-vllm.yaml - keda-scaledobject-chat.yaml Commit & push these files. # run this from atharva-dental-assistant path git add k8s/* git status git commit -m \"adding argocd app spec\" git push -u origin main 5) Register the Git repo (CLI alternative) If your repo is private , add credentials: argocd repo add https://github.com/<you>/atharva-dental-assistant.git \\ --username <user> --password <token> --insecure-ignore-host-key If public , no action needed. 6) Sync From CLI: argocd app sync atharva-root argocd app list Or click Sync in Argo UI for atharva-root . Argo will create child Applications in wave order, then deploy resources from each path. 7) Model promotion (Git change \u2192 rollout) The one-line you change to promote a model is the ImageVolume reference in k8s/40-serve/rawdeployment-vllm.yaml : volumes: - name: model image: reference: kind-registry:5001/atharva/smollm2-135m-merged:v2 # <\u2014 change tag v1 \u2192 v2 pullPolicy: IfNotPresent Flow Build & push your new model image (Lab 3 scripts), e.g. :v2 . Edit the reference: tag to :v2 , commit & push to main . ArgoCD detects the diff, syncs, and the vLLM pod rolls to the new mounted model. Validate with your Lab 4 smoke test. Rollback : git revert that commit (or change back to :v1 ) \u2192 Argo rolls back. \u2800 Bonus: add a PreSync hook job in k8s/30-model/ to cosign verify the model image before serving. If verify fails, sync fails. 8) (Optional) App-of-Apps + Staging/Prod Create two roots: atharva-root-staging and atharva-root-prod , each pointing at a different branch ( staging , main ). Promotion becomes a PR/merge from staging \u2192 main. The only change needed for a model rollout is still that one tag . 9) (Optional) Argo CD Image Updater If you\u2019d like automatic bumping when a new tag appears in the registry: Install argocd-image-updater . Annotate app-serve.yaml with tracking rules for kind-registry:5001/atharva/smollm2-135m-merged . It will open PRs or auto-commit tag bumps to your Git repo. (Keep manual edits for the bootcamp exercises first.) 10) Quick commands cheat-sheet # List applications argocd app list # Sync root argocd app sync atharva-root # See app tree argocd app get atharva-root # Rollback a child app to a past revision (if needed) argocd app history atharva-serve argocd app rollback atharva-serve <ID> Lab Summary This is what we accomplished in this lab Stood up ArgoCD and declared an App-of-Apps hierarchy. Put every lab stack under GitOps with clear sequencing. Practiced a model promotion by changing the ImageVolume tag in Git. Learned how to rollback using Git/Argo history. courses/llmops/labs/v1","title":"Lab 07 - GitOps for GenAI with ArgoCD"},{"location":"lab07/#lab-7-gitops-with-argocd","text":"","title":"Lab 7 \u2014 GitOps with ArgoCD"},{"location":"lab07/#goal","text":"Run ArgoCD in-cluster. Manage all Atharva components as declarative Applications driven from Git. Order deploys via sync waves . Promote models by editing one line (ImageVolume reference: tag) and letting Argo roll it out. Roll back with a Git revert. Assumptions Your repo is at https://github.com/<you>/atharva-dental-assistant.git (replace everywhere below). Labs 1\u20136 manifests already live under atharva-dental-assistant/k8s/** .","title":"Goal"},{"location":"lab07/#0-repo-layout-gitops-view","text":"Add this under k8s/70-gitops/ : k8s/ 70-gitops/ app-of-apps.yaml # the \u201croot\u201d Application projects/ atharva-project.yaml # (optional) ArgoCD project apps/ app-data.yaml # data synth + FAISS index jobs (Lab 1) app-train.yaml # LoRA train + merge (Lab 2) app-model.yaml # model mount check (Lab 3) or future registry checks app-serve.yaml # retriever + vLLM + chat API (Labs 1 & 4) app-observability.yaml # SMs, alerts, grafana dashboard (Lab 5) app-autoscale.yaml # HPA/VPA/KEDA (Lab 6) Each Application points at an existing path in your repo. No refactor required. cd project/atharva-dental-assistant mkdir k8s/70-gitops mkdir k8s/70-gitops/projects mkdir k8s/70-gitops/apps touch .gitignore Also prepare your repo to be revision controlled so that you could deploy using GitOps. Add .gitignore file to atharva-dental-assistant # ============================ # Python # ============================ __pycache__/ *.py[cod] *.pyo *.pyd *.pdb *.so *.orig *.log *.tmp *.swp .env .venv venv/ env/ pip-wheel-metadata/ *.egg-info/ .eggs/ dist/ build/ # ============================ # Datasets & Artifacts # ============================ artifacts/ datasets/training/* !datasets/clinic/** # Keep clinic dataset (source knowledge) # Ignore training data (sensitive or large) # ============================ # Model Checkpoints / Outputs # ============================ *.ckpt *.pt *.bin *.onnx *.safetensors *.h5 *.tar.gz *.zip *.npz # ============================ # Jupyter & Dev Artifacts # ============================ .ipynb_checkpoints/ *.ipynb~ *.DS_Store *.idea/ .vscode/ *.code-workspace # ============================ # Kubernetes & Deployment Junk # ============================ # Ignore backups or temp yaml files k8s/**/*.orig k8s/**/*.bak k8s/**/*.tmp # Keep main manifests !k8s/**/*.yaml # ============================ # Logs & Results # ============================ logs/ *.log *.out *.err nohup.out results/ outputs/ # ============================ # Temporary Scripts & Cache # ============================ tmp/ .cache/ .tox/ pytest_cache/ __snapshots__/ .coverage coverage.xml htmlcov/ # ============================ # Local Configs & Misc # ============================ *.local *.env .envrc .env.* *.cfg *.ini *.toml *.jsonl.backup # ============================ # OS / Editor # ============================ .DS_Store Thumbs.db .swp *.bak *.tmp Create a new repo https://github.com/new e.g. xxxxxx/atharva-dental-assistant Initialise the local repo and prepate to push the changes # make sure you are in the right path cd project/atharva-dental-assistant git init git add * git commit -am \"importing llmops code\" # Replace xxxxxx with your github id git remote add origin https://github.com/xxxxxx/atharva-dental-assistant.git git push -u origin main","title":"0) Repo layout (GitOps view)"},{"location":"lab07/#1-install-argocd","text":"Create namespace & install: kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml # scale down optional components kubectl -n argocd scale deploy argocd-dex-server argocd-notifications-controller argocd-redis argocd-applicationset-controller --replicas=0 # Wait & get admin password kubectl -n argocd rollout status deploy/argocd-server --timeout=300s || true kubectl -n argocd get pods kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d; echo # Patch and expose ArgoCD sevice on port 32100 kubectl patch svc argocd-server -n argocd --patch \\ '{\"spec\": { \"type\": \"NodePort\", \"ports\": [ { \"nodePort\": 32100, \"port\": 443, \"protocol\": \"TCP\", \"targetPort\": 8080 } ] } }' kubectl get svc -n argocd Login by visiting http://localhost:32100 Login in another shell: argocd login 127.0.0.1:32100 --username admin --password <printed-password> --insecure If you don\u2019t have the argocd CLI, you can do everything from the UI.","title":"1) Install ArgoCD"},{"location":"lab07/#2-argocd-project-optional-but-tidy","text":"k8s/70-gitops/projects/atharva-project.yaml apiVersion: argoproj.io/v1alpha1 kind: AppProject metadata: name: atharva namespace: argocd spec: description: Atharva Dental Clinic - LLMOps Bootcamp sourceRepos: - https://github.com/xxxxxx/atharva-dental-assistant.git destinations: - namespace: atharva-ml server: https://kubernetes.default.svc - namespace: atharva-app server: https://kubernetes.default.svc - namespace: monitoring server: https://kubernetes.default.svc - namespace: keda server: https://kubernetes.default.svc - namespace: argocd server: https://kubernetes.default.svc clusterResourceWhitelist: - group: '*' kind: '*' Ensure you replace xxxxxx with the actual Git Repository Apply: kubectl apply -f k8s/70-gitops/projects/atharva-project.yaml","title":"2) ArgoCD Project (optional but tidy)"},{"location":"lab07/#3-app-of-apps-the-root-application","text":"k8s/70-gitops/app-of-apps.yaml apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: atharva-root namespace: argocd spec: project: atharva source: repoURL: https://github.com/xxxxxx/atharva-dental-assistant.git targetRevision: main path: k8s/70-gitops/apps destination: server: https://kubernetes.default.svc namespace: argocd syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true Ensure you replace xxxxxx with the actual Git Repository Apply: kubectl apply -f k8s/70-gitops/app-of-apps.yaml","title":"3) App-of-Apps (the \u201croot\u201d Application)"},{"location":"lab07/#4-child-applications-one-per-stack","text":"We\u2019ll sequence with sync-waves so infra comes first, then serving, then autoscaling: Wave 0: observability (Prometheus already installed in Lab 0; this just adds SMs & dashboards) Wave 1: data (jobs), train (jobs), model (image-volume smoke) Wave 2: serve (retriever, vLLM, chat API) Wave 3: autoscale (HPA/VPA/KEDA) Create files in k8s/70-gitops/apps/ : cd k8s/70-gitops/apps/","title":"4) Child Applications (one per stack)"},{"location":"lab07/#app-observabilityyaml-wave-0","text":"apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: atharva-observability namespace: argocd annotations: argocd.argoproj.io/sync-wave: \"0\" spec: project: atharva source: repoURL: https://github.com/xxxxxx/atharva-dental-assistant.git targetRevision: main path: k8s/50-observability destination: server: https://kubernetes.default.svc namespace: monitoring syncPolicy: automated: { prune: true, selfHeal: true } Ensure you replace xxxxxx with the actual Git Repository","title":"app-observability.yaml (wave 0)"},{"location":"lab07/#app-serveyaml-wave-1","text":"Points at Lab 4 (vLLM RawDeployment + svc, chat API, cm). Includes ignoreDifferences for KServe\u2019s generated fields (if needed) and a health timeout. apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: atharva-serve namespace: argocd annotations: argocd.argoproj.io/sync-wave: \"1\" spec: project: atharva source: repoURL: https://github.com/xxxxxx/atharva-dental-assistant.git targetRevision: main path: k8s/40-serve destination: server: https://kubernetes.default.svc namespace: atharva-ml syncPolicy: automated: { prune: true, selfHeal: true } ignoreDifferences: - group: serving.kserve.io kind: RawDeployment jsonPointers: - /spec/deployments/0/replicas # controller may reconcile syncOptions: - Validate=true - PrunePropagationPolicy=background revisionHistoryLimit: 5 Ensure you replace xxxxxx with the actual Git Repository If your Chat API manifests live in atharva-app , keep them in k8s/40-serve/ \u2014Argo will create resources in their native namespaces. (Your YAML already sets the namespace for each object.)","title":"app-serve.yaml (wave 1)"},{"location":"lab07/#app-autoscaleyaml-wave-2","text":"Points at Lab 6 (HPA/VPA/KEDA + loadgen). apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: atharva-autoscale namespace: argocd annotations: argocd.argoproj.io/sync-wave: \"2\" spec: project: atharva source: repoURL: https://github.com/xxxxxx/atharva-dental-assistant.git targetRevision: main path: k8s/60-autoscale destination: server: https://kubernetes.default.svc namespace: atharva-app syncPolicy: automated: { prune: true, selfHeal: true } Ensure you replace xxxxxx with the actual Git Repository In case if you want to bulk replace xxxxxx with your userid do this sed -i '' 's/xxxxxx/yourid/g' *.yaml where yourid is what you are replacing the placeholders with Also create kustomization spec at k8s/60-autoscale/kustomization.yaml to apply selective manifests apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - vpa-retriever.yaml - vpa-vllm.yaml - keda-scaledobject-chat.yaml Commit & push these files. # run this from atharva-dental-assistant path git add k8s/* git status git commit -m \"adding argocd app spec\" git push -u origin main","title":"app-autoscale.yaml (wave 2)"},{"location":"lab07/#5-register-the-git-repo-cli-alternative","text":"If your repo is private , add credentials: argocd repo add https://github.com/<you>/atharva-dental-assistant.git \\ --username <user> --password <token> --insecure-ignore-host-key If public , no action needed.","title":"5) Register the Git repo (CLI alternative)"},{"location":"lab07/#6-sync","text":"From CLI: argocd app sync atharva-root argocd app list Or click Sync in Argo UI for atharva-root . Argo will create child Applications in wave order, then deploy resources from each path.","title":"6) Sync"},{"location":"lab07/#7-model-promotion-git-change-rollout","text":"The one-line you change to promote a model is the ImageVolume reference in k8s/40-serve/rawdeployment-vllm.yaml : volumes: - name: model image: reference: kind-registry:5001/atharva/smollm2-135m-merged:v2 # <\u2014 change tag v1 \u2192 v2 pullPolicy: IfNotPresent Flow Build & push your new model image (Lab 3 scripts), e.g. :v2 . Edit the reference: tag to :v2 , commit & push to main . ArgoCD detects the diff, syncs, and the vLLM pod rolls to the new mounted model. Validate with your Lab 4 smoke test. Rollback : git revert that commit (or change back to :v1 ) \u2192 Argo rolls back. \u2800 Bonus: add a PreSync hook job in k8s/30-model/ to cosign verify the model image before serving. If verify fails, sync fails.","title":"7) Model promotion (Git change \u2192 rollout)"},{"location":"lab07/#8-optional-app-of-apps-stagingprod","text":"Create two roots: atharva-root-staging and atharva-root-prod , each pointing at a different branch ( staging , main ). Promotion becomes a PR/merge from staging \u2192 main. The only change needed for a model rollout is still that one tag .","title":"8) (Optional) App-of-Apps + Staging/Prod"},{"location":"lab07/#9-optional-argo-cd-image-updater","text":"If you\u2019d like automatic bumping when a new tag appears in the registry: Install argocd-image-updater . Annotate app-serve.yaml with tracking rules for kind-registry:5001/atharva/smollm2-135m-merged . It will open PRs or auto-commit tag bumps to your Git repo. (Keep manual edits for the bootcamp exercises first.)","title":"9) (Optional) Argo CD Image Updater"},{"location":"lab07/#10-quick-commands-cheat-sheet","text":"# List applications argocd app list # Sync root argocd app sync atharva-root # See app tree argocd app get atharva-root # Rollback a child app to a past revision (if needed) argocd app history atharva-serve argocd app rollback atharva-serve <ID>","title":"10) Quick commands cheat-sheet"},{"location":"lab07/#lab-summary","text":"This is what we accomplished in this lab Stood up ArgoCD and declared an App-of-Apps hierarchy. Put every lab stack under GitOps with clear sequencing. Practiced a model promotion by changing the ImageVolume tag in Git. Learned how to rollback using Git/Argo history.","title":"Lab Summary"},{"location":"lab07/#coursesllmopslabsv1","text":"","title":"courses/llmops/labs/v1"},{"location":"lab08/","text":"Lab 8 \u2014 Lightweight pipelines with Argo Workflows Goals Install Argo Workflows (minimal footprint) on KIND. Define a WorkflowTemplate that chains our steps in one DAG. Run it with one command (or wire it into ArgoCD later if you want GitOps for workflows too). Assumptions: You already have Labs 0\u20137 in place; your KIND nodes mount your repo at /mnt/project , and your repo path is /mnt/project/atharva-dental-assistant . What you\u2019ll add atharva-dental-assistant/ \u251c\u2500 k8s/ \u2502 \u2514\u2500 80-workflows/ \u2502 \u251c\u2500 rbac-argo-wf.yaml \u2502 \u251c\u2500 workflowtemplate-atharva-train.yaml \u2502 \u2514\u2500 workflow-atharva-run.yaml # (optional) direct Workflow using the template \u2514\u2500 scripts/ \u251c\u2500 install_argo_workflows.sh \u2514\u2500 run_workflow.sh 1) Install Argo Workflows (helm, minimal) scripts/install_argo_workflows.sh #!/usr/bin/env bash set -euo pipefail # Create Argo Workflows namespace kubectl create namespace argo-workflows # Install via Helm (lightweight defaults) helm repo add argo https://argoproj.github.io/argo-helm >/dev/null helm repo update >/dev/null helm upgrade --install argo-workflows argo/argo-workflows \\ --namespace argo-workflows \\ --set server.enabled=true \\ --set server.authModes\\[0\\]=server \\ --set workflow.rbac.create=true \\ --set server.serviceType=NodePort \\ --set server.serviceNodePort=30600 # Validate kubectl get all -n argo-workflows # Print the NodePort for the UI echo \"Argo Workflows UI : http://127.0.0.1:30600\" You can also expose the UI via Ingress later; NodePort is perfect for lab. 2) Allow workflows to run in atharva-ml with access to pods/workflows # Run this from project/atharva-dental-assistant mkdir k8s/80-workflows k8s/80-workflows/rbac-argo-wf.yaml apiVersion: v1 kind: ServiceAccount metadata: name: wf-runner namespace: atharva-ml --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: wf-runner-role namespace: atharva-ml rules: - apiGroups: [\"\"] resources: [\"pods\",\"pods/log\",\"pods/exec\",\"secrets\",\"configmaps\",\"persistentvolumeclaims\",\"events\"] verbs: [\"create\",\"get\",\"list\",\"watch\",\"update\",\"patch\",\"delete\"] - apiGroups: [\"batch\"] resources: [\"jobs\"] verbs: [\"create\",\"get\",\"list\",\"watch\",\"delete\"] - apiGroups: [\"argoproj.io\"] resources: [\"workflowtaskresults\",\"workflowtasksets\"] verbs: [\"create\",\"get\",\"list\",\"watch\",\"update\",\"patch\",\"delete\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: wf-runner-rb namespace: atharva-ml subjects: - kind: ServiceAccount name: wf-runner namespace: atharva-ml roleRef: kind: Role name: wf-runner-role apiGroup: rbac.authorization.k8s.io Apply it: kubectl apply -f k8s/80-workflows/rbac-argo-wf.yaml 3) The WorkflowTemplate (our 4-step DAG) k8s/80-workflows/workflowtemplate-atharva-train.yaml apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata: name: atharva-train-merge namespace: atharva-ml spec: entrypoint: dag serviceAccountName: wf-runner podGC: strategy: OnWorkflowSuccess templates: - name: dag dag: tasks: - name: generate-data template: generate-data - name: build-index dependencies: [generate-data] template: build-index - name: train-lora dependencies: [build-index] template: train-lora arguments: parameters: - name: max-steps value: \"{{workflow.parameters.max-steps}}\" - name: merge-model dependencies: [train-lora] template: merge-model # --- Step 1: Generate synthetic training data (no heavy deps) --- - name: generate-data nodeSelector: kubernetes.io/hostname: llmops-kind-worker container: image: python:3.11-slim imagePullPolicy: IfNotPresent command: [\"bash\",\"-lc\"] args: - | set -euo pipefail python /mnt/project/atharva-dental-assistant/tools/synth_data.py \\ --clinic Pune --currency INR \\ --treatments /mnt/project/atharva-dental-assistant/datasets/clinic/treatments.json \\ --policies /mnt/project/atharva-dental-assistant/datasets/clinic/policies/*.md \\ --faq /mnt/project/atharva-dental-assistant/datasets/clinic/faq.md \\ --recent /mnt/project/atharva-dental-assistant/datasets/clinic/recent_queries.jsonl \\ --out /mnt/project/atharva-dental-assistant/datasets/training volumeMounts: - name: host mountPath: /mnt/project resources: requests: cpu: \"250m\" memory: \"512Mi\" volumes: - name: host hostPath: path: /mnt/project type: Directory # --- Step 2: Build sparse TF-IDF index (lightweight, wheels-only) --- - name: build-index nodeSelector: kubernetes.io/hostname: llmops-kind-worker container: image: python:3.11-slim imagePullPolicy: IfNotPresent command: [\"bash\",\"-lc\"] args: - | set -euo pipefail export HOME=/mnt/project VENV=\"$HOME/.venv-build\" ROOT=\"$HOME/atharva-dental-assistant/datasets/clinic\" OUT=\"$HOME/atharva-dental-assistant/artifacts/rag\" python -m venv \"$VENV\" . \"$VENV/bin/activate\" python -m pip install --upgrade pip pip install --only-binary=:all: \\ numpy==1.26.4 scipy==1.10.1 scikit-learn==1.3.2 joblib==1.3.2 mkdir -p \"$OUT\" python /mnt/project/atharva-dental-assistant/rag/build_index.py \\ --root \"$ROOT\" \\ --outdir \"$OUT\" \\ --backend sparse ls -lah \"$OUT\" && (wc -c \"$OUT\"/meta.json || true) volumeMounts: - name: host mountPath: /mnt/project resources: requests: cpu: \"500m\" memory: \"1Gi\" volumes: - name: host hostPath: path: /mnt/project type: Directory # --- Step 3: LoRA training (matches job-train-lora.yaml) --- - name: train-lora inputs: parameters: - name: max-steps value: \"400\" nodeSelector: kubernetes.io/hostname: llmops-kind-worker container: image: schoolofdevops/lora-build-python:3.11-slim imagePullPolicy: IfNotPresent command: [\"bash\",\"-lc\"] args: - | set -euo pipefail # MAX_STEPS is read by your training script from env export MAX_STEPS={{inputs.parameters.max-steps}} python /mnt/project/atharva-dental-assistant/training/train_lora.py # record last run id for the next step RUN_DIR=\"$(ls -1dt /mnt/project/atharva-dental-assistant/artifacts/train/*/ | head -n 1)\" RUN_ID=\"$(basename \"$RUN_DIR\")\" echo \"$RUN_ID\" > /mnt/project/atharva-dental-assistant/artifacts/train/LAST_RUN_ID.txt env: - name: BASE_MODEL value: \"HuggingFaceTB/SmolLM2-135M-Instruct\" - name: MAX_SEQ_LEN value: \"256\" - name: LORA_R value: \"4\" - name: LORA_ALPHA value: \"8\" - name: LORA_DROPOUT value: \"0.05\" - name: LR value: \"2e-4\" - name: WARMUP_RATIO value: \"0.02\" - name: BATCH_SIZE value: \"1\" - name: GRAD_ACCUM value: \"1\" - name: MAX_STEPS value: \"80\" # default; overridden by parameter above - name: DEMO_MAX_TRAIN_SAMPLES value: \"0\" - name: DEMO_MAX_VAL_SAMPLES value: \"0\" - name: HF_HOME value: \"/cache/hf\" - name: HF_HUB_DISABLE_TELEMETRY value: \"1\" - name: TOKENIZERS_PARALLELISM value: \"true\" - name: OMP_NUM_THREADS value: \"4\" - name: MKL_NUM_THREADS value: \"4\" - name: NUMEXPR_MAX_THREADS value: \"4\" resources: requests: cpu: \"2\" memory: \"4Gi\" ephemeral-storage: \"5Gi\" limits: cpu: \"4\" memory: \"6Gi\" ephemeral-storage: \"20Gi\" volumeMounts: - name: host mountPath: /mnt/project - name: hf-cache mountPath: /cache/hf volumes: - name: host hostPath: path: /mnt/project type: Directory - name: hf-cache hostPath: path: /mnt/hf-cache type: DirectoryOrCreate # --- Step 4: Merge LoRA into base model (matches job-merge-model.yaml) --- - name: merge-model nodeSelector: kubernetes.io/hostname: llmops-kind-worker container: image: schoolofdevops/lora-build-python:3.11-slim imagePullPolicy: IfNotPresent command: [\"bash\",\"-lc\"] args: - | set -euo pipefail # If RUN_ID not provided, read the last run id produced by previous step export RUN_ID=\"${RUN_ID:-$(tr -d ' \\n' </mnt/project/atharva-dental-assistant/artifacts/train/LAST_RUN_ID.txt)}\" echo \"Merging RUN_ID=$RUN_ID\" python /mnt/project/atharva-dental-assistant/training/merge_lora.py env: - name: BASE_MODEL value: \"HuggingFaceTB/SmolLM2-135M-Instruct\" # Optional: allow overriding RUN_ID from workflow params if ever needed # - name: RUN_ID # value: \"REPLACE_WITH_RUN_ID\" volumeMounts: - name: host mountPath: /mnt/project resources: requests: cpu: \"500m\" memory: \"2Gi\" volumes: - name: host hostPath: path: /mnt/project type: Directory arguments: parameters: - name: max-steps value: \"100\" 5) A concrete Workflow that uses the template (optional) k8s/80-workflows/workflow-atharva-run.yaml apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: atharva-run- namespace: atharva-ml spec: serviceAccountName: wf-runner workflowTemplateRef: name: atharva-train-merge arguments: parameters: - name: max-steps value: \"100\" 6) Run it # Install Argo Workflows (UI will be port-forwarded on 2746) bash scripts/install_argo_workflows.sh # (Use a new terminal if you want to keep the UI running) # Create RBAC + PV/PVC + Template kubectl apply -f k8s/80-workflows/rbac-argo-wf.yaml kubectl apply -f k8s/80-workflows/pv-pvc-project.yaml kubectl apply -f k8s/80-workflows/workflowtemplate-atharva-train.yaml # Start a run kubectl create -f k8s/80-workflows/workflow-atharva-run.yaml # Watch progress kubectl -n atharva-ml get wf kubectl -n atharva-ml get pods -w Or trigger from the Argo Workflows UI at http://127.0.0.1:30600/ (submit from template, optionally set max-steps param). When the workflow finishes, you\u2019ll have: artifacts/ rag/... train/<RUN_ID>/ lora_adapter/ tokenizer/ merged-model/ model.tgz run.json \u2026ready for Lab 3 (build OCI model image) and Lab 4 (serve via vLLM) if you want to automate promotion later. Optional: GitOps this with ArgoCD Since you already have ArgoCD managing serving/obs/scale, you can also let ArgoCD manage: the WorkflowTemplate YAML (declarative), and a CronWorkflow (if you want scheduled retraining; not included above to keep it minimal). Example ArgoCD child app path: k8s/80-workflows/ with sync-wave 1 (before serving) if you prefer. Why this is lightweight (in comparison to kubeflow ? ) Argo Workflows adds a small controller + optional UI service (we enabled the server). No MinIO/MySQL pair like full Kubeflow Pipelines; all artifacts are simply hostPath on disk. Each step is a tiny python:3.11-slim container running the same scripts from your repo\u2014no code duplication, no special SDK. You can swap images or add Kaniko/BuildKit step later if you want to bake the model image inside the workflow. Lab Summary This is what we accomplished in this lab Installed Argo Workflows and created a WorkflowTemplate to orchestrate data \u2192 index \u2192 train \u2192 merge. Reused /mnt/project via a PVC so steps share files seamlessly. Triggered a full run either via kubectl or the Argo UI . Kept the lab footprint light and aligned with your ArgoCD stack.","title":"Lab 08 - Automating LLM pipelines with Argo Workflows"},{"location":"lab08/#lab-8-lightweight-pipelines-with-argo-workflows","text":"","title":"Lab 8 \u2014 Lightweight pipelines with Argo Workflows"},{"location":"lab08/#goals","text":"Install Argo Workflows (minimal footprint) on KIND. Define a WorkflowTemplate that chains our steps in one DAG. Run it with one command (or wire it into ArgoCD later if you want GitOps for workflows too). Assumptions: You already have Labs 0\u20137 in place; your KIND nodes mount your repo at /mnt/project , and your repo path is /mnt/project/atharva-dental-assistant .","title":"Goals"},{"location":"lab08/#what-youll-add","text":"atharva-dental-assistant/ \u251c\u2500 k8s/ \u2502 \u2514\u2500 80-workflows/ \u2502 \u251c\u2500 rbac-argo-wf.yaml \u2502 \u251c\u2500 workflowtemplate-atharva-train.yaml \u2502 \u2514\u2500 workflow-atharva-run.yaml # (optional) direct Workflow using the template \u2514\u2500 scripts/ \u251c\u2500 install_argo_workflows.sh \u2514\u2500 run_workflow.sh","title":"What you\u2019ll add"},{"location":"lab08/#1-install-argo-workflows-helm-minimal","text":"","title":"1) Install Argo Workflows (helm, minimal)"},{"location":"lab08/#scriptsinstall_argo_workflowssh","text":"#!/usr/bin/env bash set -euo pipefail # Create Argo Workflows namespace kubectl create namespace argo-workflows # Install via Helm (lightweight defaults) helm repo add argo https://argoproj.github.io/argo-helm >/dev/null helm repo update >/dev/null helm upgrade --install argo-workflows argo/argo-workflows \\ --namespace argo-workflows \\ --set server.enabled=true \\ --set server.authModes\\[0\\]=server \\ --set workflow.rbac.create=true \\ --set server.serviceType=NodePort \\ --set server.serviceNodePort=30600 # Validate kubectl get all -n argo-workflows # Print the NodePort for the UI echo \"Argo Workflows UI : http://127.0.0.1:30600\" You can also expose the UI via Ingress later; NodePort is perfect for lab.","title":"scripts/install_argo_workflows.sh"},{"location":"lab08/#2-allow-workflows-to-run-in-atharva-ml-with-access-to-podsworkflows","text":"# Run this from project/atharva-dental-assistant mkdir k8s/80-workflows","title":"2) Allow workflows to run in atharva-ml with access to pods/workflows"},{"location":"lab08/#k8s80-workflowsrbac-argo-wfyaml","text":"apiVersion: v1 kind: ServiceAccount metadata: name: wf-runner namespace: atharva-ml --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: wf-runner-role namespace: atharva-ml rules: - apiGroups: [\"\"] resources: [\"pods\",\"pods/log\",\"pods/exec\",\"secrets\",\"configmaps\",\"persistentvolumeclaims\",\"events\"] verbs: [\"create\",\"get\",\"list\",\"watch\",\"update\",\"patch\",\"delete\"] - apiGroups: [\"batch\"] resources: [\"jobs\"] verbs: [\"create\",\"get\",\"list\",\"watch\",\"delete\"] - apiGroups: [\"argoproj.io\"] resources: [\"workflowtaskresults\",\"workflowtasksets\"] verbs: [\"create\",\"get\",\"list\",\"watch\",\"update\",\"patch\",\"delete\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: wf-runner-rb namespace: atharva-ml subjects: - kind: ServiceAccount name: wf-runner namespace: atharva-ml roleRef: kind: Role name: wf-runner-role apiGroup: rbac.authorization.k8s.io Apply it: kubectl apply -f k8s/80-workflows/rbac-argo-wf.yaml","title":"k8s/80-workflows/rbac-argo-wf.yaml"},{"location":"lab08/#3-the-workflowtemplate-our-4-step-dag","text":"","title":"3) The WorkflowTemplate (our 4-step DAG)"},{"location":"lab08/#k8s80-workflowsworkflowtemplate-atharva-trainyaml","text":"apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata: name: atharva-train-merge namespace: atharva-ml spec: entrypoint: dag serviceAccountName: wf-runner podGC: strategy: OnWorkflowSuccess templates: - name: dag dag: tasks: - name: generate-data template: generate-data - name: build-index dependencies: [generate-data] template: build-index - name: train-lora dependencies: [build-index] template: train-lora arguments: parameters: - name: max-steps value: \"{{workflow.parameters.max-steps}}\" - name: merge-model dependencies: [train-lora] template: merge-model # --- Step 1: Generate synthetic training data (no heavy deps) --- - name: generate-data nodeSelector: kubernetes.io/hostname: llmops-kind-worker container: image: python:3.11-slim imagePullPolicy: IfNotPresent command: [\"bash\",\"-lc\"] args: - | set -euo pipefail python /mnt/project/atharva-dental-assistant/tools/synth_data.py \\ --clinic Pune --currency INR \\ --treatments /mnt/project/atharva-dental-assistant/datasets/clinic/treatments.json \\ --policies /mnt/project/atharva-dental-assistant/datasets/clinic/policies/*.md \\ --faq /mnt/project/atharva-dental-assistant/datasets/clinic/faq.md \\ --recent /mnt/project/atharva-dental-assistant/datasets/clinic/recent_queries.jsonl \\ --out /mnt/project/atharva-dental-assistant/datasets/training volumeMounts: - name: host mountPath: /mnt/project resources: requests: cpu: \"250m\" memory: \"512Mi\" volumes: - name: host hostPath: path: /mnt/project type: Directory # --- Step 2: Build sparse TF-IDF index (lightweight, wheels-only) --- - name: build-index nodeSelector: kubernetes.io/hostname: llmops-kind-worker container: image: python:3.11-slim imagePullPolicy: IfNotPresent command: [\"bash\",\"-lc\"] args: - | set -euo pipefail export HOME=/mnt/project VENV=\"$HOME/.venv-build\" ROOT=\"$HOME/atharva-dental-assistant/datasets/clinic\" OUT=\"$HOME/atharva-dental-assistant/artifacts/rag\" python -m venv \"$VENV\" . \"$VENV/bin/activate\" python -m pip install --upgrade pip pip install --only-binary=:all: \\ numpy==1.26.4 scipy==1.10.1 scikit-learn==1.3.2 joblib==1.3.2 mkdir -p \"$OUT\" python /mnt/project/atharva-dental-assistant/rag/build_index.py \\ --root \"$ROOT\" \\ --outdir \"$OUT\" \\ --backend sparse ls -lah \"$OUT\" && (wc -c \"$OUT\"/meta.json || true) volumeMounts: - name: host mountPath: /mnt/project resources: requests: cpu: \"500m\" memory: \"1Gi\" volumes: - name: host hostPath: path: /mnt/project type: Directory # --- Step 3: LoRA training (matches job-train-lora.yaml) --- - name: train-lora inputs: parameters: - name: max-steps value: \"400\" nodeSelector: kubernetes.io/hostname: llmops-kind-worker container: image: schoolofdevops/lora-build-python:3.11-slim imagePullPolicy: IfNotPresent command: [\"bash\",\"-lc\"] args: - | set -euo pipefail # MAX_STEPS is read by your training script from env export MAX_STEPS={{inputs.parameters.max-steps}} python /mnt/project/atharva-dental-assistant/training/train_lora.py # record last run id for the next step RUN_DIR=\"$(ls -1dt /mnt/project/atharva-dental-assistant/artifacts/train/*/ | head -n 1)\" RUN_ID=\"$(basename \"$RUN_DIR\")\" echo \"$RUN_ID\" > /mnt/project/atharva-dental-assistant/artifacts/train/LAST_RUN_ID.txt env: - name: BASE_MODEL value: \"HuggingFaceTB/SmolLM2-135M-Instruct\" - name: MAX_SEQ_LEN value: \"256\" - name: LORA_R value: \"4\" - name: LORA_ALPHA value: \"8\" - name: LORA_DROPOUT value: \"0.05\" - name: LR value: \"2e-4\" - name: WARMUP_RATIO value: \"0.02\" - name: BATCH_SIZE value: \"1\" - name: GRAD_ACCUM value: \"1\" - name: MAX_STEPS value: \"80\" # default; overridden by parameter above - name: DEMO_MAX_TRAIN_SAMPLES value: \"0\" - name: DEMO_MAX_VAL_SAMPLES value: \"0\" - name: HF_HOME value: \"/cache/hf\" - name: HF_HUB_DISABLE_TELEMETRY value: \"1\" - name: TOKENIZERS_PARALLELISM value: \"true\" - name: OMP_NUM_THREADS value: \"4\" - name: MKL_NUM_THREADS value: \"4\" - name: NUMEXPR_MAX_THREADS value: \"4\" resources: requests: cpu: \"2\" memory: \"4Gi\" ephemeral-storage: \"5Gi\" limits: cpu: \"4\" memory: \"6Gi\" ephemeral-storage: \"20Gi\" volumeMounts: - name: host mountPath: /mnt/project - name: hf-cache mountPath: /cache/hf volumes: - name: host hostPath: path: /mnt/project type: Directory - name: hf-cache hostPath: path: /mnt/hf-cache type: DirectoryOrCreate # --- Step 4: Merge LoRA into base model (matches job-merge-model.yaml) --- - name: merge-model nodeSelector: kubernetes.io/hostname: llmops-kind-worker container: image: schoolofdevops/lora-build-python:3.11-slim imagePullPolicy: IfNotPresent command: [\"bash\",\"-lc\"] args: - | set -euo pipefail # If RUN_ID not provided, read the last run id produced by previous step export RUN_ID=\"${RUN_ID:-$(tr -d ' \\n' </mnt/project/atharva-dental-assistant/artifacts/train/LAST_RUN_ID.txt)}\" echo \"Merging RUN_ID=$RUN_ID\" python /mnt/project/atharva-dental-assistant/training/merge_lora.py env: - name: BASE_MODEL value: \"HuggingFaceTB/SmolLM2-135M-Instruct\" # Optional: allow overriding RUN_ID from workflow params if ever needed # - name: RUN_ID # value: \"REPLACE_WITH_RUN_ID\" volumeMounts: - name: host mountPath: /mnt/project resources: requests: cpu: \"500m\" memory: \"2Gi\" volumes: - name: host hostPath: path: /mnt/project type: Directory arguments: parameters: - name: max-steps value: \"100\"","title":"k8s/80-workflows/workflowtemplate-atharva-train.yaml"},{"location":"lab08/#5-a-concrete-workflow-that-uses-the-template-optional","text":"","title":"5) A concrete Workflow that uses the template (optional)"},{"location":"lab08/#k8s80-workflowsworkflow-atharva-runyaml","text":"apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: atharva-run- namespace: atharva-ml spec: serviceAccountName: wf-runner workflowTemplateRef: name: atharva-train-merge arguments: parameters: - name: max-steps value: \"100\"","title":"k8s/80-workflows/workflow-atharva-run.yaml"},{"location":"lab08/#6-run-it","text":"# Install Argo Workflows (UI will be port-forwarded on 2746) bash scripts/install_argo_workflows.sh # (Use a new terminal if you want to keep the UI running) # Create RBAC + PV/PVC + Template kubectl apply -f k8s/80-workflows/rbac-argo-wf.yaml kubectl apply -f k8s/80-workflows/pv-pvc-project.yaml kubectl apply -f k8s/80-workflows/workflowtemplate-atharva-train.yaml # Start a run kubectl create -f k8s/80-workflows/workflow-atharva-run.yaml # Watch progress kubectl -n atharva-ml get wf kubectl -n atharva-ml get pods -w Or trigger from the Argo Workflows UI at http://127.0.0.1:30600/ (submit from template, optionally set max-steps param). When the workflow finishes, you\u2019ll have: artifacts/ rag/... train/<RUN_ID>/ lora_adapter/ tokenizer/ merged-model/ model.tgz run.json \u2026ready for Lab 3 (build OCI model image) and Lab 4 (serve via vLLM) if you want to automate promotion later.","title":"6) Run it"},{"location":"lab08/#optional-gitops-this-with-argocd","text":"Since you already have ArgoCD managing serving/obs/scale, you can also let ArgoCD manage: the WorkflowTemplate YAML (declarative), and a CronWorkflow (if you want scheduled retraining; not included above to keep it minimal). Example ArgoCD child app path: k8s/80-workflows/ with sync-wave 1 (before serving) if you prefer.","title":"Optional: GitOps this with ArgoCD"},{"location":"lab08/#why-this-is-lightweight-in-comparison-to-kubeflow","text":"Argo Workflows adds a small controller + optional UI service (we enabled the server). No MinIO/MySQL pair like full Kubeflow Pipelines; all artifacts are simply hostPath on disk. Each step is a tiny python:3.11-slim container running the same scripts from your repo\u2014no code duplication, no special SDK. You can swap images or add Kaniko/BuildKit step later if you want to bake the model image inside the workflow.","title":"Why this is lightweight (in comparison to kubeflow ? )"},{"location":"lab08/#lab-summary","text":"This is what we accomplished in this lab Installed Argo Workflows and created a WorkflowTemplate to orchestrate data \u2192 index \u2192 train \u2192 merge. Reused /mnt/project via a PVC so steps share files seamlessly. Triggered a full run either via kubectl or the Argo UI . Kept the lab footprint light and aligned with your ArgoCD stack.","title":"Lab Summary"}]}